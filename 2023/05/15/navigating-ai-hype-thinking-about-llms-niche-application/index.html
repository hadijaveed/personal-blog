<!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--><!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes about startups, AI, health-care and overall engineering"><meta name=author content="Hadi Javeed"><link href=https://hadijaveed.me/2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/ rel=canonical><link href=../../../../2022/10/04/using-nudges-to-reinforce-health-behaviors/ rel=prev><link href=../../../../2024/03/05/tracing-and-observability-in-llm-applications/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../feed_rss_updated.xml><link rel=icon href=../../../../assets/logo.jpeg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Navigating the AI Hype and Thinking about Niche LLM Applications - Hadi Javeed's blog</title><link rel=stylesheet href=../../../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XS27CVTCE3"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XS27CVTCE3",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XS27CVTCE3",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Navigating the AI Hype and Thinking about Niche LLM Applications - Hadi Javeed's blog"><meta property=og:description content="Notes about startups, AI, health-care and overall engineering"><meta property=og:image content=https://hadijaveed.me/assets/images/social/2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://hadijaveed.me/2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/ property=og:url><meta property=twitter:card content=summary_large_image><meta property=twitter:title content="Navigating the AI Hype and Thinking about Niche LLM Applications - Hadi Javeed's blog"><meta property=twitter:description content="Notes about startups, AI, health-care and overall engineering"><meta property=twitter:image content=https://hadijaveed.me/assets/images/social/2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application.png><link href=../../../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#navigating-the-ai-hype-and-thinking-about-niche-llm-applications class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Hadi Javeed's blog" class="md-header__button md-logo" aria-label="Hadi Javeed's blog" data-md-component=logo> <img src=../../../../assets/logo.jpeg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Hadi Javeed's blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Navigating the AI Hype and Thinking about Niche LLM Applications </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../about/ class=md-tabs__link> About me </a> </li> <li class=md-tabs__item> <a href=https://newsletter.hadijaveed.me class=md-tabs__link> Subscribe </a> </li> <li class=md-tabs__item> <a href=../../../../archive/2026/ class=md-tabs__link> Archive </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Hadi Javeed's blog" class="md-nav__button md-logo" aria-label="Hadi Javeed's blog" data-md-component=logo> <img src=../../../../assets/logo.jpeg alt=logo> </a> Hadi Javeed's blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <a href=../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../about/ class=md-nav__link> <span class=md-ellipsis> About me </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=https://newsletter.hadijaveed.me class=md-nav__link> <span class=md-ellipsis> Subscribe </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../archive/2026/ class=md-nav__link> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <!-- Page content --> <article class="md-content__inner md-typeset"> <header class=md-post__header> <!-- Post authors --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <div class=md-meta__list> <div style="margin-right: 1rem;"> <span class=md-author> <a href=https://www.linkedin.com/in/hadijaveed/ > <img src="https://avatars.githubusercontent.com/u/10227760?v=5" alt="Hadi Javeed"> </a> </span> </div> <!-- Post date --> <div class="md-meta__item blog-meta-header"> <time datetime="2023-05-15 00:00:00+00:00">2023/05/15</time></div> <!-- Post categories --> <!-- Post readtime --> <div class="md-meta__item blog-meta-header"> 15 min read </div> </div> <!-- Draft marker --> </div> </header> <h1 id=navigating-the-ai-hype-and-thinking-about-niche-llm-applications>Navigating the AI Hype and Thinking about Niche LLM Applications<a class=headerlink href=#navigating-the-ai-hype-and-thinking-about-niche-llm-applications title="Permanent link">¶</a></h1> <p>Recently, there has been a surge of enthusiasm surrounding large language models (LLMs) and generative AI, and justifiably so: LLMs have the power to revolutionize entire industries. Yet, this enthusiasm often gives rise to inevitable hype. It appears somewhat counterintuitive to avoid incorporating “AI” into a product’s presentation, considering the immediate market interest it can generate.</p> <!-- more --> <p>It’s funny how we sometimes get caught up in the thrill of flashy new tools, losing sight of what really matters — solving actual problems.</p> <blockquote> <p>In this article, I’m not discussing ChatGPT prompts that promise to transform you into a 10X person, grant you a competitive edge, or make you fear being replaced by AI aficionados at work. However, I do recommend learning prompting techniques.</p> <p>For example, check out this excellent&nbsp;<a href=https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ >free course on prompt engineering by Andrew N</a>g or dive into intriguing papers&nbsp;<a href=https://arxiv.org/pdf/2305.00050.pdf>like this one</a>&nbsp;to discover effective prompting patterns and always there is enough about prompts on Twitter. Prompting is an excellent technique to get a lot out of LLMs.</p> </blockquote> <p>We’ll delve into using LLMs for specialized tasks with enterprise or organizational private data, like&nbsp;question-answering, summarization, clustering, recommendations, and crafting conversational/neural search experiences.</p> <p>I’ve decided to jot down what I’ve learned for myself and the team I work with, and I thought, why not share it through this article? In our upcoming posts, we’ll be chatting about topics like LLM Chains, Intent Recognition, adding task-specific tools, clustering, creating a recommendation system,&nbsp;fine-tuning open-source LLMs, and our way of testing the system, which we are continuously improving and learning from the research and open-source community. So, stick around, and let’s explore these ideas together!</p> <h2 id=augmenting-llms-with-private-domain-specific-data>Augmenting LLMs with private domain-specific data<a class=headerlink href=#augmenting-llms-with-private-domain-specific-data title="Permanent link">¶</a></h2> <p>LLMs, such as&nbsp;<a href=https://platform.openai.com/docs/models/overview>GPT</a>, and other&nbsp;<a href=https://github.com/eugeneyan/open-llms>open-source LLMs</a>, are exceptional technologies for&nbsp;knowledge generation and reasoning. Trained on extensive public data, these foundational models can be adapted for diverse tasks. Two common paradigms have emerged to tackle domain-specific problems and incorporate private/external knowledge:</p> <ol> <li>Fine-tuning&nbsp;a pre-trained model for domain-specific applications involves training the model on a particular dataset using hosted large language model APIs or open-source foundational models like Llama. This process incorporates existing foundational datasets and augments or aligns the model with domain-specific supervised or unsupervised data, depending on your use case. However, we won’t discuss this paradigm in the current article; I plan to write a separate piece on this topic.<br> It’s worth noting that this approach can be expensive, as fine-tuning typically requires training on costly GPUs if using an open-source model or utilizing high-priced Azure or OpenAI endpoints.&nbsp;Although&nbsp;emerging patterns like&nbsp;<a href=https://github.com/huggingface/peft>PEFT</a>&nbsp;and&nbsp;<a href=https://github.com/tloen/alpaca-lora>LORA</a>&nbsp;now enable training open-source LLMs on smaller GPUs, aligning models to specific domains such as healthcare or finance remains a challenge.&nbsp;This area necessitates extensive testing, and we’re still in the process of deepening our understanding.</li> <li>The second emerging paradigm is a&nbsp;<a href=https://arxiv.org/abs/2302.00083>context-augmented generation</a>&nbsp;or often mentioned as&nbsp;Retrieval Augmented Generation (RAG), wherein&nbsp;the relevant context is&nbsp;incorporated into the input prompt, leveraging the LLM’s reasoning capabilities to generate a response.<br> In this article, we will concentrate on this technique and explore methods for embedding domain-specific private data. Regardless if you take option 1, RAG still has its place because most of the LLMs are frozen at a certain point, where you still need RAG to connect to your LLM with ever-growing or fresh data-set through search, etc.</li> </ol> <h2 id=retrieval-augmented-generation-rag>Retrieval Augmented Generation (RAG)<a class=headerlink href=#retrieval-augmented-generation-rag title="Permanent link">¶</a></h2> <p>The concept is straightforward:&nbsp;<a href=https://arxiv.org/abs/2302.00083>use semantic search or other retrieval techniques</a>&nbsp;to extract pertinent documents from your corpus, then&nbsp;feed the relevant text segments into the language model (LM) as prompts or contexts.&nbsp;This allows the LM to reason, answer questions, and generate specific, relevant content without deviating from unrelated topics. Careful prompt design and model temperature management help keep the LM focused.<br> This approach is highly effective for many&nbsp;basic applications,&nbsp;eliminating the need to fine-tune&nbsp;the LM or complicate the deployment process, as&nbsp;most of the current LMs are frozen and don’t have live data access.&nbsp;Connecting the LM to external data sources becomes a powerful tool for addressing various queries.&nbsp;Integrating the few-shot prompting technique with retrieved text segments can further enhance the model’s ability to generate accurate responses.</p> <p>The&nbsp;retrieval process and corpus organization play crucial roles&nbsp;in this approach, as&nbsp;how documents are segmented, queried, and correlated significantly impacts the relevance of the content fed to the LM based on user queries.&nbsp;Document ranking processes&nbsp;also help ensure the most pertinent information is provided to the LM. Numerous open-source frameworks are available to facilitate these tasks.<br> To optimize results, it is essential to be pragmatic in selecting the appropriate techniques and understanding when and where to employ them.&nbsp;Additionally, considering your organization’s machine learning operations (MLOps) processes is crucial for seamless integration and deployment.</p> <p><a href=https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf>Embedding is a process of capturing</a>&nbsp;the semantic meaning of the text into numerical vectors.</p> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://miro.medium.com/v2/resize:fit:1400/1*lNRs2f1GoldIEVlw9GbJYg.png data-desc-position=bottom><img alt src=https://miro.medium.com/v2/resize:fit:1400/1*lNRs2f1GoldIEVlw9GbJYg.png></a></p> <ol> <li>In this process,&nbsp;embeddings are generated for search document segments and later fed into LLM.&nbsp;Embeddings transform the text into vector representations, situating them within a high-dimensional space.&nbsp;For those familiar with natural language processing (NLP) prior to the LLM era or before the advent of Google BERT, models such as Word2Vec and GLOVE may come to mind.<br> It is important to acknowledge the&nbsp;remarkable advancements made by the research community,&nbsp;<a href=https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf>OpenAI</a>, Cohere, and others in refining embedding techniques and in making them more developer friendly or accessible.&nbsp;These&nbsp;embeddings can be stored in common vector stores, Redis, Postgres, Vector database, or in-memory NumPy arrays, to facilitate further analysis and manipulation.</li> <li>User queries are&nbsp;similarly vectorized&nbsp;after converting relevant documents into vector representations and storing them in a vector store.&nbsp;To retrieve the most pertinent document segments based on the user’s query,&nbsp;a nearest neighbor search, often employing the k-NN algorithm, is&nbsp;performed within the vector space, usually using a Cosine similarity score&nbsp;or other distance calculation techniques.<br> This process ensures that the&nbsp;most closely related content is identified and retrieved in response to user inquiries. Furthermore,&nbsp;ranking or&nbsp;re-ranking based on relevance scores can be applied&nbsp;at this stage to&nbsp;enhance the accuracy of the results.</li> <li>A&nbsp;comprehensible response is generated by feeding the&nbsp;top-K relevant segments to the language model (LM).&nbsp;The&nbsp;LM&nbsp;enhances the neural search process by reasoning&nbsp;from the provided document segments and the&nbsp;prompt for steerability of the response, ultimately producing a highly human-like response.</li> </ol> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://miro.medium.com/v2/resize:fit:1400/1*lTlAaEEaTte3PaWPHsUMXw.png data-desc-position=bottom><img alt src=https://miro.medium.com/v2/resize:fit:1400/1*lTlAaEEaTte3PaWPHsUMXw.png></a></p> <h2 id=data-preprocessing-chunking-and-retrieval-techniques><strong>Data-preprocessing, chunking, and retrieval techniques</strong><a class=headerlink href=#data-preprocessing-chunking-and-retrieval-techniques title="Permanent link">¶</a></h2> <p><strong>Data-preprocessing</strong> The nature of your data necessitates careful pre-processing to ensure optimal results.&nbsp;It’s essential to&nbsp;eliminate extraneous information, such as redundant headers and footers, HTML tags in HTML pages, PDFs, and other documents.&nbsp;Minimizing noise in the data&nbsp;can&nbsp;enhance the accuracy and relevance of query results&nbsp;more effectively. These fundamental data-preprocessing steps are not unique to this context; they are&nbsp;essential for constructing any efficient information retrieval system.</p> <p><strong>Chunking</strong> Chunking&nbsp;involves&nbsp;breaking down larger portions of documents into smaller text blocks, which is a&nbsp;crucial data-preprocessing step&nbsp;before creating a vector store from the corpus.&nbsp;This process is important for two main reasons:</p> <ol> <li>Optimizing the accuracy of user search results by removing noise from the corpus,&nbsp;converting them into smaller relevant chunks, and retaining only the most relevant text without losing context&nbsp;or essential meaning.</li> <li>Adhering to token limits imposed by embedding models and the constraints of the LLM’s&nbsp;context must be considered.</li> </ol> <p>To&nbsp;determine the most effective&nbsp;chunking approach, you’ll need to&nbsp;experiment with various techniques&nbsp;that account for the nature of your data.&nbsp;Defining&nbsp;chunk size, chunk boundaries, and overlap is essential.&nbsp;Begin by analyzing the characteristics of your data and adjusting the techniques accordingly.</p> <p><strong>A note on pricing</strong>: When opting not to use an open-source, self-hosted embedding model, it’s crucial to be cognizant of the potential costs associated with proprietary models, such as those from OpenAI. Although the&nbsp;per-token cost might seem inconsequential (for instance, $0.004 per token), large datasets can add up to substantial expenses.</p> <p>Let’s do some quick calculations to illustrate this point. Suppose you have 100,000 documents (PDFs, CSVs, etc.), each with an average of 70,000&nbsp;<a href=https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them>tokens</a>. It’s essential to thoroughly&nbsp;remove noise and strategically chunk data, or perhaps start small to manage costs. It’s vital to&nbsp;avoid the repetitive regeneration of embeddings due to minor oversights such as inefficient chunking, inadequate application of segment overlaps, or subpar data pre-processing. These seemingly small errors can quickly become expensive in time and resources. Also, here’s a rough estimate of the scenario discussed above:</p> <p>(70,000 tokens/document * 100,000 documents) / 1000 * ($0.0004 per 1000 tokens for embeddings API) = $2,800 for using the embeddings API.</p> <p>Here are a few techniques for&nbsp;chunking data effectively. Additionally,&nbsp;<a href=https://www.pinecone.io/learn/chunking-strategies/ >this article from Pinecone</a>&nbsp;provides an excellent guide on implementing these chunking methods proficiently.</p> <p>**Create segments of paragraphs/sentences ** The simplest method for dividing text into chunks is by using a fixed size, which can be based on the number of words or tokens. For example, in&nbsp;Python, you can perform a rudimentary&nbsp;<code>text.split()</code>&nbsp;operation to obtain words or sentences,&nbsp;create&nbsp;chunks based on word count up to a specified maximum, or use&nbsp;<code>re.split(r’\n(?=[A-Z])’)</code>&nbsp;to&nbsp;separate paragraphs. Count the words and create additional paragraphs as needed to ensure that&nbsp;paragraphs don’t exceed the maximum size.&nbsp;You can use&nbsp;<a href=https://spacy.io/api/sentencizer>Spacy</a>&nbsp;or&nbsp;<a href=https://www.nltk.org/api/nltk.tokenize.html>NLTK</a>&nbsp;for sophisticated sentence segmentation&nbsp;instead of our naive regex or word split only. Alternatively, you can use&nbsp;token-based length calculations&nbsp;with a function like&nbsp;<a href=https://github.com/openai/tiktoken>OpenAI’s tiktoken</a>.</p> <p>However, this&nbsp;approach may result in incomplete paragraphs or sentences, potentially losing context among the various segments&nbsp;ultimately stored in the database.</p> <p><strong>Overlapping segments</strong> To mitigate the issue of incomplete paragraphs or context loss among various document segments, we can&nbsp;employ a&nbsp;technique that generates overlapping segments. For instance, you could&nbsp;overlap at least five segments together. Utilizing the mentioned method, create segments and overlap a minimum of five paragraphs or segments into one chunk until the maximum size is reached. Repeat this process to cover overlapping content across the five segments.</p> <p>While&nbsp;this approach helps preserve context among segments, it may also substantially increase costs, as more embeddings will be generated. But this&nbsp;approach does&nbsp;help to some context with the problem of losing the context among segments.</p> <p><strong>Choosing the right tools for storing and querying vectors</strong> The landscape of tools for vectors and vector databases has grown rapidly. The choice truly depends on which tool aligns best with your specific objectives. Key considerations include whether you require&nbsp;a self-hosted solution or a managed vector store. Additionally, if opting for a hosted solution, it’s crucial to check if it satisfies your compliance requirements, particularly if you’re&nbsp;operating in a heavily regulated environment like healthcare or fintech.</p> <p>We’ve tried the following&nbsp;tools and&nbsp;vector databases:</p> <ul> <li><a href=https://github.com/facebookresearch/faiss><strong>FAISS</strong></a>, as&nbsp;defined by Facebook, is an open-source library designed&nbsp;for efficient similarity search and clustering of dense vectors. It’s&nbsp;not a vector database, but it&nbsp;supports popular algorithms such as k-NN and nearest neighbor search. The API is user-friendly and straightforward.&nbsp;However, it’s important to note that you’ll need to host FAISS independently on a GPU or server yourself.</li> <li><a href=https://www.pinecone.io/ ><strong>Pinecone</strong></a>, fully&nbsp;managed vector database&nbsp;that has gained considerable popularity recently. It&nbsp;supports k-NN and other distance metrics such as cosine, dot product, and Euclidean distance, which are easily configurable via their user interface. Pinecone’s high availability is a strong selling point, but&nbsp;its&nbsp;metadata search functionality, which sets it apart from many other tools, is similar to MongoDB-style JSON queries.<br> This feature enables not only vector-based search but&nbsp;also filtering of metadata JSON, for example, by user ID or specific attributes. Currently,&nbsp;we’re uncertain about their support for compliance standards such as PCI-DSS and HIPAA.</li> <li><strong>Managed</strong>&nbsp;<a href=https://aws.amazon.com/opensearch-service/ ><strong>OpenSearch</strong></a>&nbsp;by&nbsp;AWS&nbsp;is another viable option that&nbsp;supports vector querying and popular algorithms like k-NN. OpenSearch could be an efficient alternative if you’re operating in a regulated environment and prefer not to manage FAISS GPU instances on your own.</li> </ul> <p>The list of semantic search-based retrieval tools provided here is not exhaustive. I have shared it based on our experiences and the outcomes of our evaluations, providing insights into the tools we’ve tried and tested.</p> <blockquote> <p>I’d also like to note that a&nbsp;semantic search-based retrieval system isn’t the only viable solution.&nbsp;k-NN might not be correct algorithm always, maybe all you need is&nbsp;approximate search a-NN. Depending on your needs, other methodologies may prove more effective. For instance, if your task involves comparing two documents or pieces of information, alternative strategies may be better suited.</p> <p>Perhaps you’re&nbsp;looking to model a&nbsp;<a href=https://arxiv.org/pdf/2305.04676.pdf>network or knowledge graph</a>, or us a graph database such as Neo4J. Or, maybe your task involves&nbsp;querying a website for specific information. e.g&nbsp;<a href=https://pypi.org/project/duckduckgo-search/ >DuckDuckGo</a>&nbsp;library for search functionality&nbsp;or even a&nbsp;proprietary API returning JSON within your organization.</p> <p>The key is to select the approach that best aligns with your specific objectives, and then feed the information to LLM for response generation and reasoning.</p> </blockquote> <h2 id=zero-shot-vs-few-shot-prompting-and-steering-llms><strong>Zero-shot vs few shot prompting and steering LLMs</strong><a class=headerlink href=#zero-shot-vs-few-shot-prompting-and-steering-llms title="Permanent link">¶</a></h2> <p>The primary distinction between zero-shot and few-shot learning lies in the&nbsp;approach to inference.&nbsp;In&nbsp;zero-shot learning, the language model is asked to&nbsp;complete a task without explicit examples, whereas, in&nbsp;few-shot learning, the&nbsp;model is provided with a limited number of examples, or “few-shots,”&nbsp;embedded within the prompt.&nbsp;Prompts are very important to steer the model in a direction and avoid hallucinations.&nbsp;Besides prompts, the&nbsp;temperature setting for the LLM also plays an important role.</p> <p>Consider the following example: We have a QnA bot tasked to&nbsp;answer questions from the provided context retrieved using a vector store and user’s question query. We want the LLM to only answer based on the context.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>prompt_prefix</span> <span class=o>=</span> <span class=s1>'''  </span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=s1>Answer only from the content that have been provided to you in the context,  </span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=s1>other wise reply "I cannot answer". You are a QnA assistant</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=s1>Context:  </span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=si>{retrieved_context_from_vector_store}</span><span class=s1>  </span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=s1>'''</span>
</span></code></pre></div> <p><strong>Zero-shot prompting example combines with context as a prefix</strong><br> In this example, we&nbsp;won’t give any explicit or prior examples of LLMs answering the question. Usually, you get good performance on zero-shot tasks as well&nbsp;unless you want LLM to answer questions in a particular manner</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=n>zero_shot_prompt</span> <span class=o>=</span> <span class=n>prompt_prefix</span> <span class=o>+</span> <span class=s1>'''  </span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=s1>Question: </span><span class=si>{user_question}</span><span class=s1>  </span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=s1>Answer: '''</span>
</span></code></pre></div> <p><strong>Few-shot prompting example combined with context as a prefix</strong><br> If you aim to&nbsp;guide the model to respond in a specific format, employing&nbsp;few-shot prompting&nbsp;techniques could be beneficial. By&nbsp;providing a handful of examples demonstrating the desired response style.&nbsp;Remember,&nbsp;prompt crafting may require experimentation, and the optimal approach could vary depending on the particular Large Language Model (LLM) in use.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>few_shot_prompting</span> <span class=o>=</span> <span class=n>prompt_prefix</span> <span class=o>+</span> <span class=s1>'''  </span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=s1>Question: Who was the president in 2015?  </span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=s1>Answer: The name of the president is Barak Obama</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=s1>Question: Tell me a joke  </span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=s1>Answer: sorry this is something not mentioned in the context</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=s1>Question: &lt;some domain-specific-question&gt;  </span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=s1>Answer: &lt;domain specific style of answering a question&gt;  </span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=s1>'''</span>
</span></code></pre></div> <p><strong>A note on</strong>&nbsp;**tools and open-source frameworks**Tools like&nbsp;<a href=https://python.langchain.com/en/latest/index.html target=_blank>Langchain</a>&nbsp;and&nbsp;<a href=https://gpt-index.readthedocs.io/en/latest/ target=_blank>llama-index</a>&nbsp;are undoubtedly powerful, helping to reduce much of the labor involved in tasks like data loading, building indexes, and creating retrievers.&nbsp;They are good for getting started or for building personal projects.</p> <blockquote> <p>Remember, the following is a very subjective experience and your mileage may vary.</p> </blockquote> <p>From our perspective, using these libraries in production systems might not be the best fit. They tend to obscure the underlying workings through many abstractions, limit customization possibilities, complicate memory management and retrieval customization, and sometimes need to be revised to reason about.&nbsp;While working with LLMs is straightforward, we found&nbsp;creating a few Python classes to meet our objectives more practical rather than investing significant effort into mastering a new&nbsp;framework.</p> <p>The open-source ecosystem, particularly that of&nbsp;Langchain, is truly impressive. It offers a wealth of&nbsp;innovative ideas from the community that are worth exploring and integrating into your own projects. Also, data&nbsp;loaders they offer are worth using because it’s a lot of investment to write data scrapers or loaders or already existing tools yourself.&nbsp;Do check out&nbsp;<a href=https://llamahub.ai/ target=_blank>llama-hub</a>&nbsp;or&nbsp;<a href=https://python.langchain.com/en/latest/modules/indexes/document_loaders.html target=_blank>LangChain</a>&nbsp;data loaders.</p> <p>Also, do&nbsp;check out&nbsp;<a href=https://cohere.com/ target=_blank>Cohere</a>, or&nbsp;<a href=https://aws.amazon.com/sagemaker/jumpstart/getting-started/ target=_blank>Sagemaker Jumpstart models</a>. They both are great options to deploy models in your VPC if you work in a heavily regulated environment and want to use open-source LLM. We plan to do another article about how to deploy the model in a restricted environment and train it. If you want to use OpenAI models Microsoft&nbsp;<a href=https://azure.microsoft.com/en-us/products/cognitive-services/openai-service target=_blank>Azure OpenAI as a service</a>&nbsp;could also be a good option, although you will have to request access to it.</p> <h2 id=using-llms-for-recommendations-and-clustering-tasks><strong>Using LLMs for recommendations and clustering tasks</strong><a class=headerlink href=#using-llms-for-recommendations-and-clustering-tasks title="Permanent link">¶</a></h2> <p>LLMs can capture the semantic meaning of a text or any data type through embeddings or numerical vectors in high-dimensional spaces, enabling the correlation of data once embedded. As previously discussed, we'll apply a&nbsp;similar technique for embedding data to augment relevant response generation. Instead of generating responses, k-NN (nearest neighbours), a-NN, or any distance algorithms can be applied on vector stores,&nbsp;etc., such as clustering users based on similar actions, product preference, or any other correlations depending on your dataset and domain.</p> <p>LLM-generated clusters&nbsp;can produce more accurate categories than rule-based systems and potentially&nbsp;address cold start problems.&nbsp;They can work with&nbsp;noisy un-labeled data sets, which can help us create collaborative filtering or content-based recommendation systems.</p> <p>Deep learning-based recommendation systems, e.g., that&nbsp;embed user activity into vectors, have already been implemented at a petabyte of billions of actions scales by platforms like&nbsp;<a href=https://arxiv.org/abs/2007.03634 target=_blank>Pinterest</a>&nbsp;and&nbsp;<a href=https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf target=_blank>YouTube</a>. However, it’s worth mentioning that&nbsp;embedding techniques have significantly improved over time, making it easier to capture semantic meaning in data, and&nbsp;with the advent of new vector query/storage tools,&nbsp;it has become easier to deploy a recommendation system compared to what it used to be five years ago.</p> <p><a href=https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb target=_blank>This OpenAI Cookbook</a>&nbsp;is an excellent resource to help you get started and understand the implementation process. To create embeddings, you&nbsp;don’t have to rely solely on OpenAI embeddings model.&nbsp;You can use any LLM, such as&nbsp;<a href=https://github.com/google-research/bert target=_blank>BERT</a>&nbsp;or any open-source model,&nbsp;<a href=https://github.com/UKPLab/sentence-transformers target=_blank>sentence transformer</a>, etc.,&nbsp;that effectively captures the semantic meaning of your data.</p> <p>Following is the image generated using&nbsp;<a href=https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb target=_blank>OpenAI cookbook</a>&nbsp;mentioned above on this&nbsp;<a href=http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html target=_blank>data set</a>. However,&nbsp;these are high-dimensional embeddings, which are hard to visualize. They have used&nbsp;<a href=https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding target=_blank>t-SNE</a>&nbsp;to&nbsp;compress them into two-dimensional space to visualize the nearest neighbor recommender concept.</p> <!-- more --> <p><a class=glightbox data-type=image data-width=auto data-height=auto href=https://miro.medium.com/v2/resize:fit:1200/1*dWLTkNfrnFJ0StOTiwoX0g.png data-desc-position=bottom><img src=https://miro.medium.com/v2/resize:fit:1200/1*dWLTkNfrnFJ0StOTiwoX0g.png title alt data-align=center></a></p> <p>Article description clusters generated using OpenAI’s notebook</p> <p>Having said all of the above,&nbsp;you may need to integrate your business logic and other rules to&nbsp;create an effective recommendation system to develop a production-grade system.&nbsp;It’s crucial to consider the diversity of the data and rigorously test the system using both online and offline evaluation&nbsp;metrics.&nbsp;Online testing may involve A/B testing tailored to your domain, while offline metrics include measuring precision, recall, F1 scores, and assessing the data’s diversity.</p> <p>Thinking about&nbsp;collaborative filtering vs content-based models&nbsp;and modeling user sessions etc., do add more complexity, and it’s yet unclear how LLMs alone can solve that. Striking the right balance between these factors is essential for a successful recommendation system.</p> <h2 id=improving-search-experiences-within-organizations>Improving search experiences within organizations<a class=headerlink href=#improving-search-experiences-within-organizations title="Permanent link">¶</a></h2> <p>There’s not much more to add here, but it’s worth emphasizing that the above-mentioned embedding techniques, when paired with LLMs, can significantly enhance an organization’s search experience. Moreover, the&nbsp;<a href=https://arxiv.org/abs/2005.11401 target=_blank>RAG</a>&nbsp;approach can enable a more descriptive and user-friendly presentation of search results.</p> <p>If your interest goes beyond just LLMs, I’d recommend this fascinating read from Pinecone explaining how&nbsp;<a href=https://www.pinecone.io/learn/spotify-podcast-search/ target=_blank>Spotify enhanced their podcast search experience</a>. The concepts aren’t new here, but the barrier to entry for creating superior experiences with less engineering effort has markedly decreased, thanks to LLMs and the new embedding techniques they unlock.</p> <h2 id=thinking-about-the-niche-applications-all-of-the-above-unlocks><strong>Thinking about the niche applications all of the above unlocks</strong><a class=headerlink href=#thinking-about-the-niche-applications-all-of-the-above-unlocks title="Permanent link">¶</a></h2> <p>Identifying the precise potential of Large Language Models (LLMs) for a particular organization is difficult; it largely&nbsp;hinges on the unique datasets and the domain expertise for a niche.&nbsp;For instance, a healthcare specialist might find a distinct set of use cases for LLMs, whereas a professional in fintech could leverage the technology in an entirely disparate manner.</p> <p>This broad applicability and adaptability is a key aspect that makes the LLM ecosystem so vibrant. For&nbsp;organizations with extensive data assets, the&nbsp;prospective advantages of deploying LLMs outweigh the associated risks, provided they are applied with caution and checks in place. We have previously discussed some use cases and their respective implementations.</p> <p>In addition, here are a few more worthy of mention:</p> <ul> <li>QnA and customer service chatbots:&nbsp;I still remember the days of NLP pre-transformers, BERT or any LLM, using scikit-learn, Keras or sometimes Regex for intent recognition, using&nbsp;Spacy/duckling for NER, and how hard it was to create domain-specific chatbots even with amazing frameworks like RASA.&nbsp;And it does not matter what you do. You still used to get compromised UX for the end users. Companies will have an opportunity to refine their customer service using LLMs. UX is so much better with LLMs. Probably the best conversation humans had with the silicon chip.</li> <li>Corporate brain:&nbsp;<a href=https://www.glean.com/ target=_blank>Glean kind</a>&nbsp;of product built for all the organization's Wiki and internal documents, except compared to Glean, it’s better since it can reason and generate better responses.</li> <li>Zero-ETL use case.&nbsp;I found this article by&nbsp;<a href=https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c target=_blank>Bar Moses really interesting</a>. As LLMs become more powerful, seeing&nbsp;how they disrupt data processes, massive ETL pipelines, and processes in place will be interesting.&nbsp;Also, this&nbsp;<a href=https://arxiv.org/abs/2303.06748 target=_blank>research paper is very interesting</a>&nbsp;if you want to read it. There still need to be more questions regarding creating&nbsp;production-grade pipelines and the necessary measures to test them effectively.</li> <li>Data labeling and&nbsp;<a href=https://proceedings.mlr.press/v206/hegselmann23a.html target=_blank>classification</a>&nbsp;at scale.&nbsp;If you are interested,&nbsp;<a href=https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb target=_blank>there is an interesting Jupyter Notebook</a>&nbsp;from&nbsp;OpenAI on classification tasks through the model and comparing the results.&nbsp;Also,&nbsp;Cohere has an interesting endpoint for a similar use case.</li> <li>Specialized CoPilots:&nbsp;Drawing inspiration from GitHub Copilot, there is potential for creating specialized CoPilots for various professions, including healthcare workers, financial analysts, and more.</li> <li>Summarizing content.&nbsp;Summaries are good, although they did&nbsp;<a href=https://arxiv.org/abs/2301.13848 target=_blank>drop a lot of relevant information</a>&nbsp;when we tried using them. There is substantial room for enhancement in this area.</li> <li>Capitalizing on untapped, unstructured data: Many organizations possess vast datasets but are still determining how best to utilize them. The&nbsp;advanced embedding techniques that LLMs offer can assist these organizations in clustering, reasoning, and making nuanced use of their data.&nbsp;Techniques like&nbsp;RAG further enable them to operationalize this data for internal processes or end users.</li> <li>Boosting developer or employee productivity:&nbsp;LLMs can undeniably enhance developer efficiency, often called the ‘10X’ effect. However, organizations must proceed cautiously, taking lessons from incidents&nbsp;<a href=https://www.bloomberg.com/news/articles/2023-05-02/samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak target=_blank>like the one with Samsung.</a>&nbsp;This is where internal LLMs deployed within an organization’s&nbsp;Virtual Private Cloud (VPC)&nbsp;could offer a valuable safeguard. It presents a compelling case for enterprises to deploy their own LLMs. This topic remains a lively debate, and seeing the industry's direction will be intriguing.</li> </ul> <p>The next article will explore building long-term user action memory and routing techniques for information retrieval. Hang on! I think I’ll update this bit later, depending on which topic I manage to publish first.</p> <h2 id=references><strong>References</strong><a class=headerlink href=#references title="Permanent link">¶</a></h2> <ul> <li><a href=https://arxiv.org/abs/2302.00083 target=_blank>In-Context Retrieval-Augmented Language Models</a></li> <li><a href=https://arxiv.org/abs/2005.11401>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li> <li><a href=https://cdn.openai.com/papers/Text_and_Code_Embeddings_by_Contrastive_Pre_Training.pdf target=_blank>Text and Code Embeddings by Contrastive Pre-Training</a></li> <li><a href=https://arxiv.org/abs/2303.06748 target=_blank>An Example-Driven Tabular Transformer by Leveraging Large Language Models</a></li> <li><a href=https://arxiv.org/pdf/2210.11610.pdf target=_blank>Large Language Models can self improve</a></li> <li><a href=https://arxiv.org/abs/2007.03634 target=_blank>PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest</a></li> <li><a href=https://arxiv.org/abs/2304.11116 target=_blank>Language Models as Recommender Systems: Evaluations and Limitations</a></li> </ul> <div class=subscribe-container> <h3>Subscribe</h3> <p>Honest takes on AI, startups, and digital health—delivered to your inbox.</p> <p>Your privacy is paramount. Expect content once or twice a month. Unsubscribe anytime if you don't like it.</p> <a href=https://newsletter.hadijaveed.me class="md-button md-button--primary" target=_blank>Get Email Updates</a> </div> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../../2022/10/04/using-nudges-to-reinforce-health-behaviors/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Using Nudges to Reinforce Healthy Behaviors"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Using Nudges to Reinforce Healthy Behaviors </div> </div> </a> <a href=../../../../2024/03/05/tracing-and-observability-in-llm-applications/ class="md-footer__link md-footer__link--next" aria-label="Next: From Concept to Production with Observability in LLM Applications"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> From Concept to Production with Observability in LLM Applications </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> © 2024 <a href=https://www.linkedin.com/in/hadijaveed target=_blank rel=noopener>Hadi Javeed</a> </div> </div> <div class=md-social> <a href=https://github.com/hadijaveed target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"></path></svg> </a> <a href=https://twitter.com/hadijpk target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"></path></svg> </a> <a href=https://www.linkedin.com/in/hadijaveed target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../../../..", "features": ["toc.integrate", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy", "announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.top", "navigation.instant", "navigation.instant.prefetch", "navigation.expand", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.footer", "navigation.tabs", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../../assets/javascripts/bundle.79ae519e.min.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>