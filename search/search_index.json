{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Following are my personal thoughts on tech, AI, startups and adoption of AI in Health-Care. You could read more about me here</p> <p>Get Email Updates</p>"},{"location":"#blog-thoughts","title":"Blog / Thoughts","text":""},{"location":"about/","title":"About Me","text":""},{"location":"about/#about-me","title":"About Me","text":"<p>With over a decade of experience in software development, I've had the privilege of working on diverse and impactful projects. As the CTO and Co-Founder of Vincere Health (acquired by Red Ventures Optum in 2023), I led the development of a behavior change platform for patients struggling with addiction which seamlessly integrates fintech solution to reward users with financial incentives for quitting, alongside an AI-powered chatbot that drives behavior change.</p> <p>Earlier, I worked at Capital One, contributing to the development of voice-enabled NLP chatbots for IVR systems as part of their telephony innovation team.</p> <p>I am the CTO and Co-Founder of RevelAI Health, where we\u2019re using generative AI to transform orthopedic care\u2014streamlining recovery journeys and improving clinical outcomes through intelligent automation.</p> <p>In the post-ChatGPT era, I also launched Prodify, a boutique AI product studio focused on strategic consulting, rapid prototyping, and compliance support. We've partnered with fast-growing startups across healthcare and adjacent sectors, built a 10-person engineering team, scaled to 7-figure revenue, and helped founders bring GenAI ideas to life. While I now serve in an advisory capacity, my partner leads engineering and day-to-day operations at Prodify.</p> <p>You can reach out to me here.</p>"},{"location":"thoughts/","title":"Thoughts","text":"<p>Following are my personal thoughts on tech, AI, startups and adoption of AI in Health-Care. You could read more about me here</p>"},{"location":"thoughts/#thoughts","title":"Thoughts","text":""},{"location":"future/benchmarking-strucutred-data-extraction/","title":"Benchmarking LLM performance on structured data extraction","text":""},{"location":"future/benchmarking-strucutred-data-extraction/#benchmarking-llm-performance-on-structured-data-extraction","title":"Benchmarking LLM performance on structured data extraction","text":"<p>Lot of true enterprise value lies in un-strucuted data</p>"},{"location":"future/from-surfacing-actions-to-taking-actions/","title":"From surfacing actions to taking actions","text":""},{"location":"future/learning-epic-and-fhir/","title":"Journey of learning integrations with Epic and FHIR","text":""},{"location":"future/learning-epic-and-fhir/#journey-of-learning-integrations-with-epic-and-fhir","title":"Journey of learning integrations with Epic and FHIR","text":"<p>In past I've used middlewares like Redox and Xealth to connect custom developed health-care applications. I decided to learn how to connect to Epic and FHIR directly and document my journey here.</p> <p>Just like there are so many developers out there, who have no idea how to connect to Epic, learn FHIR and get up-to speed with the integration. I hope this blog serve as a purpose.</p>"},{"location":"future/learning-epic-and-fhir/#medplum","title":"Medplum","text":"<p>Medplum has excellent documentation about FHIR, it can get you up-to speed with FHIR quickly. Listening to interoperability guides from Medplum. Excellent resource. Shout out to the medplum team.</p>"},{"location":"future/learning-epic-and-fhir/#next-steps","title":"Next Steps","text":"<ul> <li>Go sign-up into Fhir Developer and create an account.</li> <li>Learn about Epic's open endpoints for testing here.</li> <li>Select developer on this page to access different resources.</li> <li>Medplum resource to learn: Medplum Github Resource.</li> </ul>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/","title":"From Concept to Production with Observability in LLM Applications","text":""},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#from-concept-to-production-with-observability-in-llm-applications","title":"From Concept to Production with Observability in LLM Applications","text":"<p>Understanding observability in AI applications, particularly in Large Language Models (LLMs), is crucial. It's all about tracking how your model performs over time, which is especially challenging with text generation outputs. Unlike categorical outputs, text generation can vary widely, making it essential to monitor the behavior and performance of your model closely.</p> <p>Imagine you're developing an application tailored to a specific use case. Perhaps you're enhancing an LLM with an external corpus through techniques like RAG (Retrieval-Augmented Generation)  or interfacing with a database API to process unstructured text. By leveraging relevant snippets retrieved in this way, you aim for your model to generate useful outputs. With the advent of tools like LangChain and LLamaIndex, alongside embedding models, building such systems has become more straightforward. Your development team's initial reaction might be overwhelmingly positive, but the real challenge emerges when transitioning from a development to a production environment. How do you ensure the accuracy and reliability of your system in real-world scenarios?</p> <p>The Evolution of Chatbots and Classification Applications with LLMs</p> <p>As LLMs grow increasingly accessible, many teams are venturing into creating innovative applications. An approach might involve using a document corpus to develop a RAG pipeline tailored to your domain-specific data. Thanks to open-source libraries, assembling these applications has become significantly easier.</p> <p>You might employ a prompt to enhance the retrieval workflow, utilizing content from a vector store. Given the impressive reasoning capabilities of LLMs, such applications can provide substantial value depending on the use case, earning you accolades from your team for swift development.</p> <p>Navigating the Challenges of Production Readiness</p> <p>When your application is in staging, and your team begins to use it extensively, you're on the cusp of deploying it in a production environment. This stage brings about a critical question: How do you measure the accuracy and performance of your application? LLM-generated responses introduce a high degree of subjectivity, rendering traditional unit tests inadequate. Developing robust test cases becomes imperative, ensuring they're revisited with every modification to the application, be it in the prompts or any other component.</p> <p>Investing in LLM Operations: A Necessity, Not a Choice</p> <p>Collecting comprehensive and well-rounded feedback can seem daunting. Human annotation and labeling, while valuable, are often costly. A practical first step involves identifying 50-100 common queries and patterns. Collaborate with your team or subject matter experts to craft ideal responses for these scenarios.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#llm-response-evaluation","title":"LLM Response Evaluation","text":"<p>An effective strategy is to conduct automated evaluations based on these ideal responses. The more diverse and case-specific your questions are, the better. Even a small subset of questions reviewed by human labelers can provide invaluable insights.</p> <p>Consider implementing an evaluation pipeline similar to RAGAS, focusing on metrics like faithfulness and answer relevancy:</p> <ul> <li>Faithfulness: Assess whether the LLM creates outputs based solely on the provided content, avoiding hallucinations.</li> <li>Relevancy: Evaluate how the LLM's responses align with the user's questions.</li> </ul> <p>Beyond these metrics, you might explore additional measures tailored to your specific use case. Regularly running these automated tests, especially after updates to your RAG strategy or prompts, can significantly enhance your application's reliability. Incorporating these tests into your continuous deployment process can further streamline operations.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#tracing-and-collecting-spans-insights-into-execution","title":"Tracing and Collecting Spans: Insights into Execution","text":"<p>Drawing inspiration from OpenTelemetry, Traces\u00a0give us the big picture of what happens when a request is made to an LLM application. Traces are essential to understanding the full \u201cpath\u201d a request takes in your application, e.g, prompt, query-expansion, RAG retrieved top-k document, functional call and other mechanisms incorporated into your LLM application are represented as SPAN under one trace. Spans represent individual unit of work or operation e.g, vector store call, functional call or others.</p> <p>Understanding the intricacies of your LLM's performance is vital. Perhaps the issue isn't with the LLM itself but with the RAG component or a lack of relevant data in your corpus. Identifying the root cause of errors or hallucinations requires comprehensive traceability of your application's execution paths.</p> <p>Example: Leveraging Langsmith SDK for Enhanced Observability</p> <p>Selecting the right tools for tracing can dramatically affect your operational efficiency. While custom solutions are possible, they often require substantial effort to implement correctly. Tools like Langchain provide abstractions that facilitate tracking multiple execution units, types, and attributes without overcomplicating your codebase, allowing you to focus more on product development and less on operational infrastructure. </p> <pre><code>class LLMTracer:\n    def __init__(self, inputs: dict, meta: dict = {}):\n        self.pipeline = RunTree(\n            run_type=\"chain\",\n            name=\"&lt;your-application&gt;\",\n            inputs=inputs,\n            extra=meta,\n        )\n\n    def add_log(self, name: str, inputs: dict = {}, outputs: dict = {}, run_type: str = \"llm\"):\n        log = self.pipeline.create_child(\n            name=name,\n            run_type=run_type,\n            inputs=inputs,\n        )\n        log.end(outputs=outputs)\n        log.post()\n\n    def final(self, outputs: dict = {}):\n        self.pipeline.end(outputs=outputs)\n        self.pipeline.post()\n\n\n## example usage in the code\ntracer = LLMTracer(inputs=messages, meta={\"model\": \"xyz\", **other })\n\n## add logs to the execution trace\ntracer.add_log(\"functional_call\", inputs={**function_inputs}, outputs={**function_outputs)})\n\n\n## RAG call results or something\ntracer.add_log(\"rag\", inputs={**rag_query}, outputs={**rag_docs_etc})\n\n\n## towards the end of execution, usually before sending the reply back or final output\ntracer.final(output={**your final message or the output})\n</code></pre> <p>Leveraging either their UI or SDK, you have the flexibility to meticulously select and integrate traces into your testing pipeline or direct them towards an annotation queue, especially if your process incorporates human labelers.</p> <p>The essence of this approach underscores the necessity of a robust tracing infrastructure within your LLM application, enabling the tracking of key metrics. It's imperative to monitor metadata associated with inputs, such as chunk-size, model, prompt, and the number of documents. The more comprehensive your tracking, the more accurately you can assess what aspects are performing optimally and which areas require refinement.</p> <p>The choice of tracing application remains a matter of personal preference. Options range from DataDog and OpenTrace to custom-built solutions using Clickhouse or Postgres. However, managed services like Langsmith or Arize offer distinct advantages, particularly in streamlining test case automation and facilitating the collection of annotations via human labelers.</p> <p>Also the traces and spans let you monitor your number of token usage, latency and pricing.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#sql-database-for-tracing","title":"SQL Database for tracing:","text":"<p>Although tools such as Arize and Langsmith offer impressive features, they are not indispensable. You can adopt a strategy to monitor events using a tabular format as shown below as long as your infrastructure can support it</p> query_id question response source_documents chunks mean_cosine_scores re_ranker_score metadata 1 user-question llm-response [docid1, docid2] [chunkid1, chunkid2] 0.855 0.8 {***json-based} 2 ... ... [] [] {****} <p>Additionally, you can track user feedback in a separate table, linked by query IDs. This feedback on whether responses were helpful or not can be instrumental in fine-tuning your model or overall understanding how model could be improved later</p> query_id user_id helpful response_suggestion response_critique 1 user-id yes/no re-written response user-feedback 2 ... ... ... ..."},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#prompt-management-versioning","title":"Prompt Management (Versioning):","text":"<p>Implementing an effective version control system for your prompts can significantly enhance your testing processes. Consider the scenario where you're conducting A/B tests between two different sets of prompts to determine which yields better performance. The ability to trace the evolution of your prompts\u2014reviewing every change to understand whether these modifications have led to improvements or declines in production performance\u2014can be incredibly valuable.</p> <p>The method you choose for managing version control of your prompts is entirely up to you. Langsmith offers a private hub that allows you to track versions of your prompts along with their input variables, although this might not significantly enhance your prompt management strategy. Personally, I prefer to maintain prompts within a code editor, typically in .txt or .py files, with a designated variable for version control, and organize them within a specific directory.</p> <pre><code>prompts/\n    system_prompt/\n            2023-12-28.txt\n            2024-01-12.txt\n\n    other_prompts/\n</code></pre> <p>However, managing prompts in this manner presents challenges, particularly within a production-grade system where updates necessitate a code deployment. An alternative approach involves utilizing the Langsmith hub. Regardless of the method, it's imperative to exercise extreme caution when updating prompts in production, ensuring thorough testing in a lower environment beforehand.</p> <pre><code>SYSTEM_PROMPT = hub.pull(f\"system_prompt:{prompt_versopm}\").template\n</code></pre> <p>Now, the version of the prompt must be retrieved from the configuration or your database at the time of inference. Alternatively, you can configure your system to always use the most recent version of the prompt.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#rag-evaluation","title":"RAG Evaluation:","text":"<p>When working with proprietary and domain-specific data in your LLM application, it's crucial to implement robust measures for evaluating your Retrieval-Augmented Generation (RAG) pipeline. A comprehensive RAG pipeline typically encompasses the following components:</p> <ul> <li> <p>An embedding model, which can be either proprietary or open-source. For benchmarking, consider the MTEB benchmark.</p> </li> <li> <p>A vector store, such as PgVector, Pinecone, or similar.</p> </li> <li> <p>A chunking strategy detailing how data is segmented and whether chunks overlap.</p> </li> <li> <p>Document ingestion processes to ensure clean data extraction from PDFs, HTML, or other formats.</p> </li> <li> <p>Metadata filtering to refine the embedding search space through specific criteria.</p> </li> <li> <p>A combination of hybrid search techniques or Reciprocal Rank Fusion, utilizing both dense and sparse vectors, possibly integrating a BM25 filter for enhanced keyword search.</p> </li> <li> <p>A re-ranker or cross-encoder to improve result relevance.</p> </li> <li> <p>Query expansion techniques for optimizing the search experience through query rewriting or extraction.</p> </li> </ul> <p>Initiating a RAG pipeline might seem straightforward, but constructing a production-grade, accurate system introduces numerous complexities. For instance, replacing an embedding model with one of a different dimensionality necessitates regenerating all embeddings, a task manageable within a new database or index. Similarly, modifications to chunking strategies, document parsing, or the implementation of Reciprocal Rank Fusion or metadata-based filtering raise questions about the efficacy of these changes.</p> <p>To address these challenges, it's essential to develop specialized, robust test cases focused on retrieval. Unlike end-to-end LLM evaluations, testing individual components of the RAG pipeline can provide insightful feedback. Recommended test cases include:</p> <ul> <li>Precision@K</li> <li>Recall</li> <li>Mean Cosine Scores and Re-ranker scores</li> </ul> <p>Tracking these metrics requires a collection of 100-200 diverse test cases tailored to your specific use case. Regular analysis of RAG results upon any modification or content addition is vital.</p> <p>Enhance your RAG pipeline evaluation by meticulously documenting metadata, such as top-k results, the size of the last chunk used, and the embedding model employed. The more metadata you track, the more nuanced your understanding of the pipeline's performance, facilitating targeted improvements.</p> <p>Embedding based cluster analysis:</p> <p>Utilizing HDBSCAN to segment embeddings into distinct inference groups can be instrumental in pinpointing segments of your embeddings that are underperforming or deviating from expected patterns. Similarly, UMAP can facilitate a deeper comprehension of how your embeddings capture semantic meanings in a format that's easy to visualize. ArizeAI offers an impressive 3D visualization tool that's worth exploring, although I haven't personally experimented with it yet.</p> <p>For those with access to production data, initiating a notebook to visualize and dissect embeddings can be enlightening. In this context, tools like Arize prove to be invaluable resources for such analytical endeavors.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#fine-tuning","title":"Fine-Tuning:","text":"<p>Details on fine-tuning metrics and their significance will be provided in the future. As of now, I have not systematically tracked these metrics. However, I am currently in the process of doing so and plan to share insights on the necessity and impact of fine-tuning a model based on my experiences. This section will be updated accordingly.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#conclusion","title":"Conclusion:","text":"<p>In the rapidly evolving domain of production-grade LLM applications, there exists no one-size-fits-all strategy. The current GPU shortage raises questions about the capability of mid to smaller sized companies to support workloads exceeding 1M+ QPS. However, it's anticipated that these capacity constraints will diminish over time. Nonetheless, for LLM operations of any scale, it's crucial to have robust operational components in place. These include diligent response monitoring and establishing benchmarks for accuracy, relevancy, and RAG metrics. Such measures empower developers to make informed modifications with confidence, supported by data, and to pinpoint precise areas where adjustments are necessary.</p>"},{"location":"2024/03/05/tracing-and-observability-in-llm-applications/#references","title":"References","text":"<ul> <li>RAG (Retrieval-Augmented Generation) </li> <li>RAGAS</li> <li>Langsmith</li> <li>UMAP &amp; HDBSCAN</li> <li>MTEB benchmark</li> <li>OpenTelemetry</li> <li>Cohere Rerank</li> </ul>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/","title":"Is Your Security Posture Holding Your Healthcare Startup Back?","text":""},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#is-your-security-posture-holding-your-healthcare-startup-back","title":"Is Your Security Posture Holding Your Healthcare Startup Back?","text":"<p>When deploying a healthcare product, HIPAA compliance is crucial. No matter how innovative your solution is, without convincing the CIO or security team, you won't get deployed. I view security and HIPAA posture as essential features of any healthcare product.</p> <p>I've successfully deployed healthcare products at large payors and health systems with stringent security requirements. Through my mistakes and successes, I've gained valuable insights that I want to share.</p> <p>From my observations, early-stage startups often fall into two camps: 1. Those who postpone security considerations, thinking \"we'll handle it closer to the pilot or contracting phase.\" 2. Those who overcompensate based on online advice, creating unnecessarily complex security architectures that become difficult to manage long-term.</p> <p>As a startup CTO or tech lead, you should focus on two main goals regarding HIPAA and security:</p> <ol> <li>Protecting patient data as a fundamental responsibility. Your product must address this seriously from the start.</li> <li>Developing a security posture that convinces healthcare organizations you can deploy quickly without getting bogged down in lengthy security risk assessments.</li> </ol> <p>Note</p> <p>This post focuses on technical advice for system architecture and navigating dated security risk assessment questionnaires. It doesn't cover legal aspects of compliance policies, BAAs, or insurance needs. Those topics deserve a separate discussion.</p> <p>Note</p> <p>While this advice primarily applies to AWS cloud tools, the concepts can be adapted to other cloud providers.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#architechting-the-cloud","title":"Architechting the Cloud","text":"<p>Here's an unpopular opinion: using too many SaaS tools can complicate your HIPAA compliance process. Stick to the essentials. Why? Each SaaS provider requires a BAA, often forcing you into pricier business tiers and expanding your attack surface.</p> <p>But what about the benefits of SaaS tools? They're undeniably useful. Nowadays, we have fantastic open-source alternatives that you can self-host. Take Metabase for reporting or Posthog for analytics, for instance. There's also a wealth of open-source observability and monitoring tools at your disposal. Cloud providers also offer managed services within your VPC, giving you the best of both worlds - convenience and control.</p> <p>Note</p> <p>Some might ask, \"Why bother with self-hosting?\" It's simpler than you think. Spinning up an EC2 or GCP VM isn't rocket science. You can easily run dockerized apps, set up automatic EBS volume backups, encrypt data, and manage patches with AWS SSM. This approach often takes less time and money than juggling multiple SaaS subscriptions. While scaling might become a challenge later, by that point, you'll likely have the resources for pricier SaaS options and a dedicated DevOps team.</p> <p>An air-gapped solution can be a game-changer. It gives you fewer servers or containers to manage and more control over security.</p> <p>Keep your infrastructure simple from the start. You don't need a plethora of SaaS tools to build something great. Leverage self-hosted and open-source options where possible.</p> <p>Some providers offer HIPAA-focused wrappers for cloud services, like Aptible. However, these often come with a hefty premium.</p> <p>What about platforms like Next.js (Vercel) or Supabase (Managed)? While they now offer HIPAA-compliant BAAs, convincing security auditors during risk assessments can still be tricky. Personally, I prefer self-hosting. Explaining how Next.js runs your code to an auditor can be a challenge I'd rather avoid. You can always self-host them on your own infra.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#how-to-answer-dated-security-risk-assessment-questionnaire","title":"How to answer dated security risk assessment questionnaire","text":"<p>Let's go through some sample questions often asked in the risk assessment process and evaluated by IT teams:</p> <p>How do you patch your servers? Lol. We don't do it; AWS does it for us. Wait, what do you mean? Or hey, we're on serverless. Auditors be like, \"There's gotta be some server somewhere running your code.\" Remember, these questions are dated and still follow an on-prem mindset. If using AWS EC2, you can demonstrate patches with AWS SSM, or demonstrate ECS Fargate patching, or share documentation around how Lambda layers are patched.</p> <p>Some example questions below...</p> <ul> <li>What's your disaster recovery plan?</li> <li>Do you have a firewall? Deny all traffic by default?</li> <li>What's the intrusion detection and prevention?</li> <li>What are the password requirements? Do you lock out on failed attempts, have inactivity timeouts, etc.?</li> </ul> <p>Above are all the good ones, then there are the questions about physical server locations, employee training/access which is not covered in this post.</p> <p>Following are the components you need to consider for your architecture</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#network-isolation","title":"Network Isolation","text":"<p>Network isolation is a critical first step for protecting patient data. You need to properly isolate your resources using VPCs and Subnets - this isn't optional.</p> <p>Here's what you need to know: Never expose sensitive resources like databases and caches directly to the internet. Put them in private subnets where they're shielded from external access. AWS has made this really straightforward with their VPC and subnet creation tools.</p> <p>For your architecture: - Place your API containers and servers in private subnets - Only expose them through public subnets using an API Gateway, Load Balancer, or Reverse proxy - Use security groups to strictly control communication between subnets, opening only required ports and protocols</p> <p>I've seen many teams skip proper network isolation or accidentally expose databases to the internet. This is a major security risk that will immediately raise red flags in any security assessment. Don't make this mistake - be rigorous about your network architecture from day one.</p> <p></p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#encrypting-in-rest-and-transit","title":"Encrypting in Rest and Transit","text":"<p>Needless to say, use HTTPS for all web communication. Use TLS certs for communicating to DBs, caches, etc. It's very easy to set up.</p> <p>For data at rest, make sure you're encrypting DB backups, S3 ideally or GCP cloud management services. At the very least, use SSE (S3) or AES256 encryption.</p> <p>For development, migration DBs could be accessed from local using either VPN (OpenVPN on EC2 is cheap and easy) or use a bastion host. Spin up a tiny EC2 with appropriate security group. Instead of using SSH keys, use AWS Session Manager. It's easy to give short-lived credentials to your developers. Or if you want to use managed VPN Tailscale is a good option.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#least-privilege-model-and-secrets-management","title":"Least Privilege Model and Secrets Management","text":"<p>When it comes to security, less is more. Implement the principle of least privilege to minimize potential attack surfaces:</p> <ul> <li>Use IAM roles for your servers and containers to access AWS services. This eliminates the need to expose access keys directly.</li> <li>Leverage AWS Organizations to manage accounts and services across different environments (DEV, TEST, PROD). This gives you granular control over access and resources.</li> <li>Never, ever write secrets directly into your codebase or container images. It's a recipe for disaster. Instead, use AWS Parameter Store or AWS/GCP Secrets Manager. These services make it easy to store and manage sensitive information securely.</li> </ul>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#egress-traffic-filtering","title":"Egress traffic filtering","text":"<p>This is something often overlooked. If you've a web application server, you can use Fully Qualified Domain Name (FQDN) filtering to allow traffic to certain domains and block all others. This is a good approach to limit the data leakage from your API.</p> <p>With NPM packages or Python packages, they bring a lot of dependencies. There is risk where these dependencies expose your API data to the internet, and you might be exposing your server to the internet without knowing. NAT gateway alone won't help here. AWS Route 53 Resolver DNS Firewall can help here. It's super easy to set up and manage, and you can create reports/monitoring with AWS CloudWatch. Or, if you're ambitious, you can deploy FQDN yourself with NAT, though this isn't recommended for most cases. During security reviews, there have been multiple sites that have asked these questions.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#penetration-testing","title":"Penetration Testing","text":"<p>Involve a decent penetration testing company to test your application for vulnerabilities. This is a good exercise to find out if you have any undiscovered vulnerabilities. Plus you get a certificate to demonstrate that you have done a penetration testing.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#data-retention-and-deletion","title":"Data Retention and Deletion","text":"<p>Retaining CloudWatch logs, retaining DB backups, retaining observability tracing data, retaining API access logs, etc. This is something you can set up. Depending on your policy and data sensitivity, you can set up different retention policies. This will come up in risk assessment questionnaires.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#intrusion-detection-and-prevention","title":"Intrusion Detection and Prevention","text":"<p>This is a tricky one to start with. Because every risk assessment questionnaire will assess you about intrusion detection and prevention.</p> <p>There are managed services at least in AWS, particularly for web applications, using AWS WAF / AWS Shield can do a lot of things for prevention. You can integrate AWS WAF very easily either in API Gateway or in ALB. You can use rules to block/allow with pretty easy setup such as preventing SQL injections, cross-site scripting, bad bots, etc.</p> <p>For intrusion detection, you can use AWS GuardDuty, to monitor all the resources, set up alerts and demonstrate that you have it enabled.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#disaster-recovery","title":"Disaster Recovery","text":"<p>Disaster recovery is a crucial aspect that often comes up in security questionnaires. They'll ask about your disaster recovery plan or request details of a DR exercise. Don't let this intimidate you - with AWS, it's more manageable than you might think.</p> <p>AWS availability zones are a good start. You can set up multi-region resources, create DB replicas with RDS, and implement multi-region S3 replication. However, the database often becomes the bottleneck when documenting your failover exercise.</p> <p>Most auditors still have an on-premises datacenter mindset. You'll need to educate them on how availability zones work. Be prepared to explain and demonstrate your recovery time objectives (RTO) and recovery point objectives (RPO).</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#frontend-considerations","title":"Frontend Considerations","text":"<p>Make sure you use secure https cookies, implement in-activity timeout, lockout on failed attempts. Implement MFA etc. Lot of these could be implemented with any decent web auth libraries without too much overhead. For out of box solution, Auth0 can provide these or open-source self-hosted solutions like Keycloak or Zitadel. If you have a web application, auditors will ask about frontend security.</p>"},{"location":"2024/10/23/is-your-security-posture-holding-your-healthcare-startup-back/#ending-notes","title":"Ending Notes","text":"<p>The above might seem overwhelming, especially for a young company, but in reality, it's manageable. Cloud providers like AWS simplify the process. CTOs often get intimidated by these implementations, but it's not too hard, honestly. You have CDK scripts open-sourced by the Medplum team and many other templates to build your stack from scratch. Implementing VPCs/Subnets, encrypting data, and adopting a least privilege model isn't difficult.</p> <p>Remember, no matter how robust your application is, you won't get deployed until you convince the IT risk assessment team. No approval means no revenue.</p> <p>I've also seen CTOs and tech leads make the mistake of hiring a fractional CISO or a dedicated DevOps person too early. The reality is that tech leads with basic knowledge can learn and implement these measures using tools provided by AWS/GCP, particularly AWS in my experience. It doesn't take much time to do this. </p> <p>By learning security from the ground up, as a CTO or tech lead, you'll be comfortable answering these questions. Remember, you have a duty to protect patient data rather than completely delegating it to a fractional person or third party. Additionally, many of the measures mentioned above will help you achieve SOC2 Type 2 compliance faster with tools like Vanta or Drata. You can use security compliance policies from these tools and let them monitor your cloud infrastructure. The next frontier will be HITRUST, but it's often not required upfront and can be addressed in later stages of the company.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/","title":"AI Gave Engineers 20 Hours Back Per Week. Why Aren't We Shipping Faster?","text":""},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#ai-gave-engineers-20-hours-back-per-week-why-arent-we-shipping-faster","title":"AI Gave Engineers 20 Hours Back Per Week. Why Aren't We Shipping Faster?","text":"<p>AI has compressed time in my life. This time compression has unlocked a lot, but perhaps not in the ways you'd expect.</p> <p>Nothing you don't already know, right?</p> <p>The Productivity Gains Are Real. The time savings compound in ways that still surprise me:</p> <ul> <li>Code Generation: With Cursor/Claude Code, what took 2 hours now takes 20 minutes</li> <li>UI Prototyping: V0/Loveable let me skip Figma entirely, concept to interactive UI in under an hour</li> <li>Documentation: PRDs, technical specs, API docs, all generated and refined 10x faster</li> <li>Research: Competitor analysis, technical architecture decisions, library evaluations, compressed from days to hours</li> </ul>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#where-the-compressed-time-goes","title":"Where the Compressed Time Goes","text":"<p>That reclaimed time splits into two buckets that matter deeply to me:</p> <p>As a Father:</p> <ul> <li>I pick up my son from pre-school</li> <li>We play soccer in the backyard</li> <li>I'm actually present during bedtime, not mentally debugging production issues</li> </ul> <p>As a Technical Founder:</p> <ul> <li>I spend real time with customers, understanding their actual problems</li> <li>I dive deeper into GTM strategies instead of just shipping features</li> <li>I think more about users and less about implementation details</li> <li>I can afford to experiment with ideas that might fail</li> </ul> <p>The founder benefits are transformative. When you're not drowning in implementation details, you can actually think about the business.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#the-intelligence-paradox","title":"The Intelligence Paradox","text":"<p>But here's the uncomfortable truth: AI hasn't made me smarter. It's made me faster, but getting to production still takes forever.</p> <p>Let me break down the reality:</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#testing","title":"Testing","text":"<p>Unit tests are fine, AI can write those well. But end-to-end test cases with multiple systems integrating together? That's where things fall apart. Some might call it a skill issue or context engineering problem, but I might as well write the test cases myself. The mental model required to verify complex integrations still lives entirely in my head.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#business-logic-limitations","title":"Business Logic Limitations","text":"<p>The code quality for business logic is... okay. Just okay. It's functional but rarely optimal. I still need to create mental proofs to verify it works in the broader system context. The amount of time it takes me to write a prompt with all the context is a lot, might as well start writing code. I'm proficient in writing code, so I just start doing it. What I'm trying to say is that putting thoughts from my head into code is faster than giving AI all the context in a prompt.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#design-coherence-gap","title":"Design Coherence Gap","text":"<p>V0 and Loveable are incredible for prototyping, but the design language is all over the place. Components don't speak the same product language. There's a lack of reusability that makes production deployment painful. You get a working prototype fast, but making it production-ready requires manual, thoughtful work.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#the-over-engineering-problem","title":"The Over-Engineering Problem","text":"<p>AI-generated PRDs tend to over-specify. Engineers end up more confused, not less. O3 and Gemini do a decent job, but they still need heavy editing to provide actual clarity. It's like having an eager intern who writes novels when you need haikus.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#performance-at-scale","title":"Performance at Scale","text":"<p>I'm still spending significant time optimizing data models, applying the right indexes, dealing with scale issues. AI can suggest indexes, sure. But understanding query patterns, anticipating growth, making trade-offs between read and write performance remains a human problem.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#the-production-gap","title":"The Production Gap","text":"<p>All these limitations point to one reality: while AI has given me time leverage through compression, getting to actual production still takes longer than it should.</p> <p>One day this will change. GPT-5 will arrive, and Twitter/X will explode with another round of \"developers are doomed\" discourse. But the reality, particularly in enterprise environments with dense business logic and complex data models, is that there's still a mountain of work in the trenches.</p>"},{"location":"2025/07/26/ai-productivity-paradox-faster-not-smarter/#the-evolution-im-still-learning","title":"The Evolution I'm Still Learning","text":"<p>Beyond AI writing code, there's another pattern I've noticed in myself and other engineers: the constant temptation to write everything from scratch. It feels safe. It feels like control.</p> <p>But just like how we evolved to use open source, leveraging the collective intelligence of developers worldwide, we need to make the same leap with AI-generated code. Reading code, whether from team members, open source libraries, or AI, and building a mental model around it is a skill. And I'll admit, I haven't fully evolved yet. I'm still learning.</p> <p>I've become a better programmer over the years by reading open source code and learning from the collective intelligence of others. Take a bloated library like LangChain. My instinct says writing a token text splitter myself would be easy. But when I actually read their implementation, the inputs, outputs, test cases, I realize they've thought through edge cases I haven't even imagined.</p> <p>The value isn't in rolling something from scratch. It's in delivering customer value. There's no pride in reinventing wheels when you could be solving actual problems.</p> <p>The skill lies in making trade-offs:</p> <ul> <li>When to use existing solutions</li> <li>When to write from scratch</li> <li>When to fork and modify</li> <li>When to wrap and abstract</li> </ul> <p>With AI, it's the same evolution. The code is there, generated in seconds. My job is to read it, vet it, understand it, and adapt it to my specific needs and problems. Just like with open source, I need to leverage this collective knowledge, except now it's the collective knowledge of AI trained on millions of code repositories.</p> <p>I'd rather have this problem, too much compressed time and not enough production readiness, than the old problem of never having enough time at all.</p> <p>The future isn't about AI replacing us. It's about learning when to leverage AI for time compression and when to apply human intelligence for the complex, nuanced work that still defines great software. And just like my journey with open source, this evolution takes time.</p> <p>I'm still learning. We all are.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/","title":"Does Your Startup Really Need Complex Cloud Infrastructure?","text":""},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#does-your-startup-really-need-complex-cloud-infrastructure","title":"Does Your Startup Really Need Complex Cloud Infrastructure?","text":"<p>I recently listened to Pieter Levels on the Lex Friedman Podcast, and it was eye-opening. Pieter has built numerous successful micro-SaaS businesses by running his applications on single server, avoiding cloud infrastructure complexity, and focusing on what truly matters: product-market fit.</p> <p>While his approach might not suit teams and generally every startup, but it raises a valid point: we've often made deployment and infrastructure management complex for complexity's sake.</p> <p>For small dev teams moving past the MVP stage, managing deployments and databases can be challenging. But here's the truth: not every project needs Kubernetes, complex distributed systems, or auto-scaling from day one. Simple infrastructure can often suffice, allowing teams to focus on building a great product and finding market fit.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#recent-observations","title":"Recent Observations","text":"<p>Let me share two recent examples of projects I've worked on that highlight this issue:</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#project-1-lambda-overload","title":"Project 1: Lambda Overload","text":"<ul> <li>20-30 Lambda functions for different services</li> <li>SQS and various background jobs backed by Lambda</li> <li>Logs scattered across CloudWatch</li> </ul> <p>Result? Painful debugging, difficult changes, and complex deployments, even in a monorepo. Could this have been simplified to a single NodeJS container or Python Flask/FastAPI app with Redis for background tasks? Absolutely.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#project-2-microservices-mayhem","title":"Project 2: Microservices Mayhem","text":"<ul> <li>7 small microservices on Kubernetes (EKS)</li> <li>Separate services for CRUD and business logic</li> </ul> <p>While Kubernetes is powerful, the team spent more time on infrastructure than building features. Was this level of separation necessary for their scale?</p> <p>Note</p> <p>Enterprise-scale companies face different challenges with compliance and large workforces. Startups don't need to mimic this complexity. Early-stage companies should prioritize product-market fit and rapid iteration.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#the-power-of-single-server-setups","title":"The Power of Single Server Setups","text":"<p>Modern servers pack a punch. You can get powerful VMs from Hetzner or latitude.sh at budget-friendly prices. Even GCP VMs and EC2 instances are reasonably priced.</p> <p>These machines offer robust compute power - think 40GB RAM and multiple cores - often outperforming distributed services or multiple Lambdas or ECS tasks. Plus, everything's centralized and easier to manage.</p> <p>Worried about scaling to millions of QPS? Cross that bridge when you come to it. By then, you'll likely have an infrastructure team to handle it.</p> <p>For a reliable single VM setup, you need:</p> <ol> <li>A robust machine (EC2, GCP VM, Hetzner, etc.)</li> <li>Secure access (HTTPS for web, IP-restricted SSH or SSM for deployment)</li> <li>CI/CD for zero-downtime deployments</li> <li>DNS configuration</li> <li>Regular database backups</li> <li>A standby VM for redundancy</li> </ol> <p>Yes, you'll need a solid disaster recovery strategy and tested mean recovery time, but it's achievable with a backup VM.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is fantastic for local development, managing multiple services with a single command. Surprisingly, it's underutilized in production environments, and Docker Swarm was deprecated..</p> <p>While Docker Compose can cause downtime during updates, there are guides for production deployment. It's a balance between simplicity and production readiness.</p>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#docker-compose-anywhere-a-weekend-project","title":"Docker Compose Anywhere: A Weekend Project","text":"<p>To simplify this setup further, I created Docker Compose Anywhere over the weekend. This opinionated template offers:</p> <ul> <li>One-click Linux server setup via GitHub Actions</li> <li>Zero-downtime continuous deployment using GitHub Container Registry and Docker Rollout</li> <li>Environment variable and secret management (considering age or sops for improved security)</li> <li>Automated Postgres backups via GitHub Actions</li> <li>Multi-app support on a single VM</li> <li>Automated SSL with Traefik and Let's Encrypt</li> <li>Deploy Next.js apps, GO, Python, Node.js, and more</li> </ul>"},{"location":"2024/09/08/does-your-startup-really-need-complex-cloud-infrastructure/#few-considerations","title":"Few Considerations","text":"<p>For security, remember to:</p> <ul> <li>Set strict firewall rules (open only necessary ports)</li> <li>Secure SSH keys (prefer SSM on AWS or CLI on GCP)</li> <li>Use a bastion host for enhanced security</li> <li>Protect secrets and consider using a WAF or Cloudflare</li> </ul> <p>Don't forget about data protection:</p> <ul> <li>Send encrypted database backups to secure cloud storage (e.g., S3 or equivalent)</li> <li>Regularly snapshot your disks for added redundancy</li> <li>Implement a retention policy for backups and snapshots</li> </ul> <p>As engineers, our primary goal should be advocating for simplicity in our setup and focusing on the core product. </p> <p>It's all too easy to get distracted by shiny new tools or complex setups that mimic what Google engineers or large enterprises are doing. But here's the truth: whether you're in a startup or not, what truly matters is talking to your users and finding product-market fit.</p>"},{"location":"2025/12/24/product-and-engineering-principles/","title":"Blueprint and Engineering Principles","text":""},{"location":"2025/12/24/product-and-engineering-principles/#blueprint-and-engineering-principles","title":"Blueprint and Engineering Principles","text":"<p>These are the principles we follow at RevelAI Health. They've shaped how we build and ship. Might be useful for other early to late stage startups too.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#ownership-and-agency","title":"Ownership and agency","text":"<p>When you build something, own it end to end. Relying on QA or other team members? Those days are gone.</p> <p>AI has helped save us time writing code. But shipping to production still takes longer than you'd expect. The bottleneck isn't coding anymore. It's everything else.</p> <ul> <li>When you code something, test it yourself</li> <li>If you have a dependency on others (a vendor, internal team member, anyone), own the communication</li> <li>Do a follow-through in production. See how it's behaving</li> <li>You can't throw features over the wall</li> </ul>"},{"location":"2025/12/24/product-and-engineering-principles/#developing-product-taste","title":"Developing product taste","text":"<p>Product taste is a term used by many leaders. Sarah Guo wrote a great piece on it. Taste is a coherent philosophy encoded in thousands of aligned decisions.</p> <p>The engineering role has evolved beyond hiding behind your screen and getting work done. Understanding product from first principles, sitting in user shoes, and raising the bar on product UX. This matters now.</p> <p>Get in front of customers. Learn from their pain points. The best engineers I've worked with have strong product opinions.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#shipping-fast","title":"Shipping fast","text":"<p>We always encourage shipping fast. Not the old \"move fast and break things.\" That doesn't work with AI in the loop.</p> <p>The only way to know if something works at a startup is by shipping. Put features in front of customers. Get feedback. Iterate. Repeat.</p> <p>Velocity matters. Perfection doesn't.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#good-async-comms","title":"Good async comms","text":"<p>Writing gives you clarity. When you write down your thoughts, you think about edge cases. You give clarity of thought to yourself and others.</p> <p>Communicating clear PRDs beats hopping on really long calls. You can reference it later. Others can digest it on their own time.</p> <p>We encourage everyone to default to async.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#know-when-to-break-from-async","title":"Know when to break from async","text":"<p>Too much back and forth on Slack can drag. If something is taking more than 5 to 10 messages to resolve, just jump on a huddle.</p> <p>Get it resolved quickly. It's much better to do 10 minutes synchronously than write long essays of messages over 2 days.</p> <p>Async is the default. But know when to break from it.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#use-your-product-as-a-user","title":"Use your product as a user","text":"<p>Many engineers don't use their own product. If you're building in healthcare, you're not a nurse or doc. If you're building in fintech, you might not be doing investments.</p> <p>But you can pretend to be a user. Use it daily. You learn a ton using your own product.</p> <p>As engineers, we're busy shipping. But using what we build is how you develop intuition for what's broken.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#the-real-reward-is-adoption","title":"The real reward is adoption","text":"<p>I've seen teams celebrate GitHub pull requests. Teams celebrating \"we shipped feature X.\"</p> <p>The real reward is feature adoption. Anything you've built that's not getting adopted by customers? Something is wrong. Not just you, the whole product is doing something wrong.</p> <p>PRs merged and features shipped feel good. But they're not the goal. Customer adoption is.</p>"},{"location":"2025/12/24/product-and-engineering-principles/#no-one-is-coming-to-clear-your-tech-debt","title":"No one is coming to clear your tech debt","text":"<p>You're never going to get dedicated time to clear tech debt. It's hard to convince everyone, make a plan, and get it done.</p> <p>Don't wait for someone to tell you. Find ways to clean it up incrementally. Get buy-in from the team as you go.</p> <p>If you wait for permission, you'll wait forever.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/","title":"Clawdbot and the Era of AI in a Box","text":""},{"location":"2026/01/28/clawdbot-beyond-the-hype/#clawdbot-and-the-era-of-ai-in-a-box","title":"Clawdbot and the Era of AI in a Box","text":"<p>There's a lot of hype around Clawdbot. People claiming it'll make you a billion dollars, automate your business, act as your chief of staff. And yes, it's also a security nightmare.</p> <p>But there's something real here. Clawdbot (now renamed Moltbot) is pointing toward a fundamentally different relationship with AI. Not a chat window you visit, but a system running on YOUR machine, 24/7, on your infrastructure, with your files. AI in a box.</p> <p>2026 is the year everyone builds their own agent harness. The Agent SDK makes it easy. Clawdbot is opening the door.</p> <p>If you're a developer curious about running AI agents on your own infrastructure, here's what I've learned after a week.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#whats-actually-different","title":"What's Actually Different","text":"<p>The AI isn't sitting in some browser tab waiting for you to type. It's running in the background, on your server, executing tasks on your behalf. You trigger it from wherever, Telegram, WhatsApp, Slack. I use Telegram for the security benefits.</p> <p>It's a Claude Code-like agent running on a loop, packed with skills and constructs like cron jobs and channels. That's a fundamentally different relationship with AI.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#my-setup","title":"My Setup","text":"<p>I run Clawdbot on an Arch Linux box. Got the Arch bug after seeing DHH's Omarchy setup, had to try it. Never became my daily driver (MacBook still travels with me), but it's become my dedicated Linux dev server. Clawdbot lives there now.</p> <p>Here's what I actually use it for:</p> <ul> <li>Twitter monitoring: Read what people are building in AI and healthcare, track trends</li> <li>Brave research: Deep dives on competitive landscape, market analysis</li> <li>LinkedIn scanning: Check updates from people I follow</li> <li>Reddit threads: Research discussions and community sentiment</li> <li>Competitive analysis: Full reports on product spaces</li> </ul> <p>Example: I wanted competitive analysis on a product space. Triggered it through Telegram, went to bed. Clawdbot opened the browser, spent hours crawling sites, used Claude Opus 4.5 for the heavy reasoning and cheaper models for the grunt work. By morning, the report was sitting in a markdown file on my server.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#why-this-beats-chatgptclaude-console","title":"Why This Beats ChatGPT/Claude console","text":"<p>You could do research with ChatGPT/Claude trigger it from you mobile phone. Upload files and ask questions. So what's the difference?</p> <ol> <li> <p>Local execution: Commands run on your machine. It reads your filesystem, your codebase, your local context. Not just what you paste into a chat window.</p> </li> <li> <p>Persistent memory: ChatGPT's memory is superficial, it remembers you mentioned something, but doesn't deeply understand your context. Clawdbot maintains a memory filesystem that builds understanding over time.</p> </li> <li> <p>Background operation: The cron job system means it works while you sleep and notifies you when it's done. That overnight research report? Ready before I wake up.</p> </li> </ol> <p>It's the difference between a tool you use and a system that works for you.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#security-warning","title":"Security Warning","text":"<p>I'd be lying if I said I wasn't nervous. An AI that can execute commands on your machine, browse the web, read your files, that's a security surface area that keeps me up at night.</p> <p>Clawdbot is experimental, open source, and still rough around the edges. I wish it had more granular guardrails out of the box. What commands are allowed? What directories are off-limits? What requires explicit approval?</p> <p>We need proper sandbox environments. The tooling isn't there yet.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#getting-started","title":"Getting Started","text":"<p>The docs are rough, but the community has filled in the gaps. Best resource I've found is this Reddit setup guide, gets you running in about 30 minutes.</p>"},{"location":"2026/01/28/clawdbot-beyond-the-hype/#whats-next","title":"What's Next","text":"<p>Given how easy the Agent SDK makes building these harnesses, I expect an explosion of custom agent setups this year. Verticalized, specialized, tailored to specific workflows. Every serious developer will have their own.</p> <p>Clawdbot opens the door. Others will walk through it.</p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/","title":"Cognitive Architecture Patterns in Health Care for LLMs","text":""},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#cognitive-architecture-patterns-in-health-care-for-llms","title":"Cognitive Architecture Patterns in Health Care for LLMs","text":"<p>We're inspired by the ideas from this Cognitive Architecture paper and an insightful Langchain Blog by Harrison Chase. </p> <p>At RevelAI Health, we're exploring how to create closed-loop, safe agents in healthcare \u2014 systems that can reason and execute on patient needs in a secure and reliable way. The key is understanding how these agentic systems should think, the flow of execution in response to patient intent, and ensuring safety through structured, observable loops.</p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#building-safe-and-reliable-healthcare-agents","title":"Building Safe and Reliable Healthcare Agents","text":"<p>We've experimented with different architectures and patterns to develop effective healthcare agents, and here, I want to share our progress and ongoing work. This is a continuous journey; we're committed to evolving and learning as we go.</p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#level-1-single-llm-call-qna-chatbot","title":"Level 1: Single LLM Call (QnA Chatbot)","text":"<p>At the simplest level, we start with a tailored prompt-based LLM, designed for specific personas like an orthopedic care assistant or primary care assistant. Success here depends heavily on precise prompt engineering, including improving retrieval with a hybrid approach \u2014 for instance, combining semantic search and sparse vector search (e.g., BM25), along with metadata-based filtering that considers the patient phase of care, specific patient contexts like summarized notes from system records, patient history, and other contextual fields.</p> <p>Note</p> <p>We're not diving into the technical details of our retrieval system here, as that deserves its own dedicated post. The retrieval process is arguably the most critical component, as it's the primary driver of accuracy alongside prompt engineering. Getting this right is essential for safe and reliable healthcare agents.</p> <p>Grounding the LLM in evidence-based content curated by providers and subject matter experts is key to ensuring quality. It's not just about retrieval but also providing contextual awareness: patient phase of care, whether they're pre-op or post-op, current prescribed pathways, and other relevant information. The LLM must be firmly anchored in a patient's specific context to be effective.</p> <p>Our retrieval process relies on a hybrid approach that uses system records like EMRs, combined with databases such as social needs referrals, to build a complete picture and provide accurate responses.</p> <p></p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#level-2-chaining-multiple-llm-calls","title":"Level 2: Chaining Multiple LLM Calls","text":"<p>When simple QnA isn\u2019t sufficient, we use multiple specialized LLMs, each designed for a specific task such as rewriting questions, translating based on language preferences, or adjusting responses based on literacy levels gathered through tools like the Single Item Literacy Screener (SILS-2).</p> <p>In healthcare, questions often require multiple interactions for a complete answer. Our agents proactively engage in dialogue, asking follow-up questions based on different intents to gather the necessary context before providing a final response. We have developed several intent-specific dialogues to ensure the model collects all relevant information prior to giving an answer. Depending on the scenario, multiple specialized LLMs may collaborate in a sequence, working as an ensemble to deliver a final answer that is both accurate and personalized.</p> <p>We also utilize an LLM as a judge, evaluating final responses against strict criteria to ensure compliance, such as avoiding unauthorized medication advice, and determining when the LLM should either refrain from answering or respond in a specific, predefined way.</p> <p></p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#level-3-the-intent-router-and-the-orchestrator","title":"Level 3: The Intent Router and the Orchestrator","text":"<p>We've developed multiple sub-specialized agents, each one focusing on a specific area \u2014 like orthopedics, diabetes, or social care. These agents include an LLM, specialized retrieval content, and specific tools. They allow us to work in silos with healthcare professionals to refine accuracy before integrating them into broader orchestration systems. Every specialized agent follows it's own evaluation criteria and the testing</p> <p>The most significant challenge is routing patient intent to the right agent based on the context and data available. This goes beyond typical function-based OpenAI routing; we build datasets of different intents (e.g., orthopedic care, medication refill, social care, appointment requests) to guide the routing accurately. Our intent prediction is done through in-context-learning ICL or other mechanism to ensure the accuracy. We have developed multiple accuracy benchmarks on creating evaluation and predicting the correct intent across like 20 categories.Additionally, we have developed a dataset for acuity prediction, categorizing patient inquiries based on urgency, specificity, and need. Each inquiry is classified into one of three acuity levels: high, medium, or low.</p> <p>We provide a dashboard for care team members to manage incoming inquiries, whether they come via text, call, or other communication channels. The system suggests actions based on the acuity level and determines when human intervention is necessary, coordinating between agents to fulfill patient intents either automatically or manually.</p> <p></p> <p>Once intents are identified, orchestrators manage the specialized agents, coordinating them either sequentially or in parallel based on the context. Some agents complete tasks autonomously, such as drafting medication refill requests or forwarding notes to system records, effectively closing the loop. </p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#level-4-looping-agents-and-state-machines","title":"Level 4: Looping Agents and State Machines","text":"<p>Based on the predicted intents, our system can loop through agents and create a state machine to orchestrate workflows effectively. This means different agents work together seamlessly, adapting to changes in the patient\u2019s condition or needs as they move through the healthcare journey.</p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#level-5-towards-full-autonomy-with-agent-loops","title":"Level 5: Towards Full Autonomy with Agent Loops","text":"<p>We're not fully autonomous yet, and we believe in keeping humans in the loop for high-stakes decisions. For example, an agent can collect patient preferences for an appointment, but the final booking action still requires human oversight.</p> <p>We\u2019ve built agents capable of predicting the acuity level of patient requests (high, medium, or low). For high-acuity needs, we loop in healthcare professionals. For medium ones, we may involve a nurse, and for low-risk tasks, we can close the loop automatically. But we are cautious, recognizing the importance of human validation, especially in critical scenarios.</p> <p>Proactive outreach is another complex area \u2014 moving beyond hard-coded pathways (e.g., sending a message X days after discharge). We envision a dynamic state machine that adapts to patient data and engagement patterns, reaching out proactively and intelligently. However, this requires rigorous testing to ensure safety and effectiveness, and we're committed to learning and iterating. We believe that an autonomous state machine capable of proactively reaching out to patients based on engagement patterns or temporal relationships could significantly improve our scalability, eliminating the need to build rigid pathways for every possible scenario. It has the potential to transform population health management at scale, fundamentally changing how care is delivered. This is why we continue to experiment and refine our approach.</p>"},{"location":"2024/11/17/cognitive-architecture-patterns-in-health-care/#final-thoughts","title":"Final Thoughts","text":"<p>LLMs are still evolving, and the stakes in healthcare are high \u2014 getting things wrong can have severe consequences. We must balance the need for experimentation with the need for caution. By involving healthcare professionals in our iterative processes, maintaining a human-in-the-loop approach, and building strong feedback mechanisms, we aim to de-risk our systems while maximizing the value they bring.</p> <p>Our work is just beginning. We believe in learning every day and evolving our approach to bring safe, effective, and scalable AI-driven healthcare solutions to life. Tracing response paths, annotation-based evaluation, and leveraging LLMs as judges are just some of the methods we're using to ensure accuracy, minimize bias, and automate the testing process.</p> <p>We're off to a promising start, and we\u2019re eager to keep pushing forward, mindful of the responsibility that comes with innovation in healthcare.</p>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/","title":"Delayed task queue: How Vincere evolved from one Lambda to a persistent queue","text":""},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#delayed-task-queue-how-vincere-evolved-from-one-lambda-to-a-persistent-queue","title":"Delayed task queue: How Vincere evolved from one Lambda to a persistent queue","text":"<p>Vincere uses incentives. behavioral nudges and evidence-based interventions for healthy behaviors and making healthy choices. This requires us to track multiple events for users and nudging them through reminders/notifications towards better health</p>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#our-background-and-requirements","title":"Our Background and Requirements","text":"<p>Vincere allows health coaches to create a participant monitoring Campaign. A health coach can define the following things in a Campaign</p> <ul> <li> <p>Bio-marker feedback. e.g, CO(Carbon Monoxide) monitoring for smokers through a breathalyzer device using a mobile app. We are capable of doing other bio-marker feedback as well</p> </li> <li> <p>Define a Campaign to run for multiple days/months that can track multiple events just like on a calendar, e.g, breath test at a specific time or on a specific day, other bio-marker feedback, payment event where participants will be getting paid at a certain time or day, or participant/coach appointments for video/audio calls</p> </li> <li> <p>Define certain incentives/rewards criteria for achieving a certain goal in the defined testing window or making it to the appointments</p> </li> <li> <p>Define notifications criteria or reminders. These notifications are very personalized notifications and could be configured to go out at different intervals looking at the user information</p> </li> </ul> <p>Since we have to track different time-based testing windows and sending notifications at certain defined times we could not do this in a synchronous web request. We had some design discussions internally within the team about doing all of this on the client-side (front-end). We soon realized it will be hard to schedule something on the client-side if the app is not running in the background and another problem was besides doing push notifications we also deal with email/SMS notifications both for app participants and the health coaches, and events like payment processing where users will be getting paid. We decided to write a cronjob.</p> <p>Like many initial designs, our goal was to get something out there fast and working, to gather feedback from our users, and optimize later</p>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#first-iteration-of-background-jobs","title":"First Iteration of Background Jobs","text":"<p>The first iteration worked like this</p> <ol> <li> <p>We had one Lambda function. that was triggered every 1 minute through a scheduled cloud-watch event</p> </li> <li> <p>It will scan all the rows in the Postgres Database and based on UTC timestamp it will decide to take action and perform different kinds of mutations in the database</p> </li> </ol> <p></p>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#pros","title":"Pros","text":"<ul> <li>It was really easy to develop a Lambda function, where cron scheduling is handled by CloudWatch triggered events, which makes sure there is one execution of lambda function to avoid duplicate processing and we got something functional out there faster</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#cons","title":"Cons","text":"<ul> <li>It was a single point of failure and not fault-tolerant. And it did fail multiple times in later stages when there was an error related to a single event processing e.g, missing or malformed data and it had a cascading effect on the processing of all the events in that scheduled window. We re-factored the code but catching these errors in a fault-tolerant manner was hard. Also making sure one error does not cause all events to fail was hard</li> <li>One Lambda cannot scale horizontally. As we were not doing any workload distribution or a fan-out. As the events were growing it used to take one Lambda function much longer to process them all and most of the notifications were delayed. We vertically scaled the Lambda by increasing the memory/CPU and time limit within the AWS console. This gave us some runway to optimize later</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#second-iteration-of-background-jobs","title":"Second Iteration of Background Jobs","text":"<p>We decided to use a background job queue to process different events and handle the scheduling piece of them using a delayed task queue. We had the following requirements from a job queue</p> <ul> <li>Tracking job state and stats e.g, the status of the job, the run duration, etc.</li> <li>Automatic recovery if a worker crashes e.g retries with exponential backoffs and tracking them</li> <li>Pushed based PUB/SUB design instead of continuous polling</li> <li>Scheduled and delayed jobs</li> <li>Low over-head over creating queue topics or multiple types of queues</li> <li>Concurrency and horizontally scaling the load among distributed workers across different CPUs</li> </ul> <p>Our stack was running on AWS. So we decided to explore SQS as a distributed job queue for the fan-out and reliability. But we soon realized long scheduling jobs in the future are hard to achieve with SQS with an upper limit of 12 hours visibility timeout (time after which message is visible for processing). For long-scheduled tasks, a message needs to be picked up and delayed again depending on how long in the future a job is scheduled. This makes the design very complex when you have a job that has to run in the future e.g, an incentive payment event that has to run after a month</p> <p>Our team was already using Redis for caching purposes. We decided to use Redis as a delayed task execution queue. Redis supports delayed tasks with reasonable timing precision with minimal resource waste when idle. We explored a couple of stable packages for this use-case. e.g, Sidekiq in Rails, Celery with Redis in Python, and Bull in NodeJs. As most of our tech stack was in NodeJs we decided to go with the bull for running long-scheduled tasks and it fulfilled most of our requirements at the time</p> <p>The second iteration worked like this</p> <ol> <li>At the time of Campaign creation, we submit all the scheduled events to Redis through bull package with a timestamp it is supposed to run on, with high-level metadata about Campaign starting and ending date in the database</li> <li>A NodeJs process will process a job at some point in time when it is ready. We can add re-try logic in-case of failures and this was all handled by bull queue manager</li> <li>There could be multiple job processors running in multiple Node Docker containers for horizontal scalability. Bull aims for \u201cat least once strategy\u201d. But in some scenarios, a job could be retired multiple times. Making processes atomic is your job. We achieved this through PostgreSQL, which is discussed later in the article</li> </ol> <p></p>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#pros_1","title":"Pros","text":"<ul> <li>We were able to achieve horizontal scalability, every event was processed in a separate process/worker and had no impact on other events in-case of failure</li> <li>With BULL, there was built-in retry logic with an exponential backoff which helps to prevent sporadic errors like network issues</li> <li>Pushed based PUB/SUB design with delayed events were processed with reasonable timing precision</li> <li>For monitoring jobs, we were able to use an open-source UI which helped us a lot to track job states</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#cons_1","title":"Cons","text":"<ul> <li>Redis is not backed by disk. It works really well for the immediate jobs where you need high throughput, to keep jobs that have to run in the future consumes lots of memory and it is costly</li> <li>The way we were using Redis for long-scheduled tasks was not ideal. We used to submit all the events in Redis at the time of Campaign creation and individual job state mutations were not being tracked in the SQL database in an immutable or append-only manner. Since every historical state mutation at any point in time was not being tracked in a centralized place it was really hard for us to re-try or audit all the events processed or failed. At one point our EC2 server EBS volumes ran out of disk space due to an issue where log files were not being truncated. All the Redis jobs started failing because they couldn\u2019t write the log statements to disk and exhausted all the re-try attempts even with exponential backoff. Since not all the events were tracked it was really hard for us to re-try a particular event after fixing the issue</li> <li>As the backend at Vincere was maturing we started creating multiple micro-services. Most of them had to interact with a job queue for delayed or immediate processing. We were very tightly coupled with Redis and Bull. There wasn\u2019t one documented interface to enqueue jobs. All the developers had to get familiar with Bull API. And if we were to change our queue e.g, move to SQS or Kafka in the future it would be impossible to change application code due to tight coupling</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#third-iteration-of-background-jobs","title":"Third Iteration of Background Jobs","text":"<p>After a couple of iterations, we decided to write our own Job Queue utilizing PostgresSQL. We called it Programma. You might be wondering why \"reinvent the wheel\". Following were the main reasons we took this path</p> <ul> <li>Our use-case was to track these jobs in one place for simplicity, like SQL/NoSQL store without too much effort in a simple schema model. We attempt to track job states in Postgres through a simple interface. We choose Postgres due to its SKIP LOCK feature that is very suitable for building a queue backed by Postgres</li> <li>The goal of Programma is to expose a very flexible and simple API. Where client could nudge the job processing lifecycle by calling utility methods without us dictating the specific lifecycle of a job</li> <li>Programma ensures a job is delivered and claimed by the processor with retryAfterSeconds logic until job status is changed. This parameter is customizable and you can use it for exponential backoff logic as well by changing the retryAfterSeconds. Received messages that are not changed to either Processing, Completed, or FAILED state will appear again after retryAfterSecond timeout</li> <li>Programma exposes Promise-based API and written in typescript which helps us a lot since most of our stack is in NodeJS and Typescript helps us to create self-documenting job interfaces</li> </ul> <p>The third iteration and current iteration works like this</p> <p></p> <ol> <li>At the time of Campaign creation, we enqueue all the jobs using Programma API and add high-level campaign metadata in our application database</li> <li>Programma creates entries of jobs in the event-store(Postgres)</li> <li>Programma processor keeps polling database at a certain interval to see if there are any jobs that ready to be processed. We run multiple job poolers on different servers/containers for workload distribution. We can customize the pooling interval and max jobs per interval. This is where the Postgres SKIP LOCK feature comes really handy to skip the rows that are already claimed and avoid double processing</li> <li>Once a job is ready to be processed we change the job status to processing. Programma does not implement any job worker logic. That's why we fan-out all the ready jobs to Redis Queue with Bull for processing</li> <li>Once a job is processed by the Bull Worker we update the job status in the event-store using Programma API. Re-try logic is handled by the Bull queue manager</li> </ol> <p>Our application code and different microservices enqueue the jobs using the following API</p> <pre><code>interface IJobConfig {\n  data: {}\n  attributes?: {}\n  runAfterSeconds?: number\n  runAfterDate?: string | Date\n  retryAfterSeconds?: number | null\n}\n\ninterface IProgramma {\n  addJob(topicName: string, job: IJobConfig): Promise&lt;string | null&gt;\n}\n\nconst job = await programma.addJob('sendEmail', {\n  data: { email: 'test@xyz.com' },\n  runAfterDate: '2021-11-30T06:41:26.536Z', // run job next year\n})\n</code></pre> <p>Job Processing microservices keep pooling the jobs like in the following example</p> <pre><code>interface IProgramma {\n  receiveJobs(config: IReceiveMessageConfig, handler: IHandlerCallback): void\n}\n\nprogramma.receiveJobs({ topicName: 'sendEmail' }, async (job: IReceiveJob) =&gt; {\n  // use bull to fan-out the jobs to different\n  // reliable sandboxed workers with retry logic\n  await bullRedisQueue.add(\n    { id: job.id, data: job.data, attributes: job.data },\n    { retries: 3, backoff: 20000, timeout: 15000 }\n  )\n  // move job to processing, after submitting it. \n  // the status will be changed to processing and Redis queue will handle it\n  // if a job is not moved to different state\n  // it will be retired after retryAfterSecond period\n  await programma.moveJobToProcessing(job.id)\n})\n</code></pre> <p>Each job will be processed by an individual Bull Worker which is a separate Node process. Once a job is processed successfully, we change job status and track in DB through Programma API</p> <pre><code>bullEmailRedisQueue.process(async (job, done) =&gt; {\n  try {\n    // send email\n    await programma.moveJobToDone(jod.data.id)\n    return Promise.resolve()\n  } catch (e) {\n    return Promise.reject()\n  }\n}\n\n// job failed after all the back-off retries\n// we can track that in SQL through programma API\nbullRedisQueue.on('failed', (id, error) =&gt; {\n  const job = await bullRedisQueue.getJob(id)\n  await programma.moveJobToFailed(jod.data.id)\n  // also can track error in SQL\n  await programma.setAttributes(job.id, { error: error })\n})\n</code></pre>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#pros_2","title":"Pros","text":"<ul> <li>Code abstraction that\u2019s easy for developers to work with and different micro-services use simple API to enqueue background jobs</li> <li>Delayed tasks that have to run in the future do not consume too much memory</li> <li>Everything is tracked in one centralized event-store which is reliable i.e, backed by the disk, and for throughput, we utilize Redis PUB/SUB to fan-out jobs to different processors</li> <li>We can achieve horizontal scalability by running job polling logic on different CPUs/containers. Thanks to Postgres SKIP LOCK which help us achieve this</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#cons_2","title":"Cons","text":"<p>Some of these might not be cons. But we will discuss trade-offs and future scalability problems</p> <ul> <li>Since every job regardless if it has to run immediately or in the future is tracked in event-store it consumes lots of space and the index size grows too. We plan to solve this in the future by running a configurable job archival process</li> <li>One SQL Database won\u2019t be able to handle all the load. We plan to create multiple Programma clusters and shard tasks by Organization Id or something more efficient and route them to different clusters. Since Programma handles Job Table schema creation on the fly if it does not exist, we can run multiple local Postgres databases per programma cluster and scale-out. We are very inspired by the Pinterest implementation of pinlater and how it can scale out with MySQL based queue implementation</li> <li>Pooling based design instead of PUB/SUB wastes some CPU resources where you are continuously pooling even when no jobs are available to be processed in a particular interval. This is one of the tradeoffs we have to make for simplicity and reliability</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#avoiding-duplicate-processing-of-jobs","title":"Avoiding duplicate processing of Jobs","text":"<p>As we process payment events, where our users get paid through different payment gateways. We cannot take risk of double processing the event. Even with Postgres SKIP LOCK and Bull it\u2019s hard to run into a scenario where a job is processed twice, but the situation could arise when the queue is stalled. For such jobs, we use distributed locks with Redis Redlock. You can read this topic here regarding the distributed lock implementation</p> <pre><code>try {\n  // try to acquire lock\n  redlock.lock(job.id, 5000) // acquire lock for 5 seconds\n  // process payment\n} catch (e) {\n  // failed to acquire job. try next time\n}\n</code></pre>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#next-steps","title":"Next Steps","text":"<ul> <li>Most of our background job workflows are evolving into a DAG (Directed Acyclic Graph). We plan to model the parent-child relationship of jobs in the future for maintainability and having better visibility over workflows</li> <li>Creating a better API for queue metrics and measuring queue depth. If a job is not processed in a specific interval maybe set up some alarms etc.</li> <li>Creating a better archival process for the processed job for event-store maintainability</li> </ul>"},{"location":"2021/01/26/delayed-task-queue-how-vincere-evolved-from-one-lambda/#references","title":"References","text":"<ul> <li>Bull Queue Documentation</li> <li>Understanding SELECT ... SKIP LOCKED in PostgreSQL</li> <li>Redis Distributed Locks</li> <li>Qmessage: Handling Billions of Tasks Per Day</li> <li>Pinterest's Pinlater</li> <li>Building a Multi-Tenant Job Queue System with PostgreSQL</li> </ul>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/","title":"Escaping Context Amnesia: Practical Strategies for Long-Running AI Agents","text":""},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#escaping-context-amnesia-practical-strategies-for-long-running-ai-agents","title":"Escaping Context Amnesia: Practical Strategies for Long-Running AI Agents","text":"<p>The promise of autonomous AI agents is vast: give them a high-level goal, grant them access to tools, and watch them execute complex workflows. But reality often hits hard. Specifically, it hits the context window.</p> <p>Models like Claude Sonnet 4.5 now offer 200K tokens (up to 1M in beta), and GPT-5.1 supports 400K tokens with native compaction that claims to work across millions of tokens. Problem solved, right?</p> <p>Not quite. Bigger context windows don't solve the problem. They mask it.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-real-problem-its-not-just-about-size","title":"The Real Problem: It's Not Just About Size","text":"<p>There are three reasons why simply \"using more context\" fails for long-running agents:</p> <p>1. Cost Scales Linearly (or Worse)</p> <p>Every token you send costs money. At scale, a 500K token context per request adds up fast. Anthropic even charges premium rates (2x input, 1.5x output) for requests exceeding 200K tokens.</p> <p>2. Latency Increases</p> <p>More tokens mean longer processing time. For interactive agents where users expect quick responses, stuffing the context window creates a sluggish experience.</p> <p>3. The \"Lost in the Middle\" Phenomenon</p> <p>This is the critical one. Research from Stanford and others (Liu et al., 2023) demonstrated that LLM performance degrades significantly when relevant information is positioned in the middle of long contexts. Models perform best when key information is at the beginning or end, but struggle to access what's buried in between.</p> <p>The cause? A combination of causal attention (where earlier tokens get processed more) and positional encoding effects that diminish attention to middle-positioned tokens. Even models explicitly designed for long contexts suffer from this.</p> <p>So when your agent has 800K tokens of conversation history and the critical instruction from turn #3 is now buried in the middle, the model literally \"forgets\" it. Not because it ran out of space, but because attention doesn't reach it effectively.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#what-this-post-covers","title":"What This Post Covers","text":"<p>After analyzing the open-source strategies of leading developer agents, including OpenAI's Codex, SST's OpenCode, and the VS Code agent Cline, I've synthesized the state-of-the-art approaches to solving the context crunch.</p> <p>While I'll focus on coding agents as examples, these techniques apply to any long-running agent: customer support bots, research assistants, data analysis pipelines, or workflow automation. Any agent that maintains state across many turns faces the same challenges.</p> <p>Here's a deep dive into a tiered approach for building agents with effectively infinite context.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-problem-death-by-a-thousand-tool-calls","title":"The Problem: Death by a Thousand Tool Calls","text":"<p>Before diving into solutions, let's visualize the problem. Here's what happens during a typical multi-turn agent session:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                             \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502    \u2502 System Prompt (2k)      \u2502                                              \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502    \u2502 User: Refactor auth     \u2502                                              \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502         \u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inference!        \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502    \u2502 Model: Ok, let me read  \u2502                                              \u2502\n\u2502    \u2502 Tool Call: read_file    \u2502                                              \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502              *\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Run tool          \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                        \u2502\n\u2502         \u2502 Tool Result: [8k tokens] \u2502                                        \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                        \u2502\n\u2502         \u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inference!        \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502    \u2502 Model: Now run tests    \u2502                                              \u2502\n\u2502    \u2502 Tool Call: run_tests    \u2502                                              \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502              *\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Run tool          \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                       \u2502\n\u2502         \u2502 Tool Result: [25k tokens] \u2502  \u2190 Verbose test output                \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2502\n\u2502         \u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inference!        \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                              \u2502\n\u2502    \u2502 Model: Tests pass, now  \u2502                                              \u2502\n\u2502    \u2502 Tool Call: read_file    \u2502                                              \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                              \u2502\n\u2502              *\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Run tool          \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                       \u2502\n\u2502         \u2502 Tool Result: [12k tokens] \u2502                                       \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2502\n\u2502                                                                             \u2502\n\u2502    \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550     \u2502\n\u2502    Context: 47k / 128k tokens used... and we're just getting started        \u2502\n\u2502    \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550     \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If an agent runs a test suite and gets back 15,000 lines of successful \"OK\" messages followed by one failure at the end, 99% of that output is noise. Yet the model has to process all of it, and it consumes precious context space.</p> <p>The solution isn't larger windows. It's smarter context engineering through a tiered compression strategy.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-tiered-approach-to-infinite-context","title":"The Tiered Approach to Infinite Context","text":"<p>The key insight from studying production agents is that context management isn't a single strategy. It's a cascade of increasingly aggressive techniques. Each tier activates only when the previous one proves insufficient.</p> <pre><code>                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  New Turn Complete  \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Tokens &gt; 60%?      \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 No                     Yes  \u2502\n                    \u25bc                              \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502    Continue     \u2502           \u2502  TIER 1: Pruning    \u2502\n          \u2502    Normally     \u2502           \u2502  Truncate old tool  \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502  outputs (head+tail)\u2502\n                    \u25b2                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                              \u2502\n                    \u2502                              \u25bc\n                    \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                   \u2502  Still &gt; 70%?       \u2502\n                    \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                              \u2502\n                    \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502               \u2502 No                     Yes  \u2502\n                    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u25bc\n                    \u2502                             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                             \u2502 TIER 1.5: Compress  \u2502\n                    \u2502                             \u2502 Replace with smart  \u2502\n                    \u2502                             \u2502 placeholders        \u2502\n                    \u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                                        \u2502\n                    \u2502                                        \u25bc\n                    \u2502                             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                             \u2502  Still &gt; 85%?       \u2502\n                    \u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                                        \u2502\n                    \u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                          \u2502 No                   Yes  \u2502\n                    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u25bc\n                    \u2502                                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                       \u2502  TIER 2: Handoff    \u2502\n                    \u2502                                       \u2502  Summarize \u2192 Clear  \u2502\n                    \u2502                                       \u2502  \u2192 Restart          \u2502\n                    \u2502                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                                                  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#tier-1-the-input-guard-smart-pruning","title":"Tier 1: The Input Guard (Smart Pruning)","text":"<p>The most cost-effective strategy is to prevent \"garbage\" context from entering the window in the first place. Tool outputs are the primary culprit for context pollution.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-strategy-token-aware-truncation-with-protection-zones","title":"The Strategy: Token-Aware Truncation with Protection Zones","text":"<p>Both Codex and OpenCode moved away from naive \"line limits\" (e.g., \"keep first 100 lines\") because 100 lines of dense JSON is very different from 100 lines of whitespace.</p> <p>Instead, they implement a strict Token Budget for tool outputs. If an output exceeds the budget, the system keeps the critical \"Head\" (context) and \"Tail\" (results/errors), aggressively pruning the middle.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BEFORE: Original Tool Output (20k tokens)                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 HEAD: Command executed, processing started...          (500 tokens) \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 MIDDLE: [OK] Test 1 passed                                          \u2502    \u2502\n\u2502  \u2502         [OK] Test 2 passed                                          \u2502    \u2502\n\u2502  \u2502         [OK] Test 3 passed                                          \u2502    \u2502\n\u2502  \u2502         ... 14,997 more lines of verbose logs ...     (18k tokens)  \u2502    \u2502\n\u2502  \u2502         [OK] Test 15000 passed                                      \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 TAIL: [FAIL] Test 15001 - AssertionError at line 42                 \u2502    \u2502\n\u2502  \u2502       Summary: 15000 passed, 1 failed                 (1.5k tokens) \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502 Tier 1 Pruning\n                                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AFTER: Pruned Output (2k tokens)                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 HEAD: Command executed, processing started...          (500 tokens) \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 [...truncated 18,000 tokens...]                                     \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 TAIL: [FAIL] Test 15001 - AssertionError at line 42                 \u2502    \u2502\n\u2502  \u2502       Summary: 15000 passed, 1 failed                 (1.5k tokens) \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                             \u2502\n\u2502  \u2713 Model sees: what command ran + the actual error it needs to fix          \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#key-implementation-details","title":"Key Implementation Details","text":"<p>OpenCode uses two critical thresholds in their compaction logic:</p> <ul> <li>PRUNE_MINIMUM: ~20,000 tokens. Don't bother pruning if potential savings are below this.</li> <li>PRUNE_PROTECT: ~40,000 tokens. Recent conversation window that's always protected.</li> </ul> <p>The algorithm walks backwards through conversation history, identifying tool outputs that exceed a maximum length threshold, but only outside the protection zone:</p> <pre><code>def prune(conversation):\n    protection_zone = find_last_N_user_turns(2)\n\n    for each item in conversation (backwards):\n        if item.index &gt;= protection_zone.start:\n            continue  // Protected recent context\n\n        if item.role == \"tool\" AND item.tokens &gt; MAX_OUTPUT_TOKENS:\n            item.content = head(500) + \"[...truncated...]\" + tail(1500)\n\n    return conversation\n</code></pre> <p>Why it works: It ensures that no single action can \"brick\" the session, while still allowing the model to see the command it ran and the final error message it needs to react to.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#tier-15-placeholder-compression","title":"Tier 1.5: Placeholder Compression","text":"<p>Pruning helps, but sometimes you need to be more aggressive without losing the structure of the conversation. This is where placeholder compression shines.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-strategy-actionable-hints-not-silent-removal","title":"The Strategy: Actionable Hints, Not Silent Removal","text":"<p>The key insight from OpenCode's summary.ts and Cline's ContextManager is that replacing content with informative placeholders is better than silent removal.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BEFORE: Full Conversation History                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502    \u2502 User: Query sales data            \u2502                                    \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502         \u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inference!        \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502    \u2502 Model: Let me query that          \u2502                                    \u2502\n\u2502    \u2502 Tool Call: execute_query          \u2502                                    \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502              *\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Run tool          \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502         \u2502 Tool Result:                          \u2502                           \u2502\n\u2502         \u2502 {\"rows\": [                            \u2502                           \u2502\n\u2502         \u2502   {\"id\": 1, \"amount\": 500, ...},      \u2502  \u2190 15k tokens of JSON     \u2502\n\u2502         \u2502   {\"id\": 2, \"amount\": 750, ...},      \u2502                           \u2502\n\u2502         \u2502   ... 4998 more rows ...              \u2502                           \u2502\n\u2502         \u2502 ]}                                    \u2502                           \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n\u2502         \u25a0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Inference!        \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502    \u2502 Model: Total sales are $2.3M      \u2502                                    \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502 Tier 1.5: Placeholder Compression\n                                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AFTER: Compressed with Actionable Placeholder                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502    \u2502 User: Query sales data            \u2502                                    \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502    \u2502 Model: Let me query that          \u2502                                    \u2502\n\u2502    \u2502 Tool Call: execute_query          \u2502  \u2190 Tool call preserved (structure) \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502         \u2502 [Query executed. Use execute_query again for fresh results] \u2502     \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2191                               \u2502\n\u2502    \u2502 Model: Total sales are $2.3M      \u2502    \u2502                               \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                               \u2502\n\u2502                                             \u2502                               \u2502\n\u2502    \u2713 Model knows: action succeeded + how to get data again if needed        \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-critical-difference-actionable-vs-generic-placeholders","title":"The Critical Difference: Actionable vs. Generic Placeholders","text":"<p>A placeholder like <code>[removed]</code> or <code>[truncated]</code> leaves the model confused. But a placeholder like:</p> <pre><code>[Query executed successfully. Use execute_query again if you need fresh results]\n</code></pre> <p>...gives the model a clear path forward. It knows the action completed and how to recover the data if needed.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#implementation-pattern","title":"Implementation Pattern","text":"<pre><code>PLACEHOLDERS = {\n    \"query_execution\":    \"[Query executed. Re-run if you need fresh results]\",\n    \"file_read\":          \"[File read previously. Read again if needed]\",\n    \"search_results\":     \"[Search completed. Search again for current results]\",\n    \"html_generation\":    \"[HTML generated. Use get_current_state for latest]\",\n}\n\ndef compact(conversation):\n    essential_tools = {\"schema_inspection\", \"database_structure\"}\n    keep_latest_only = {\"saved_state\", \"current_config\"}\n\n    for each tool_result in conversation:\n        tool_type = categorize(tool_result)\n\n        if tool_type in essential_tools:\n            continue  // Never compress these\n\n        if tool_type in keep_latest_only:\n            if not is_most_recent_of_type(tool_result):\n                replace_with_placeholder(tool_result, PLACEHOLDERS[tool_type])\n        else:\n            replace_with_placeholder(tool_result, PLACEHOLDERS[tool_type])\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#clines-duplicate-detection","title":"Cline's Duplicate Detection","text":"<p>Cline adds another clever optimization: duplicate file read detection. If the same file is read multiple times in a conversation, subsequent reads are replaced with a notice pointing to the original. They target 30% character savings before moving to more aggressive truncation.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#tier-2-session-handoff-total-recall","title":"Tier 2: Session Handoff (Total Recall)","text":"<p>Pruning and placeholders help, but for truly long-running tasks (e.g., a multi-hour refactoring mission), the history will eventually fill up with necessary turns.</p> <p>When the context window reaches a critical threshold (e.g., 85-90%), you need a \"Reset Button.\" This is the strategy heavily utilized by OpenAI's Codex.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-strategy-compaction-via-recursive-summarization","title":"The Strategy: Compaction via Recursive Summarization","text":"<p>Rather than simply deleting old messages, the system triggers a \"Handoff\" event.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SESSION HANDOFF FLOW                                                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   1. DETECT: Context reaches 90% threshold                                  \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                 \u2502\n\u2502                                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502 Context Window: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591 92%   \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502   2. PAUSE: Stop processing, gather full history                            \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                            \u2502\n\u2502                                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u2502\n\u2502   \u2502 Full Conversation History          \u2502                                    \u2502\n\u2502   \u2502 \u251c\u2500 System prompt                   \u2502                                    \u2502\n\u2502   \u2502 \u251c\u2500 User message #1                 \u2502                                    \u2502\n\u2502   \u2502 \u251c\u2500 Assistant + tools               \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502   \u2502 \u251c\u2500 User message #2                 \u2502       \u2502                            \u2502\n\u2502   \u2502 \u251c\u2500 Assistant + tools               \u2502       \u2502 Send to                    \u2502\n\u2502   \u2502 \u251c\u2500 ... (50 more turns) ...         \u2502       \u2502 Fast Model                 \u2502\n\u2502   \u2502 \u2514\u2500 Last user message \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2510   \u2502                            \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502                            \u2502\n\u2502                                            \u2502   \u25bc                            \u2502\n\u2502   3. SUMMARIZE: Fast model generates handoff note                           \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                           \u2502\n\u2502                                            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502                                            \u2502   \u2502 \"Summarize this        \u2502   \u2502\n\u2502                                            \u2502   \u2502  conversation for      \u2502   \u2502\n\u2502                                            \u2502   \u2502  continuation...\"      \u2502   \u2502\n\u2502                                            \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                            \u2502               \u2502                \u2502\n\u2502                                            \u2502               \u25bc                \u2502\n\u2502   4. RESTART: New session with summary + last user message                  \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                  \u2502\n\u2502                                            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502 Handoff Note:          \u2502   \u2502\n\u2502   \u2502 NEW Context Window                 \u2502   \u2502   \u2502 - Goal: Refactor auth  \u2502   \u2502\n\u2502   \u2502                                    \u2502   \u2502   \u2502 - Done: Updated 3 files\u2502   \u2502\n\u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502   \u2502 - Current: Fixing test \u2502   \u2502\n\u2502   \u2502 \u2502 System: PREVIOUS SESSION       \u2502\u25c4\u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502 - Next: Run test suite \u2502   \u2502\n\u2502   \u2502 \u2502 [Handoff Note: ~500 tokens]    \u2502 \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502                                \u2502\n\u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502                                \u2502\n\u2502   \u2502 \u2502 User: [Last message]           \u2502\u25c4\u2502\u2500\u2500\u2500\u2518                                \u2502\n\u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                                    \u2502\n\u2502   \u2502                                    \u2502                                    \u2502\n\u2502   \u2502 Context: \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 5%  \u2502  \u2190 Fresh start!                    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502                                                                             \u2502\n\u2502   \u2713 100k tokens of history \u2192 500 tokens of pure signal                      \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-handoff-note-format","title":"The Handoff Note Format","text":"<p>The summary isn't prose. It's a structured state definition. OpenCode's compaction prompt instructs the summarizer to capture:</p> <ol> <li>Current Goal: What is the user ultimately trying to achieve?</li> <li>Progress Made: What has been completed successfully?</li> <li>Files Modified: Which files have been changed and how?</li> <li>Current State: What was the agent doing when handoff triggered?</li> <li>Next Steps: What should happen immediately after restart?</li> </ol>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#implementation","title":"Implementation","text":"<pre><code>def handoff(conversation, threshold=0.90):\n    if token_count(conversation) &lt; context_limit * threshold:\n        return conversation  // Not needed yet\n\n    // Use a fast, cheap model for summarization\n    summary = call_model(\n        model = \"fast-summarizer\",  // e.g., GPT-4o-mini, Haiku\n        prompt = HANDOFF_PROMPT,\n        content = conversation\n    )\n\n    last_user_message = find_last_user_message(conversation)\n\n    new_session = [\n        {\"role\": \"system\", \"content\": \"PREVIOUS SESSION CONTEXT:\\n\" + summary},\n        last_user_message\n    ]\n\n    return new_session  // 100k tokens \u2192 ~500 tokens\n</code></pre> <p>Why it works: It converts 100k tokens of messy history into 500 tokens of pure signal, allowing the agent to continue working indefinitely by \"passing the baton\" to itself.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#tier-25-middle-out-compression-optional","title":"Tier 2.5: Middle-Out Compression (Optional)","text":"<p>A common failure mode of naive summarization is the loss of critical details from the beginning of the task (the original goal) and the end (the current state). Cline pioneered a \"Middle-Out\" strategy that addresses this.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-strategy-preserve-the-ends-crush-the-middle","title":"The Strategy: Preserve the Ends, Crush the Middle","text":"<p>When context pressure mounts, this strategy doesn't wipe the slate clean. Instead, it performs surgery on the conversation history.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  MIDDLE-OUT COMPRESSION                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   BEFORE                                       AFTER                        \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500                                       \u2500\u2500\u2500\u2500\u2500                        \u2502\n\u2502                                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502 HEAD                    \u2502                 \u2502 HEAD                    \u2502   \u2502\n\u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502   \u2502 \u2502 System Prompt       \u2502 \u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u25b6   \u2502 \u2502 System Prompt       \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 User: \"Refactor     \u2502 \u2502   PRESERVED    \u2502 \u2502 User: \"Refactor     \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 the auth system\"    \u2502 \u2502   VERBATIM     \u2502 \u2502 the auth system\"    \u2502 \u2502   \u2502\n\u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502   \u2502 (The original goal)     \u2502                 \u2502 (Goal intact)           \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502 MIDDLE                  \u2502                 \u2502 MIDDLE                  \u2502   \u2502\n\u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn 2: Read files  \u2502 \u2502                 \u2502 \u2502 [Summary: Read 5    \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn 3: Analyze     \u2502 \u2502                 \u2502 \u2502  files, identified  \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn 4: First edit  \u2502 \u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u25b6   \u2502 \u2502  auth patterns,     \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn 5: Run tests   \u2502 \u2502   SUMMARIZED   \u2502 \u2502  made 3 edits,      \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn 6: Fix bug     \u2502 \u2502                 \u2502 \u2502  tests passing]     \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 ... 20 more turns   \u2502 \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502 (Journey compressed)    \u2502   \u2502\n\u2502   \u2502 (The noisy journey)     \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2502\n\u2502                                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502 TAIL                    \u2502                 \u2502 TAIL                    \u2502   \u2502\n\u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn N-4: Edit JWT  \u2502 \u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u25b6   \u2502 \u2502 Turn N-4: Edit JWT  \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn N-3: Test fail \u2502 \u2502   PRESERVED    \u2502 \u2502 Turn N-3: Test fail \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn N-2: Debug     \u2502 \u2502   VERBATIM     \u2502 \u2502 Turn N-2: Debug     \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn N-1: Fix found \u2502 \u2502                 \u2502 \u2502 Turn N-1: Fix found \u2502 \u2502   \u2502\n\u2502   \u2502 \u2502 Turn N: User input  \u2502 \u2502                 \u2502 \u2502 Turn N: User input  \u2502 \u2502   \u2502\n\u2502   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502   \u2502 (Current state)         \u2502                 \u2502 (State intact)          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                             \u2502\n\u2502   \u2713 Agent knows WHY it started + WHERE it is now                            \u2502\n\u2502   \u2717 Forgets the noisy journey in between (that's fine!)                     \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#clines-implementation","title":"Cline's Implementation","text":"<p>In Cline's <code>getNextTruncationRange()</code> function, they explicitly protect indices 0-1 (the first user message and assistant response):</p> <pre><code>rangeStartIndex = 2  // Never touch the first exchange\n</code></pre> <p>They then calculate how much to remove based on pressure:</p> <ul> <li>\"half\" mode: Keep 75% of messages after deduplication</li> <li>\"quarter\" mode: Keep 50% of messages (more aggressive)</li> </ul> <p>The truncation always removes from the middle, leaving both the \"why we started\" (head) and \"where we are now\" (tail) intact.</p> <p>Why it works: The agent always knows why it started the task and where it is right now, without being distracted by the noisy journey in between.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#what-worked-for-us","title":"What Worked For Us","text":"<p>After studying these open-source implementations, we built our own tiered system. Here's the algorithm that's proven effective:</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#our-tiered-implementation","title":"Our Tiered Implementation","text":"<p>Tier 1: Protection Zone Pruning</p> <pre><code>def prune(conversation):\n    // Protect last 2 user turns unconditionally\n    protection_zone_start = find_nth_last_user_turn(2)\n    tokens_before = count_tokens(conversation)\n\n    for idx in reverse(range(len(conversation))):\n        if idx &gt;= protection_zone_start:\n            continue  // In protection zone\n\n        item = conversation[idx]\n        if item.role == \"tool\" and len(item.content) &gt; MAX_OUTPUT_LENGTH:\n            preview = item.content[:1000]\n            item.content = f\"{preview}...\\n[Output pruned. Original: {len(item.content)} chars]\"\n\n    tokens_after = count_tokens(conversation)\n    log(f\"Tier 1: {tokens_before} \u2192 {tokens_after} tokens\")\n    return conversation\n</code></pre> <p>Tier 1.5: Smart Placeholder Replacement</p> <pre><code>def compact(conversation):\n    essential = {\"schema_inspection\", \"structure_query\"}  // Never touch\n    latest_only = {\"saved_state\"}  // Keep only most recent\n\n    tool_id_to_type = analyze_tool_calls(conversation)\n    latest_of_type = {}\n\n    // First pass: identify latest of each \"keep latest\" type\n    for result in conversation where result.role == \"tool\":\n        tool_type = tool_id_to_type[result.id]\n        if tool_type in latest_only:\n            latest_of_type[tool_type] = result.id\n\n    // Second pass: replace with placeholders\n    for result in conversation where result.role == \"tool\":\n        tool_type = tool_id_to_type[result.id]\n\n        if tool_type in essential:\n            continue\n\n        if tool_type in latest_only and result.id == latest_of_type[tool_type]:\n            continue  // This is the latest, keep it\n\n        result.content = PLACEHOLDERS[tool_type]\n\n    return conversation\n</code></pre> <p>Tier 2: Session Handoff</p> <pre><code>def handoff(conversation, llm_connection):\n    if token_ratio(conversation) &lt; 0.85:\n        return conversation\n\n    summary = call_fast_model(\n        prompt = HANDOFF_PROMPT,\n        conversation = conversation,\n        model = \"fast-cheap-model\"\n    )\n\n    last_user = find_last_user_message(conversation)\n\n    return [\n        {\"role\": \"system\", \"content\": f\"SESSION CONTEXT:\\n{summary}\"},\n        last_user\n    ]\n</code></pre>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#key-decisions-that-made-the-difference","title":"Key Decisions That Made the Difference","text":"<ol> <li>Protection zones by turn count, not tokens: Last 2 user messages are always safe, regardless of their size</li> <li>Actionable placeholders: Every placeholder tells the model how to recover the data</li> <li>Essential tools whitelist: Schema and structure tools are never compressed. They're foundational.</li> <li>Keep-latest-only pattern: For cumulative state (saved queries, current config), only the most recent matters</li> </ol>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#the-future-from-reactive-to-proactive","title":"The Future: From Reactive to Proactive","text":"<p>Today's strategies are primarily reactive: the agent hits a limit, then compresses. The next frontier is proactive context management:</p> <ul> <li>Semantic RAG for conversation history: Instead of keeping everything, index past turns and retrieve only what's relevant to the current task</li> <li>Externalized state: Maintain a persistent \"scratchpad\" file outside the context window that survives any amount of clearing</li> <li>Predictive pruning: Use a lightweight model to predict which tool outputs will be needed later, pruning aggressively on low-value results</li> </ul> <p>The battle for AI reliability won't be won by larger context windows. It will be won by smarter context engineering. By implementing strategies like Input Pruning, Placeholder Compression, and Session Handoffs, we can build agents that don't just work for the first five minutes. They work for the entire project.</p>"},{"location":"2025/11/26/escaping-context-amnesia-ai-agents/#references","title":"References","text":"<ul> <li>OpenAI Codex: Session memory patterns, token counting</li> <li>SST OpenCode: Two-tier prune \u2192 compact strategy</li> <li>compaction.ts</li> <li>summary.ts</li> <li>Cline: Middle-out compression, duplicate detection</li> <li>ContextManager.ts</li> </ul>"},{"location":"2022/03/04/helping-people-quit-smoking-through-financial-rewards/","title":"Helping People Quit Smoking Through Financial Rewards","text":""},{"location":"2022/03/04/helping-people-quit-smoking-through-financial-rewards/#helping-people-quit-smoking-through-financial-rewards","title":"Helping People Quit Smoking Through Financial Rewards","text":"<p>Guest post on AWS Startups Blog</p> <p>Smoking is still the leading cause of preventable death, and the pandemic made matters worse: cigarette sales increased in 2020 for the first time in over 20 years. There are over 34 million adult smokers in the US, many of whom are lower income. Reaching them and providing effective support at this scale while keeping costs down can only be achieved with the aid of technology and carefully designed patient experience.</p> <p>Vincere Health is one of few health technology platforms built for people in diverse socio-economic categories. Unlike traditional cessation programs, Vincere offers low-cost access to addiction healthcare using reward-based habit training. Our belief is clinicians in the loop are integral to lasting behavior change, and the technology serves to facilitate and personalize this relationship at scale.</p> <p></p>"},{"location":"2022/03/04/helping-people-quit-smoking-through-financial-rewards/#need-for-a-personalized-quit-journey","title":"Need for a personalized \u201cquit journey\u201d","text":"<p>Smoking cessation is best represented as a journey, not a single event. The personalized care experience that is individualized to each participant\u2019s needs has shown increased rates of success conquering addiction with care compliance rates up to 80% and quit rates up to 35%. Participants are encouraged to set compelling yet reachable weekly goals using our program and stay connected to their health coaches for support.</p> <p>Our platform allows health coaches to create a personalized participant quit journey.</p> <ul> <li>Health coaches can create a care journey for multiple days/months that can track multiple events just like on a calendar, e.g, breath test at a specific time/day, complete a self-reported outcome survey, or participant/coach video call appointment.</li> <li>Coaches can define financial reward criteria for achieving a certain goal. e.g, participants can earn rewards for completing a self-reported survey, breath test habit compliance, or making it to the appointments.</li> <li>Define personalized reminders and notifications. These reminders could be configured to go out at different intervals looking at the user information for behavioral reinforcement and motivational nudges.</li> </ul> <p>During the program, we collect and aggregate lots of data from participants, to measure engagement based on habit compliance, take proactive actions, and inform health coaches about participant triggers or relapses.</p>"},{"location":"2022/03/04/helping-people-quit-smoking-through-financial-rewards/#using-aws-cloud-to-scale","title":"Using AWS Cloud to scale","text":"<p>We chose AWS as our cloud provider because AWS provided the necessary tools to help us build a HIPAA-compliant platform. The plug-and-play nature of AWS architecture helped us iterate faster as a startup with a small team.</p> <p></p> <p>Let\u2019s explore the main components behind our architecture.</p> <ul> <li>We embrace microservice architecture that runs on top of AWS Fargate. Fargate helps us a lot to avoid the operational burdens of managing servers and allows us to scale, meeting the growing needs of our workloads.</li> <li>Amazon Relational Database Service (Amazon RDS) is our main transactional database. Amazon ElastiCache complements our core database in performance by caching read-heavy data.</li> <li>We use Amazon Redshift as the central data warehouse and Amazon S3 as a scalable data lake, merging all application data with events, engagement data, device data, and rewards for analytics and understanding our participants better.</li> <li>Amazon Chime SDK helped us build audio/video communication tools faster, which are leveraged by our health coaches to communicate and keep a personal connection with participants.</li> </ul>"},{"location":"2022/03/04/helping-people-quit-smoking-through-financial-rewards/#going-forward","title":"Going Forward","text":"<p>Due to our personalized smoking cessation platform built on AWS and our amazing health coaches, we have been able to launch successful clinical validations with leading institutions such as Boston Medical Center and their research has been accepted by a prestigious American Thoracic Society journal.</p> <p>Our strategic partners and individual participants are seeing higher program satisfaction rates of 82% Net Promoter Score, 68% reduction in tobacco usage across the population when CO (Carbon Monoxide) was measured objectively using the devices, and higher program engagement rates where on average 7.4 weekly touchpoints were measured during the clinical trial.</p> <p>AWS proved to be the best architecture for Vincere Health because it provides capabilities to allow us to comply with HIPAA with ease and confidence. Achieving our goal started with data gathering and learning more from participants. Our next steps are to continue to build our platform and add more intelligent layers on the platform using Machine Learning and advanced data gathering techniques to scale our program and reach out to participants proactively who are in the need of the most.</p>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/","title":"How Claude Opus 4.5 Gave Me a Perfect Tmux Setup","text":""},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#how-claude-opus-45-gave-me-a-perfect-tmux-setup","title":"How Claude Opus 4.5 Gave Me a Perfect Tmux Setup","text":"<p>I started with Zellij. The learning curve was low, commands were intuitive, and I adopted it quickly. Since I've abandoned IDEs for the terminal, having a solid multiplexer was essential.</p> <p></p>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#why-i-left-zellij","title":"Why I Left Zellij","text":"<p>No complaints about Zellij - the team is excellent and they've built something great. But it felt overdone for my taste. Too much padding, heavy borders, chunky tabs. I could have restyled it through their KDL config files, but I never explored that path.</p> <p>Tmux out of the box is minimal. Almost nothing going on. That's exactly what I wanted.</p>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#the-claude-opus-45-experience","title":"The Claude Opus 4.5 Experience","text":"<p>I fed Claude my Zellij config and asked it to replicate the same keybinding experience in Tmux. I was already comfortable with Zellij's modal approach:</p> <ul> <li><code>Ctrl+P</code> \u2192 Pane mode (split, navigate, close)</li> <li><code>Ctrl+T</code> \u2192 Tab mode (new, rename, switch)</li> <li><code>Ctrl+Y</code> \u2192 Resize mode</li> <li><code>Ctrl+M</code> \u2192 Move mode</li> </ul> <p>5-10 messages. That's it. Claude understood what I wanted and gave me a config that matched my muscle memory while keeping Tmux's minimal aesthetic.</p>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#the-config","title":"The Config","text":"<p>Here's what Claude designed (full config):</p>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#theme-catppuccin-mocha-transparent","title":"Theme (Catppuccin Mocha, Transparent)","text":"<pre><code># Minimal transparent status bar\nset -g status-bg default\nset -g status-style \"bg=default\"\nset -g status-left \"\"\nset -g status-right \"#[fg=#45475a,bg=default]#[fg=#cdd6f4,bg=#45475a]  #S #[fg=#45475a,bg=default]\"\n\n# Window styling\nset -g window-status-format \"#[fg=#45475a,bg=default]#[fg=#a6adc8,bg=#45475a] #I #W #[fg=#45475a,bg=default]\"\nset -g window-status-current-format \"#[fg=#89b4fa,bg=default]#[fg=#1e1e2e,bg=#89b4fa,bold] #I #W #[fg=#89b4fa,bg=default]\"\n</code></pre>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#modal-keybindings","title":"Modal Keybindings","text":"<pre><code># Root triggers\nbind -n C-p switch-client -T pane\nbind -n C-t switch-client -T tab\nbind -n C-y switch-client -T resize\nbind -n C-m switch-client -T move\n\n# Pane mode (Ctrl+P, then...)\nbind -T pane h select-pane -L\nbind -T pane j select-pane -D\nbind -T pane k select-pane -U\nbind -T pane l select-pane -R\nbind -T pane n split-window -h\nbind -T pane d split-window -v\nbind -T pane x kill-pane\nbind -T pane f resize-pane -Z\n\n# Tab mode (Ctrl+T, then...)\nbind -T tab h previous-window\nbind -T tab l next-window\nbind -T tab n new-window\nbind -T tab x kill-window\nbind -T tab r command-prompt -I \"#W\" \"rename-window '%%'\"\n</code></pre>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#session-restore","title":"Session Restore","text":"<pre><code>set -g @plugin 'tmux-plugins/tmux-resurrect'\nset -g @plugin 'tmux-plugins/tmux-continuum'\nset -g @continuum-restore 'on'\nset -g @resurrect-processes 'lazygit nvim \"~claude\" \"~codex\" cursor-agent'\n</code></pre>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#shell-aliases","title":"Shell Aliases","text":"<p>I added these to my <code>~/.zshrc</code> for quick session management:</p> <pre><code>alias tm='tmux'\nalias tms='tmux new-session -s'\nalias tma='tmux attach -t'\nalias tmd='tmux kill-session -t'\n# tm ls to list sessions\n</code></pre>"},{"location":"2025/12/05/how-claude-opus-gave-me-perfect-tmux-setup/#the-result","title":"The Result","text":"<p>Couldn't be happier. The setup is clean, the keybindings match what my fingers already knew, and session restore means I never lose my workspace.</p> <p>This is exactly what AI should be good at: taking your preferences and translating them into configuration you'd never have the patience to write yourself.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/","title":"How LLMs Revolutionized My Productivity","text":""},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#how-llms-revolutionized-my-productivity","title":"How LLMs Revolutionized My Productivity","text":"<p>In the ever-evolving landscape of LLMs, I've observed two distinct camps: the doomsayers who predict a dystopian future and the overly optimistic who claim AI has completely transformed their lives overnight. As for me? I find myself somewhere in the middle \u2013 cautiously optimistic about the technology's potential while actively seeking ways to harness it for practical, everyday use.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#personal-perspective","title":"Personal Perspective","text":"<p>Let's face it: as engineers, tech managers, and technical entrepreneurs, we often find ourselves bogged down in mundane, repetitive tasks. But now, with LLMs, we have the opportunity to offload these tasks. This shift allows us to free up our mental bandwidth, zoom out, and focus on the bigger picture.</p> <p>For me, being an engineer isn't about hiding behind a screen, churning out code in isolation. It's about stepping out, collaborating, and leveraging every tool at our disposal \u2013 AI included \u2013 to create solutions that actually make a dent. Understanding the business needs, questioning why we're building something in the first place, and focusing on user-centric experiences are all part of our job. We need to work hand-in-hand with product and business teams, not in our own little bubble. What gets me fired up is how AI-powered automation of those tedious tasks gives me more headspace to see the big picture and contribute more meaningfully to the product's success.</p> <p>Since Large Language Models (LLMs) hit the scene, my productivity and problem-solving approach have taken a serious leap. Here's how AI has changed my workflow and allowed me to zoom out on the bigger picture:</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#automating-and-streamlining-engineering-tasks","title":"Automating and Streamlining Engineering Tasks","text":"<p>LLMs have become my go-to tool for handling both monotonous tasks that once consumed hours of my day:</p> <ul> <li> <p>JSON Data Wrangling: Most of the engineering tasks involve parsing JSON data, reading API docs, or understanding interfaces to consume. I now use LLMs to write Python/TypeScript code for these tasks, significantly speeding up the process of integration.</p> </li> <li> <p>Database Schema Design: Once I understand the business problem, I use AI to design SQL tables or NoSQL collections. By providing the current model layout and structure, AI can reason through the current state and suggest changes. This allows me to spend more time understanding business objects required, and what I'm trying to solve, while AI automates the table/collection creation process.</p> </li> <li> <p>REST API Development: Let's be honest, creating basic APIs can sometimes feel like a task a well-trained monkey could do. Now, I use LLMs to write controllers, define REST endpoints, and create service classes. This shift allows me to focus more on defining the spec and questioning why I need an endpoint in the first place, rather than getting bogged down in repetitive code.</p> </li> <li> <p>Documentation: Documentation was always an area where I struggled, as it often felt like a task easy to procrastinate on. With AI, I can now provide my schema definitions, API designs, and code running instructions, and it can automate much of the documentation process. Creating clear, comprehensive documentation for projects is now a collaborative effort with AI. I also use it to document infrastructure and security policies when I'm involved in documenting security architecture.</p> </li> <li> <p>Debugging: A lot of time is often consumed by environment issues. While AI/LLMs aren't yet fully aware of specific environments (though this is changing with tools like Cursor, GitHub Copilot Workspace, and Gemini), I can now provide error codes and stack traces to the model. It then suggests next steps and potential fixes, streamlining the debugging process. By describing an error or unexpected behavior, I often receive insightful suggestions for potential fixes or areas to investigate.</p> </li> <li> <p>Project Bootstrapping: Starting a project from scratch often involves dealing with boilerplate code and repetitive setup tasks. Whether it's setting up a REST API, initializing a React/Next.js project, or writing basic components, LLMs can generate the foundational code structure. This allows me to focus on the unique aspects of the project rather than getting bogged down in setup details.</p> </li> </ul> <p>It's worth noting that by no means are LLMs perfect at these tasks. But with the right question or prompt, they do a pretty good job for me. Often, asking the right question is harder than finding the answer or solution. This is where spending more time is usually better, regardless of the approach.</p> <p>Your workflow might differ in a larger enterprise, but I think bigger companies are already moving in this direction with local models and such.</p> <p>It's also important to note that the tasks I've mentioned primarily involve automating routine, repetitive work. AI hasn't replaced the need for thoughtful research, architecting complex systems, or writing intricate, business-critical code. These higher-level tasks still require human expertise and creativity.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#tools-i-use-daily","title":"Tools I Use Daily","text":"<p>Here's a rundown of the AI-powered tools that have become indispensable in my workflow:</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#cursor","title":"Cursor","text":"<p>Cursor, paired with Claude Sonnet 3.5 or GPT-4o, is my go-to for coding tasks. As a former VSCode user, I appreciate Cursor's familiar yet enhanced IDE experience:</p> <ul> <li>The Command + K shortcut is far more useful than auto-complete or GitHub Copilot's sometimes intrusive suggestions.</li> <li>Contextual prompts using @file, @folder, and @codebase shortcuts are incredibly intuitive.</li> <li>While the web-search feature needs improvement, I'm optimistic about its potential.</li> <li>For new projects, I've transitioned from gpt-engineer to Composer, though the file modification UX still has room for growth.</li> </ul> <p>I spend a significant portion of my time crafting detailed specs in Markdown. This not only creates solid documentation but also helps me formulate better prompts for AI assistance. The @shortcuts for referencing are particularly helpful here.</p> <p>A lot of my focus goes into creating robust data models (e.g., SQLAlchemy models, TypeScript interfaces, Pydantic or Prisma schemas). Well-defined data objects are crucial for maintaining code health and hygiene.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#warp-terminal","title":"Warp Terminal","text":"<p>After years of using iTerm2, I've switched to Warp Terminal. Its speed (thanks to being written in Rust) and features like Command + I for quick AI question and environment queries have won me over.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#web-based-ai-tools","title":"Web-based AI Tools","text":"<p>Needles to say, I regularly use GPT, Claude's console, and Perplexity for web searches and more complex QnA.</p>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#areas-where-id-like-to-see-ai-improve","title":"Areas Where I'd Like to See AI Improve","text":"<p>While AI has significantly boosted my productivity, there are still areas where I believe AI-powered tools have room for improvement. Here are some domains where I'm eagerly anticipating advancements:</p> <ul> <li> <p>UX Design: As engineers, we often lack good instincts about user experience. I'm constantly looking for better ways to translate requirements into rough mockups, but haven't found the perfect AI tool for this yet. Figjam is trying to get there, but I did not have too much success with it</p> </li> <li> <p>Legacy Code Translation: Dealing with large, legacy codebases remains a challenge. While this could be a standalone product, I'm hoping future models like Gemini will have enough context to reason through complex systems. The main hurdle seems to be creating a user-friendly interface for interacting with extensive file systems.</p> </li> <li> <p>Complex SQL Queries: AI still struggles with intricate SQL models and data warehouses involving numerous objects and relationships. This likely stems from the contextual knowledge required, and it's unclear when AI will be able to fully tackle this complexity.</p> </li> <li> <p>Cloud Infrastructure: While tools like AWS, GCP, and Azure are making strides with AI integration (e.g., StarAgent and Gemini), we're not yet at a point where AI can independently create infrastructure, set up alerts, and debug cloud environments. The main limitations are lack of context and incomplete tooling.</p> </li> <li> <p>Improving Soft Skills in an Async World: This might be very aspirational, I'm curious about how AI could enhance soft skills by analyzing communication patterns, suggesting improvements in correspondence, and providing real-time feedback during virtual meetings. While AI could potentially assist with understanding meeting context, streamlining documentation, and generating talking points, it's crucial to remember that genuine human connection and empathy remain irreplaceable in developing strong professional relationships.</p> </li> </ul>"},{"location":"2024/08/11/how-llms-revolutionized-my-productivity/#zooming-out-the-bigger-picture","title":"Zooming Out: The Bigger Picture","text":"<p>With LLMs and a curious mindset, I've been able to accomplish more. I believe you can too. In my humble opinion, our day-to-day workflows as engineers need to evolve. We need to work more effectively, though what that means may differ between large tech companies and startups/scale-ups. While I may not be best suited to advise someone working in a larger company, if you're a builder creating products, you might relate to my experience.</p> <p>Here's how I see the role of engineers expanding:</p> <ul> <li> <p>Become a Product Engineer: Interface with both business and technical teams, write code faster, and produce documentation quickly. Lead or be an influential individual contributor</p> </li> <li> <p>Dive Deeper into Business Processes: Learn the crux of how businesses operate. Leverage your technical vision and AI to iterate on products rapidly and complement business needs</p> </li> <li> <p>Connect Directly with Customers: Spend more time understanding customer needs. Don't solely rely on product managers to translate requirements; be the front-facing person yourself, you will build better</p> </li> <li> <p>Embrace Growth Engineering: This might be aspirational, but it's something I'm working towards. Understand the growth funnel and see how you can leverage AI tooling and your technical expertise to drive growth towards the product</p> </li> </ul> <p>Much of what I've mentioned above, points towards companies doing more with smaller teams. But isn't that the current trend? Instead of complaining or lagging behind, get ahead of the game. Use these tools to your advantage, rather than worrying about how they might replace you.</p> <p>Remember, these are just my personal opinions based on how I've used AI to my advantage. The key is to find what works best for you and your team.</p>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/","title":"Making responsive grid with Flexbox and LessJS","text":""},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#making-responsive-grid-with-flexbox-and-lessjs","title":"Making responsive grid with Flexbox and LessJS","text":"<p>Understanding FlexBox can be challenging in the start. But once you start understanding FlexBox it is really awesome. There are some really good resources out there on FlexBox. This one is the best that I found on medium. In this article, I will cover the concepts to create your own configurable Grid framework based on FlexBox.</p>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#motivation-behind-writing-grid-system-with-flexbox","title":"Motivation behind writing grid system with FlexBox","text":"<ol> <li>To have a minimal and light weight grid system.</li> <li>To have a highly configurable grid where I can control breakpoints and gutters.</li> <li>To align content easily e.g vertical alignment which is really easy to do with FlexBox.</li> <li>To understand FlexBox in more depth by writing about it.</li> </ol>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#wrapping","title":"Wrapping","text":"<p>To use FlexBox you have to wrap your children elements with the container having the flex property. By default, the flex flow will be row. That means all the children elements will be placed in one row.</p> <pre><code>    display: flex;\n}\n.column {\n    margin: 10px 1% 0 1%;\n    width:  calc(25% - 2%);\n    height: 150px;\n    background: #395B50;\n}\n&lt;div class=\"grid\"&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n  &lt;div class=\"column\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>So every child element has 25% width with the little adjustments of margin left and right. The more columns you add to the grid more smaller they will get. Since every column is being placed in one row.</p> <p></p> <p>This can be a good feature if you want non-stacking grid where every column is being accommodated in one row.</p> <p>That is something we don\u2019t want for a conventional gird. Since every row in the grid is 100% wide. It should accommodate only 4 children elements with 25% width.</p> <pre><code>.gird {\n    display: flex;\n    flex-flow: row wrap;\n}\n.column {\n    margin: 10px 1% 0 1%;\n    flex-basis: calc(25% - 2%);\n    height: 150px;\n    background: #395B50;\n}\n</code></pre> <p>The flex-flow sets the flow direction to row and enables wrapping if width exceeds 100%. The flex-basis property determines the size of the content-box.</p> <p></p> <p>After putting some content in columns grid looks like this.</p> <p></p> <p>All the columns are setting their height equal to the tallest column in the grid. This is because align-items by default is set to stretch which is stretching every column. Since we don\u2019t want this behavior for our grid. We\u2019ll override this property with flex-start and that will fix the issue.</p> <p></p>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#grid-system","title":"Grid System","text":"<p>Most of the UI frameworks use 12 columns grid. That means every row will have 12 columns and you can specify helper classes to expand each child in the grid to certain columns on different view-ports.</p> <p></p> <p>We will create a grid system having 12 columns in each row.</p>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#lets-unleash-the-power-of-css-pre-processor-lessjs","title":"Let\u2019s unleash the power of CSS pre-processor LessJs","text":"<p>To calculate the width of each column I am using this less mixin as a function. This mixin function calculates the flex-size of a column. On a 100% row, you can expand every children element to certain columns. You can also set custom gutters or no gutters later. Less mixin funcitons are really handy when you want to add logic to your CSS.</p> <p></p><pre><code>.flex-size(@col: 6, @gutter: 1%) {\n  flex-basis: (100% / (12 / @col)) - @gutter;\n}\n</code></pre> Sometimes you want to control gutters. Like in common dashboard interfaces with sidebar aligned to the left where you don\u2019t want any gutters between the sidebar column and the content column. Something like this.<p></p> <p></p> <p>For sidebar column I have used mixin .flex-size(3, 0) since it expands to 3 columns with 0% gutters and for content column .flex-size(9, 0).</p> <p>To expand elements to certain columns in different view-ports I wrote another mixin as a function that uses LessJS variable interpolation.</p> <pre><code>.viewport-columns(@screen-type: desktop) {\n  .@{screen-type}-three {\n    .flex-size(3);\n  }\n  .@{screen-type}-nine {\n    .flex-size(9);\n  }\n }\n</code></pre> <p>With the variable interpolation you can call this mixin on different breakpoints and can have helper classes like desktop-three, tablet-nine or mobile-twelve. With the power of variable interpolation you can have more classes like extra-small-phone-nine with only adding a new breakpoint with .viewport-columns(extra-small-phone).</p> <pre><code>@media (min-width: 991px) {\n  .viewport-columns(desktop);\n}\n@media (max-width: 991px) {\n  .viewport-columns(tablet);\n}\n@media (max-width: 661px) {\n  .viewport-columns(phone);\n}\n</code></pre>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#gutters","title":"Gutters","text":"<p>For creating custom gutters I have created these helper classes.</p> <pre><code>.no-gutters {\n  [class*='desktop-'], [class*='tablet-'], [class*='phone-'] {\n    margin: 0 0 10px 0;\n  }\n}\n.relaxed-gutters {\n  [class*='desktop-'], [class*='tablet-'], [class*='phone-'] {\n    margin: 0 1% 10px 1%;\n  }\n}\n// that also change the calculation of every column's width\n@media (max-width: 661px) {\n  .relaxed-gutters {\n     .viewport-columns(phone, 2%);\n   }\n}\n</code></pre> <p>Of all the celebrated features of FlexBox these are the ones I like the most.</p> <p>Row Reverse</p> <p></p> <p>You can reverse the columns in a grid</p> <pre><code>.grid.reverse {\n    flex-flow: row-reverse wrap;\n}\n.centered {\n    justify-content: center;\n}\n&lt;div class=\"grid centered reverse\"&gt;\n  &lt;div class=\"desktop-six tablet-six sky-blue\"&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;p&gt;1&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"desktop-six tablet-six orange\"&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;p&gt;2&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p></p> <p>Vertical Alignment With overriding the property of align-items from flex-start to center you can vertically align every column in the grid. Provided your layout have some defined height it should not be defaulting to height: auto;.</p> <pre><code>.vertically.aligner {\n    align-items: center;\n}\n</code></pre> <p>Centered Grid With overriding the property of justify-content, from inherit to center, you can horizontally center your columns in a row. justify-content only has an effect if there's space left over in the row. That means in the grid your elements are not taking whole 12 column space.</p>"},{"location":"2017/07/29/making-responsive-grid-with-flexbox-and-lessjs/#conclusion","title":"Conclusion","text":"<p>I hope this article has given you a perspective to create a simple functional grid system based on FlexBox. With LessJs you can create helper classes easily to have a highly configurable grid system.</p> <p>Here is the Codepen where you can play with this grid system. Open it in a new tab to see columns expanding on different view-ports.</p> <p>Codepen To play with</p>"},{"location":"2024/08/19/my-desk-setup-2024/","title":"My desk setup in 2024","text":""},{"location":"2024/08/19/my-desk-setup-2024/#my-desk-setup-in-2024","title":"My desk setup in 2024","text":"<p>My desk setup as of writing this post. I've been working from home for the past 1 year and have been slowly evolving my setup to be more ergonomic and efficient.</p> <p>Also, I want to use this blog to track changes for my setup over time and share with fellow devs / team-mates.</p> <p></p>"},{"location":"2024/08/19/my-desk-setup-2024/#things-i-optimized-for","title":"Things I optimized for:","text":"<ul> <li>Minimalism: More stuff means more clutter. It's harder to clean and I am lazy.</li> <li>A setup I cannot make messy</li> <li>Ergonomics: I had neck and back problems with a two-monitor setup. I optimized for a viewing and sitting angle that gives me comfort.</li> <li>Standing desk: Honestly, I cannot do heads-down work while standing. But it's good to stand from time to time, and take calls while standing.</li> <li>A good keyboard and mouse that are ergonomic, durable, and give me good tactile feedback.</li> <li>Affordability: Something that doesn't cost me a lot and make my wife angry. I did a DIY desk to keep it cheap.</li> <li>Cable management: I wanted to live a single cable life if possible.</li> </ul>"},{"location":"2024/08/19/my-desk-setup-2024/#desk-setup","title":"Desk Setup","text":"<p>To give a minimal look, I tried to keep the setup black and white, mostly white to maximize the amount of light coming from the windows.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#desk","title":"Desk","text":"<p>I DIYed my desk to avoid paying crazy money while having a durable setup, and something I could change over time.</p> <ul> <li>TOPSKY electric motor legs: I can adjust the length of the tabletop to my liking, but Boston apartment space is limited. Can also use a butcher block top in the future.</li> <li>Ikea Table Top: $20 tabletop. </li> <li>Under desk cable management tray: Where I can hide my cables. Also gives me the ability to move my desk around. I wanted to live a single cable life if possible.</li> <li>Cable clips for two USB-C ports on the desk, one for the laptop and other for charging.</li> <li>Orbit desk mat given to me by a good friend, so technically free for me.</li> <li>Wooden vertical laptop stand: I never open up my laptop once it's on the desk.</li> </ul>"},{"location":"2024/08/19/my-desk-setup-2024/#desk-chair","title":"Desk Chair","text":"<p>I have an ergonomic chair from Autonomous that I bought in 2020 for 400 bucks.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#monitor","title":"Monitor","text":"<p>I used to be a two-monitor guy. I started feeling subtle pain in my neck due to the angle and tilt of the monitors. It was unnoticeable at first. I switched to one giant curved monitor where my viewing angle is more natural and centered, and all the pain went away.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#lg-38-inch-curved-monitor-i-cannot-recommend-this-monitor-enough-its-a-game-changer","title":"LG 38-inch curved monitor. I cannot recommend this monitor enough. It's a game-changer.","text":"<ul> <li>This monitor comes with an excellent ergonomic stand that can be adjusted in multiple directions. I've tried many ergonomic arms, but the quality is never good. LG makes one of the best solid ergo stands. The black color monitor stand on the white desk gives a good contrast too.</li> <li>This monitor also helped with my cable management. I have one USB-C cable that comes out of the monitor to my laptop. Laptop charging is built into the monitor. My camera, mic, and other lights are hooked to the monitor too. I can't stop recommending this monitor.</li> </ul>"},{"location":"2024/08/19/my-desk-setup-2024/#lights-attached-to-monitor","title":"Lights attached to monitor","text":"<p>I wanted to have an aesthetically pleasing light setup, while having good utility too. - LED strip on the back of the monitor, plugged into the monitor too. - For reading or whenever I need light (very rare though), I use BenQ ScreenBar Halo. The temperature and light controls are very good. Not sure if I recommend it for the price. I got it as a gift.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#keyboards","title":"Keyboard(s)","text":"<p>I use Logitech MX Mechanical keyboard. This keyboard is excellent. While it gives you excellent tactile feedback, unlike other mechanical keyboards, the build quality is excellent. Logitech software and the battery life are excellent too. Bluetooth lag and wireless connectivity are top-notch.</p> <p>Keyboards that are expensive and I don't use day-to-day (displayed on the wall to give a feel to the room, got them for fun and aesthetically pleasing sounds, less utility):</p> <p></p> <ul> <li>Anne Pro 2: Cute 60% keyboard. Less utility.</li> <li>Keychron C1 Tenkeyless: Cute too, but elevated for me. Gave me wrist pain.</li> </ul>"},{"location":"2024/08/19/my-desk-setup-2024/#mouse","title":"Mouse","text":"<p>I use Logitech MX Master 3S. I like the scroll wheel and the build quality. I highly recommend it. Bluetooth lag and the build quality are excellent.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#mic-setup","title":"Mic Setup","text":"<p>I wanted a mic with good sound quality, but I didn't want to go through the hassle of setting it up. I use the Blue Yeti X, which is plug-and-play.</p>"},{"location":"2024/08/19/my-desk-setup-2024/#camera","title":"Camera","text":"<p>I wanted a good quality camera but didn't want to go through the hassle of setting it up. I use this Anker PowerConf C300.</p> <p>That's it, folks! More changes to come...</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/","title":"Navigating the AI Hype and Thinking about Niche LLM Applications","text":""},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#navigating-the-ai-hype-and-thinking-about-niche-llm-applications","title":"Navigating the AI Hype and Thinking about Niche LLM Applications","text":"<p>Recently, there has been a surge of enthusiasm surrounding large language models (LLMs) and generative AI, and justifiably so: LLMs have the power to revolutionize entire industries. Yet, this enthusiasm often gives rise to inevitable hype. It appears somewhat counterintuitive to avoid incorporating \u201cAI\u201d into a product\u2019s presentation, considering the immediate market interest it can generate.</p> <p>It\u2019s funny how we sometimes get caught up in the thrill of flashy new tools, losing sight of what really matters \u2014 solving actual problems.</p> <p>In this article, I\u2019m not discussing ChatGPT prompts that promise to transform you into a 10X person, grant you a competitive edge, or make you fear being replaced by AI aficionados at work. However, I do recommend learning prompting techniques.</p> <p>For example, check out this excellent\u00a0free course on prompt engineering by Andrew Ng or dive into intriguing papers\u00a0like this one\u00a0to discover effective prompting patterns and always there is enough about prompts on Twitter. Prompting is an excellent technique to get a lot out of LLMs.</p> <p>We\u2019ll delve into using LLMs for specialized tasks with enterprise or organizational private data, like\u00a0question-answering, summarization, clustering, recommendations, and crafting conversational/neural search experiences.</p> <p>I\u2019ve decided to jot down what I\u2019ve learned for myself and the team I work with, and I thought, why not share it through this article? In our upcoming posts, we\u2019ll be chatting about topics like LLM Chains, Intent Recognition, adding task-specific tools, clustering, creating a recommendation system,\u00a0fine-tuning open-source LLMs, and our way of testing the system, which we are continuously improving and learning from the research and open-source community. So, stick around, and let\u2019s explore these ideas together!</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#augmenting-llms-with-private-domain-specific-data","title":"Augmenting LLMs with private domain-specific data","text":"<p>LLMs, such as\u00a0GPT, and other\u00a0open-source LLMs, are exceptional technologies for\u00a0knowledge generation and reasoning. Trained on extensive public data, these foundational models can be adapted for diverse tasks. Two common paradigms have emerged to tackle domain-specific problems and incorporate private/external knowledge:</p> <ol> <li>Fine-tuning\u00a0a pre-trained model for domain-specific applications involves training the model on a particular dataset using hosted large language model APIs or open-source foundational models like Llama. This process incorporates existing foundational datasets and augments or aligns the model with domain-specific supervised or unsupervised data, depending on your use case. However, we won\u2019t discuss this paradigm in the current article; I plan to write a separate piece on this topic.    It\u2019s worth noting that this approach can be expensive, as fine-tuning typically requires training on costly GPUs if using an open-source model or utilizing high-priced Azure or OpenAI endpoints.\u00a0Although\u00a0emerging patterns like\u00a0PEFT\u00a0and\u00a0LORA\u00a0now enable training open-source LLMs on smaller GPUs, aligning models to specific domains such as healthcare or finance remains a challenge.\u00a0This area necessitates extensive testing, and we\u2019re still in the process of deepening our understanding.</li> <li>The second emerging paradigm is a\u00a0context-augmented generation\u00a0or often mentioned as\u00a0Retrieval Augmented Generation (RAG), wherein\u00a0the relevant context is\u00a0incorporated into the input prompt, leveraging the LLM\u2019s reasoning capabilities to generate a response.    In this article, we will concentrate on this technique and explore methods for embedding domain-specific private data. Regardless if you take option 1, RAG still has its place because most of the LLMs are frozen at a certain point, where you still need RAG to connect to your LLM with ever-growing or fresh data-set through search, etc.</li> </ol>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>The concept is straightforward:\u00a0use semantic search or other retrieval techniques\u00a0to extract pertinent documents from your corpus, then\u00a0feed the relevant text segments into the language model (LM) as prompts or contexts.\u00a0This allows the LM to reason, answer questions, and generate specific, relevant content without deviating from unrelated topics. Careful prompt design and model temperature management help keep the LM focused. This approach is highly effective for many\u00a0basic applications,\u00a0eliminating the need to fine-tune\u00a0the LM or complicate the deployment process, as\u00a0most of the current LMs are frozen and don\u2019t have live data access.\u00a0Connecting the LM to external data sources becomes a powerful tool for addressing various queries.\u00a0Integrating the few-shot prompting technique with retrieved text segments can further enhance the model\u2019s ability to generate accurate responses.</p> <p>The\u00a0retrieval process and corpus organization play crucial roles\u00a0in this approach, as\u00a0how documents are segmented, queried, and correlated significantly impacts the relevance of the content fed to the LM based on user queries.\u00a0Document ranking processes\u00a0also help ensure the most pertinent information is provided to the LM. Numerous open-source frameworks are available to facilitate these tasks. To optimize results, it is essential to be pragmatic in selecting the appropriate techniques and understanding when and where to employ them.\u00a0Additionally, considering your organization\u2019s machine learning operations (MLOps) processes is crucial for seamless integration and deployment.</p> <p>Embedding is a process of capturing\u00a0the semantic meaning of the text into numerical vectors.</p> <p></p> <ol> <li>In this process,\u00a0embeddings are generated for search document segments and later fed into LLM.\u00a0Embeddings transform the text into vector representations, situating them within a high-dimensional space.\u00a0For those familiar with natural language processing (NLP) prior to the LLM era or before the advent of Google BERT, models such as Word2Vec and GLOVE may come to mind.    It is important to acknowledge the\u00a0remarkable advancements made by the research community,\u00a0OpenAI, Cohere, and others in refining embedding techniques and in making them more developer friendly or accessible.\u00a0These\u00a0embeddings can be stored in common vector stores, Redis, Postgres, Vector database, or in-memory NumPy arrays, to facilitate further analysis and manipulation.</li> <li>User queries are\u00a0similarly vectorized\u00a0after converting relevant documents into vector representations and storing them in a vector store.\u00a0To retrieve the most pertinent document segments based on the user\u2019s query,\u00a0a nearest neighbor search, often employing the k-NN algorithm, is\u00a0performed within the vector space, usually using a Cosine similarity score\u00a0or other distance calculation techniques.    This process ensures that the\u00a0most closely related content is identified and retrieved in response to user inquiries. Furthermore,\u00a0ranking or\u00a0re-ranking based on relevance scores can be applied\u00a0at this stage to\u00a0enhance the accuracy of the results.</li> <li>A\u00a0comprehensible response is generated by feeding the\u00a0top-K relevant segments to the language model (LM).\u00a0The\u00a0LM\u00a0enhances the neural search process by reasoning\u00a0from the provided document segments and the\u00a0prompt for steerability of the response, ultimately producing a highly human-like response.</li> </ol> <p></p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#data-preprocessing-chunking-and-retrieval-techniques","title":"Data-preprocessing, chunking, and retrieval techniques","text":"<p>Data-preprocessing The nature of your data necessitates careful pre-processing to ensure optimal results.\u00a0It\u2019s essential to\u00a0eliminate extraneous information, such as redundant headers and footers, HTML tags in HTML pages, PDFs, and other documents.\u00a0Minimizing noise in the data\u00a0can\u00a0enhance the accuracy and relevance of query results\u00a0more effectively. These fundamental data-preprocessing steps are not unique to this context; they are\u00a0essential for constructing any efficient information retrieval system.</p> <p>Chunking Chunking\u00a0involves\u00a0breaking down larger portions of documents into smaller text blocks, which is a\u00a0crucial data-preprocessing step\u00a0before creating a vector store from the corpus.\u00a0This process is important for two main reasons:</p> <ol> <li>Optimizing the accuracy of user search results by removing noise from the corpus,\u00a0converting them into smaller relevant chunks, and retaining only the most relevant text without losing context\u00a0or essential meaning.</li> <li>Adhering to token limits imposed by embedding models and the constraints of the LLM\u2019s\u00a0context must be considered.</li> </ol> <p>To\u00a0determine the most effective\u00a0chunking approach, you\u2019ll need to\u00a0experiment with various techniques\u00a0that account for the nature of your data.\u00a0Defining\u00a0chunk size, chunk boundaries, and overlap is essential.\u00a0Begin by analyzing the characteristics of your data and adjusting the techniques accordingly.</p> <p>A note on pricing: When opting not to use an open-source, self-hosted embedding model, it\u2019s crucial to be cognizant of the potential costs associated with proprietary models, such as those from OpenAI. Although the\u00a0per-token cost might seem inconsequential (for instance, $0.004 per token), large datasets can add up to substantial expenses.</p> <p>Let\u2019s do some quick calculations to illustrate this point. Suppose you have 100,000 documents (PDFs, CSVs, etc.), each with an average of 70,000\u00a0tokens. It\u2019s essential to thoroughly\u00a0remove noise and strategically chunk data, or perhaps start small to manage costs. It\u2019s vital to\u00a0avoid the repetitive regeneration of embeddings due to minor oversights such as inefficient chunking, inadequate application of segment overlaps, or subpar data pre-processing. These seemingly small errors can quickly become expensive in time and resources. Also, here\u2019s a rough estimate of the scenario discussed above:</p> <p>(70,000 tokens/document * 100,000 documents) / 1000 * ($0.0004 per 1000 tokens for embeddings API) = $2,800 for using the embeddings API.</p> <p>Here are a few techniques for\u00a0chunking data effectively. Additionally,\u00a0this article from Pinecone\u00a0provides an excellent guide on implementing these chunking methods proficiently.</p> <p>**Create segments of paragraphs/sentences ** The simplest method for dividing text into chunks is by using a fixed size, which can be based on the number of words or tokens. For example, in\u00a0Python, you can perform a rudimentary\u00a0<code>text.split()</code>\u00a0operation to obtain words or sentences,\u00a0create\u00a0chunks based on word count up to a specified maximum, or use\u00a0<code>re.split(r\u2019\\n(?=[A-Z])\u2019)</code>\u00a0to\u00a0separate paragraphs. Count the words and create additional paragraphs as needed to ensure that\u00a0paragraphs don\u2019t exceed the maximum size.\u00a0You can use\u00a0Spacy\u00a0or\u00a0NLTK\u00a0for sophisticated sentence segmentation\u00a0instead of our naive regex or word split only. Alternatively, you can use\u00a0token-based length calculations\u00a0with a function like\u00a0OpenAI\u2019s tiktoken.</p> <p>However, this\u00a0approach may result in incomplete paragraphs or sentences, potentially losing context among the various segments\u00a0ultimately stored in the database.</p> <p>Overlapping segments To mitigate the issue of incomplete paragraphs or context loss among various document segments, we can\u00a0employ a\u00a0technique that generates overlapping segments. For instance, you could\u00a0overlap at least five segments together. Utilizing the mentioned method, create segments and overlap a minimum of five paragraphs or segments into one chunk until the maximum size is reached. Repeat this process to cover overlapping content across the five segments.</p> <p>While\u00a0this approach helps preserve context among segments, it may also substantially increase costs, as more embeddings will be generated. But this\u00a0approach does\u00a0help to some context with the problem of losing the context among segments.</p> <p>Choosing the right tools for storing and querying vectors The landscape of tools for vectors and vector databases has grown rapidly. The choice truly depends on which tool aligns best with your specific objectives. Key considerations include whether you require\u00a0a self-hosted solution or a managed vector store. Additionally, if opting for a hosted solution, it\u2019s crucial to check if it satisfies your compliance requirements, particularly if you\u2019re\u00a0operating in a heavily regulated environment like healthcare or fintech.</p> <p>We\u2019ve tried the following\u00a0tools and\u00a0vector databases:</p> <ul> <li>FAISS, as\u00a0defined by Facebook, is an open-source library designed\u00a0for efficient similarity search and clustering of dense vectors. It\u2019s\u00a0not a vector database, but it\u00a0supports popular algorithms such as k-NN and nearest neighbor search. The API is user-friendly and straightforward.\u00a0However, it\u2019s important to note that you\u2019ll need to host FAISS independently on a GPU or server yourself.</li> <li>Pinecone, fully\u00a0managed vector database\u00a0that has gained considerable popularity recently. It\u00a0supports k-NN and other distance metrics such as cosine, dot product, and Euclidean distance, which are easily configurable via their user interface. Pinecone\u2019s high availability is a strong selling point, but\u00a0its\u00a0metadata search functionality, which sets it apart from many other tools, is similar to MongoDB-style JSON queries.   This feature enables not only vector-based search but\u00a0also filtering of metadata JSON, for example, by user ID or specific attributes. Currently,\u00a0we\u2019re uncertain about their support for compliance standards such as PCI-DSS and HIPAA.</li> <li>Managed OpenSearch\u00a0by\u00a0AWS\u00a0is another viable option that\u00a0supports vector querying and popular algorithms like k-NN. OpenSearch could be an efficient alternative if you\u2019re operating in a regulated environment and prefer not to manage FAISS GPU instances on your own.</li> </ul> <p>The list of semantic search-based retrieval tools provided here is not exhaustive. I have shared it based on our experiences and the outcomes of our evaluations, providing insights into the tools we\u2019ve tried and tested.</p> <p>I\u2019d also like to note that a\u00a0semantic search-based retrieval system isn\u2019t the only viable solution.\u00a0k-NN might not be correct algorithm always, maybe all you need is\u00a0approximate search a-NN. Depending on your needs, other methodologies may prove more effective. For instance, if your task involves comparing two documents or pieces of information, alternative strategies may be better suited.</p> <p>Perhaps you\u2019re\u00a0looking to model a\u00a0network or knowledge graph, or us a graph database such as Neo4J. Or, maybe your task involves\u00a0querying a website for specific information. e.g\u00a0DuckDuckGo\u00a0library for search functionality\u00a0or even a\u00a0proprietary API returning JSON within your organization.</p> <p>The key is to select the approach that best aligns with your specific objectives, and then feed the information to LLM for response generation and reasoning.</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#zero-shot-vs-few-shot-prompting-and-steering-llms","title":"Zero-shot vs few shot prompting and steering LLMs","text":"<p>The primary distinction between zero-shot and few-shot learning lies in the\u00a0approach to inference.\u00a0In\u00a0zero-shot learning, the language model is asked to\u00a0complete a task without explicit examples, whereas, in\u00a0few-shot learning, the\u00a0model is provided with a limited number of examples, or \u201cfew-shots,\u201d\u00a0embedded within the prompt.\u00a0Prompts are very important to steer the model in a direction and avoid hallucinations.\u00a0Besides prompts, the\u00a0temperature setting for the LLM also plays an important role.</p> <p>Consider the following example: We have a QnA bot tasked to\u00a0answer questions from the provided context retrieved using a vector store and user\u2019s question query. We want the LLM to only answer based on the context.</p> <pre><code>prompt_prefix = '''  \nAnswer only from the content that have been provided to you in the context,  \nother wise reply \"I cannot answer\". You are a QnA assistant\n\nContext:  \n{retrieved_context_from_vector_store}  \n'''\n</code></pre> <p>Zero-shot prompting example combines with context as a prefix In this example, we\u00a0won\u2019t give any explicit or prior examples of LLMs answering the question. Usually, you get good performance on zero-shot tasks as well\u00a0unless you want LLM to answer questions in a particular manner</p> <pre><code>zero_shot_prompt = prompt_prefix + '''  \nQuestion: {user_question}  \nAnswer: '''\n</code></pre> <p>Few-shot prompting example combined with context as a prefix If you aim to\u00a0guide the model to respond in a specific format, employing\u00a0few-shot prompting\u00a0techniques could be beneficial. By\u00a0providing a handful of examples demonstrating the desired response style.\u00a0Remember,\u00a0prompt crafting may require experimentation, and the optimal approach could vary depending on the particular Large Language Model (LLM) in use.</p> <pre><code>few_shot_prompting = prompt_prefix + '''  \nQuestion: Who was the president in 2015?  \nAnswer: The name of the president is Barak Obama\n\nQuestion: Tell me a joke  \nAnswer: sorry this is something not mentioned in the context\n\nQuestion: &lt;some domain-specific-question&gt;  \nAnswer: &lt;domain specific style of answering a question&gt;  \n'''\n</code></pre> <p>A note on\u00a0**tools and open-source frameworks**Tools like\u00a0Langchain\u00a0and\u00a0llama-index\u00a0are undoubtedly powerful, helping to reduce much of the labor involved in tasks like data loading, building indexes, and creating retrievers.\u00a0They are good for getting started or for building personal projects.</p> <p>Remember, the following is a very subjective experience and your mileage may vary.</p> <p>From our perspective, using these libraries in production systems might not be the best fit. They tend to obscure the underlying workings through many abstractions, limit customization possibilities, complicate memory management and retrieval customization, and sometimes need to be revised to reason about.\u00a0While working with LLMs is straightforward, we found\u00a0creating a few Python classes to meet our objectives more practical rather than investing significant effort into mastering a new\u00a0framework.</p> <p>The open-source ecosystem, particularly that of\u00a0Langchain, is truly impressive. It offers a wealth of\u00a0innovative ideas from the community that are worth exploring and integrating into your own projects. Also, data\u00a0loaders they offer are worth using because it\u2019s a lot of investment to write data scrapers or loaders or already existing tools yourself.\u00a0Do check out\u00a0llama-hub\u00a0or\u00a0LangChain\u00a0data loaders.</p> <p>Also, do\u00a0check out\u00a0Cohere, or\u00a0Sagemaker Jumpstart models. They both are great options to deploy models in your VPC if you work in a heavily regulated environment and want to use open-source LLM. We plan to do another article about how to deploy the model in a restricted environment and train it. If you want to use OpenAI models Microsoft\u00a0Azure OpenAI as a service\u00a0could also be a good option, although you will have to request access to it.</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#using-llms-for-recommendations-and-clustering-tasks","title":"Using LLMs for recommendations and clustering tasks","text":"<p>LLMs can capture the semantic meaning of a text or any data type through embeddings or numerical vectors in high-dimensional spaces, enabling the correlation of data once embedded. As previously discussed, we'll apply a\u00a0similar technique for embedding data to augment relevant response generation. Instead of generating responses, k-NN (nearest neighbours), a-NN, or any distance algorithms can be applied on vector stores,\u00a0etc., such as clustering users based on similar actions, product preference, or any other correlations depending on your dataset and domain.</p> <p>LLM-generated clusters\u00a0can produce more accurate categories than rule-based systems and potentially\u00a0address cold start problems.\u00a0They can work with\u00a0noisy un-labeled data sets, which can help us create collaborative filtering or content-based recommendation systems.</p> <p>Deep learning-based recommendation systems, e.g., that\u00a0embed user activity into vectors, have already been implemented at a petabyte of billions of actions scales by platforms like\u00a0Pinterest\u00a0and\u00a0YouTube. However, it\u2019s worth mentioning that\u00a0embedding techniques have significantly improved over time, making it easier to capture semantic meaning in data, and\u00a0with the advent of new vector query/storage tools,\u00a0it has become easier to deploy a recommendation system compared to what it used to be five years ago.</p> <p>This OpenAI Cookbook\u00a0is an excellent resource to help you get started and understand the implementation process. To create embeddings, you\u00a0don\u2019t have to rely solely on OpenAI embeddings model.\u00a0You can use any LLM, such as\u00a0BERT\u00a0or any open-source model,\u00a0sentence transformer, etc.,\u00a0that effectively captures the semantic meaning of your data.</p> <p>Following is the image generated using\u00a0OpenAI cookbook\u00a0mentioned above on this\u00a0data set. However,\u00a0these are high-dimensional embeddings, which are hard to visualize. They have used\u00a0t-SNE\u00a0to\u00a0compress them into two-dimensional space to visualize the nearest neighbor recommender concept.</p> <p></p> <p>Article description clusters generated using OpenAI\u2019s notebook</p> <p>Having said all of the above,\u00a0you may need to integrate your business logic and other rules to\u00a0create an effective recommendation system to develop a production-grade system.\u00a0It\u2019s crucial to consider the diversity of the data and rigorously test the system using both online and offline evaluation\u00a0metrics.\u00a0Online testing may involve A/B testing tailored to your domain, while offline metrics include measuring precision, recall, F1 scores, and assessing the data\u2019s diversity.</p> <p>Thinking about\u00a0collaborative filtering vs content-based models\u00a0and modeling user sessions etc., do add more complexity, and it\u2019s yet unclear how LLMs alone can solve that. Striking the right balance between these factors is essential for a successful recommendation system.</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#improving-search-experiences-within-organizations","title":"Improving search experiences within organizations","text":"<p>There\u2019s not much more to add here, but it\u2019s worth emphasizing that the above-mentioned embedding techniques, when paired with LLMs, can significantly enhance an organization\u2019s search experience. Moreover, the\u00a0RAG\u00a0approach can enable a more descriptive and user-friendly presentation of search results.</p> <p>If your interest goes beyond just LLMs, I\u2019d recommend this fascinating read from Pinecone explaining how\u00a0Spotify enhanced their podcast search experience. The concepts aren\u2019t new here, but the barrier to entry for creating superior experiences with less engineering effort has markedly decreased, thanks to LLMs and the new embedding techniques they unlock.</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#thinking-about-the-niche-applications-all-of-the-above-unlocks","title":"Thinking about the niche applications all of the above unlocks","text":"<p>Identifying the precise potential of Large Language Models (LLMs) for a particular organization is difficult; it largely\u00a0hinges on the unique datasets and the domain expertise for a niche.\u00a0For instance, a healthcare specialist might find a distinct set of use cases for LLMs, whereas a professional in fintech could leverage the technology in an entirely disparate manner.</p> <p>This broad applicability and adaptability is a key aspect that makes the LLM ecosystem so vibrant. For\u00a0organizations with extensive data assets, the\u00a0prospective advantages of deploying LLMs outweigh the associated risks, provided they are applied with caution and checks in place. We have previously discussed some use cases and their respective implementations.</p> <p>In addition, here are a few more worthy of mention:</p> <ul> <li>QnA and customer service chatbots:\u00a0I still remember the days of NLP pre-transformers, BERT or any LLM, using scikit-learn, Keras or sometimes Regex for intent recognition, using\u00a0Spacy/duckling for NER, and how hard it was to create domain-specific chatbots even with amazing frameworks like RASA.\u00a0And it does not matter what you do. You still used to get compromised UX for the end users. Companies will have an opportunity to refine their customer service using LLMs. UX is so much better with LLMs. Probably the best conversation humans had with the silicon chip.</li> <li>Corporate brain:\u00a0Glean kind\u00a0of product built for all the organization's Wiki and internal documents, except compared to Glean, it\u2019s better since it can reason and generate better responses.</li> <li>Zero-ETL use case.\u00a0I found this article by\u00a0Bar Moses really interesting. As LLMs become more powerful, seeing\u00a0how they disrupt data processes, massive ETL pipelines, and processes in place will be interesting.\u00a0Also, this\u00a0research paper is very interesting\u00a0if you want to read it. There still need to be more questions regarding creating\u00a0production-grade pipelines and the necessary measures to test them effectively.</li> <li>Data labeling and\u00a0classification\u00a0at scale.\u00a0If you are interested,\u00a0there is an interesting Jupyter Notebook\u00a0from\u00a0OpenAI on classification tasks through the model and comparing the results.\u00a0Also,\u00a0Cohere has an interesting endpoint for a similar use case.</li> <li>Specialized CoPilots:\u00a0Drawing inspiration from GitHub Copilot, there is potential for creating specialized CoPilots for various professions, including healthcare workers, financial analysts, and more.</li> <li>Summarizing content.\u00a0Summaries are good, although they did\u00a0drop a lot of relevant information\u00a0when we tried using them. There is substantial room for enhancement in this area.</li> <li>Capitalizing on untapped, unstructured data: Many organizations possess vast datasets but are still determining how best to utilize them. The\u00a0advanced embedding techniques that LLMs offer can assist these organizations in clustering, reasoning, and making nuanced use of their data.\u00a0Techniques like\u00a0RAG further enable them to operationalize this data for internal processes or end users.</li> <li>Boosting developer or employee productivity:\u00a0LLMs can undeniably enhance developer efficiency, often called the \u201810X\u2019 effect. However, organizations must proceed cautiously, taking lessons from incidents\u00a0like the one with Samsung.\u00a0This is where internal LLMs deployed within an organization\u2019s\u00a0Virtual Private Cloud (VPC)\u00a0could offer a valuable safeguard. It presents a compelling case for enterprises to deploy their own LLMs. This topic remains a lively debate, and seeing the industry's direction will be intriguing.</li> </ul> <p>The next article will explore building long-term user action memory and routing techniques for information retrieval. Hang on! I think I\u2019ll update this bit later, depending on which topic I manage to publish first.</p>"},{"location":"2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/#references","title":"References","text":"<ul> <li>In-Context Retrieval-Augmented Language Models</li> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li> <li>Text and Code Embeddings by Contrastive Pre-Training</li> <li>An Example-Driven Tabular Transformer by Leveraging Large Language Models</li> <li>Large Language Models can self improve</li> <li>PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest</li> <li>Language Models as Recommender Systems: Evaluations and Limitations</li> </ul>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/","title":"Designing a Patient Self-Reporting Stack Around Humans and AI","text":""},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#designing-a-patient-self-reporting-stack-around-humans-and-ai","title":"Designing a Patient Self-Reporting Stack Around Humans and AI","text":"<p>Every care team we talk to has the same complaint: patients happily text, leave voicemails, and fill out surveys, but those signals rarely make it into the plan of care.</p> <p>Electronic records were never built to absorb that ambient context, and the people who could act on it are already drowning in portal messages and follow-up calls. Yet the value is obvious, timely symptom reporting keeps people out of the ED, surfaces social needs, and lets providers adjust therapy before a flare turns into a crisis.</p> <p>What we need is a stack that captures self-reported data, triages it with large language models, and still gives clinicians the last word. The winning pattern blends thoughtful UX, observability, and a human-in-the-loop workflow.</p> <p>This piece is co-written with my co-founder (CEO) and co-author, Dr. Christian Pean, a rockstar clinician and operator I\u2019ve had so much fun building with at RevelAI Health.</p> <p></p>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#where-patient-self-reporting-stands-today","title":"Where Patient Self-Reporting Stands Today","text":"<ul> <li>Remote symptom logging works when someone is watching. In the RELIEF pilot, oncology patients completed 80% of daily check-ins and nurses intervened on half of the triggered alerts, zero symptom-related ED visits during the study window (Curr Oncol, 2021).</li> <li>PROM and PGHD programs still stall in pilots. An umbrella review catalogued fractured adoption because implementation teams underinvested in workflow design, training, and staff capacity (Syst Rev Protocol, 2024).</li> <li>Integrating patient-generated data into the EHR is barely out of the lab. Only 19 studies met the bar in a 2019\u20132021 scoping review, most of them diabetes pilots that cited resource load and inconsistent review patterns as blockers (JAMIA, 2021).</li> <li>Portals add work faster than they add value. Cleveland Clinic primary care teams saw quarterly message volume double (340 \u2192 695) and every 10 extra messages meant ~12 minutes more after-hours EHR time (J Gen Intern Med, 2024).</li> </ul> <p>The signal is rich, but the stack is brittle. We ask patients to talk and then make it the nurse\u2019s job to find needles in the haystack.</p>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#why-the-workflow-breaks","title":"Why the Workflow Breaks","text":"<ol> <li>Fragmented entry points: IVR trees, unsecured SMS threads, and chatbots all capture different slices of the story with no shared state.</li> <li>Review is entirely manual: care coordinators listen to voicemails, gather context from the chart, and often re-document the same complaint in yet another system.</li> <li>Outbound outreach is slow: when everything needs a human touch from scratch, patients feel ignored and high-acuity cases wait.</li> <li>No closed loop: patients don\u2019t hear back quickly, so engagement drops; clinicians don\u2019t trust the data, so they stop looking.</li> </ol> <p>We can\u2019t fix this by \u201cadding AI\u201d to a broken assembly line. We need to reframe the patient self-reporting stack as an orchestration problem.</p>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#patterns-for-a-human-centered-self-reporting-stack","title":"Patterns for a Human-Centered Self-Reporting Stack","text":""},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#1-intake-that-sets-expectations-up-front","title":"1. Intake that sets expectations up front","text":"<ul> <li>Large language models can score acuity quickly (VUMC\u2019s retrieval-augmented triage hit 0.98 sensitivity JAMIA, 2024).</li> <li>Patients hear a commitment (\u201ca nurse will join in under a minute if needed\u201d or \u201cwe\u2019ll text you an answer in two hours\u201d), which keeps engagement high.</li> </ul>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#2-inbox-triage-that-surfaces-context-not-noise","title":"2. Inbox triage that surfaces context, not noise","text":"<ul> <li>Inbox views should group by intent (med refill, wound concern, paperwork) and stack related history\u2014last summary, vitals, social determinants callouts.</li> <li>Drafting LLM replies isn\u2019t about replacing clinicians; Mayo Clinic teams used AI drafts in ~20% of portal responses and reported lower task load afterward (JAMA Netw Open, 2024).</li> <li>Auto-resolution stays limited to low-acuity scripts (e.g., PT protocol reminders) and still leaves an audit trail.</li> </ul>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#3-outbound-nudges-that-feel-bespoke","title":"3. Outbound nudges that feel bespoke","text":"<ul> <li>Aggregated self-reports feed longitudinal trends (pain scores, medication adherence, social needs). When a metric drifts, the orchestrator drafts follow-up language tailored to that patient\u2019s literacy and history.</li> <li>Humans confirm the plan, especially when it involves medication changes or complex counseling.</li> </ul>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#4-a-shared-workspace-with-observable-loops","title":"4. A shared workspace with observable loops","text":"<ul> <li>Every automated action should show provenance: which notes were summarized, how the LLM scored acuity, and who signed off.</li> <li>Dashboards for clinical leaders track alert volume, average time to human response, and which prompts drive escalations.</li> <li>Patients see status indicators (\u201creviewed by triage nurse at 3:12 PM\u201d), reinforcing trust that their data matters.</li> </ul>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#how-llms-help-when-theyre-tethered-to-humans","title":"How LLMs Help, When They\u2019re Tethered to Humans","text":"<p>LLMs earn their keep by shrinking the research burden on the care team:</p> <ul> <li>Context gathering: assemble medication history, recent labs, and prior outreach before a human opens the chart.</li> <li>Summarization loops: recursive summaries keep cross-channel interactions digestible without losing dosage details or timelines.</li> <li>Acuity classification: flag high-risk language inside texts and voicemails instantly, so humans act faster.</li> <li>Drafting outbound communication: suggest responses that a nurse can approve or tweak in seconds.</li> </ul> <p>But they always operate inside a governed loop. High-acuity cases trigger synchronous handoffs. Every draft carries a \u201csource disclosure\u201d showing the snippets the model used. Clinicians can thumbs-up or correct the draft, feeding evaluation back into the system.</p>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#translating-agentic-workflows-to-care-transformation","title":"Translating Agentic Workflows to Care Transformation","text":"<p>In value\u2011based contracts, the early post\u2011discharge window concentrates avoidable utilization. A conversation\u2011first engagement layer turns those patient signals into Transitional Care Management that actually happens: the agent confirms discharge in plain language, reconciles medications from chart context, books the 7\u201314\u2011day visit in the same thread, and routes any red\u2011flag phrases to a human within minutes. The same playbook helps pre\u2011op. If a joint\u2011replacement patient repeatedly skips readiness assessments or leaves a distressed voicemail, the system tags \u201cearly\u2011intervention,\u201d prompts a navigator call that solves transportation or medication confusion, and swaps in a tele\u2011visit when needed. These are the touches that move outcomes; among Medicare beneficiaries, receipt of TCM is associated with lower short\u2011term costs and mortality.</p> <p>PROMs and patient\u2011generated data work best when missingness is treated as a clinical signal. The agent makes completion feel human\u2014\u201cWould you rather text or talk? It takes two minutes\u201d\u2014then, after two missed pings, automatically buckets the patient for navigator outreach. From there, campaigns are concrete: close missed follow\u2011ups, establish primary care, schedule the Annual Wellness Visit, and line up preventive screenings; for ED diversion, concerning symptom language triggers same\u2011day coaching and an urgent clinic slot. Boards can read this on one page: completion rates, time\u2011to\u2011first human touch after a concerning signal, and gap\u2011closure by segment. Randomized trials show that systematic electronic symptom monitoring improves quality of life and reduces unplanned use when tied to real\u2011time follow\u2011up\u2014the exact loop this design operationalizes. (Steinbeck et al.)</p>"},{"location":"2025/09/16/building-a-patient-self-reporting-stack/#what-were-building-at-revelai-health","title":"What We\u2019re Building at RevelAI Health","text":"<p>At RevelAI Health, we are stitching these patterns together so health systems don\u2019t have to bolt on yet another point solution. Our orchestration layer listens to phone triage, portal inboxes, and AI-assisted callers, runs LLM-based acuity and intent models, and hands the care team a structured summary with suggested actions. Humans always close the loop, but they start from a prepared canvas instead of a blank screen.</p> <p>The payoff is simple: patients know someone is listening, care teams spend time on judgment calls instead of transcription, and leaders can finally measure whether self-reporting is reducing avoidable visits.</p> <p>Incumbent EHR vendors Epic, Cerner, the rest of the club certainly have the data gravity to tackle this, and they own the workflows inside the chart. But their strength on in-visit documentation hasn\u2019t translated into nimble inbound and outbound engagement. Telephony integrations remain bolt-ons, multi-channel intent routing is brittle, and most of the tooling still assumes a human will manually stitch the story together.</p> <p>We\u2019re still refining evaluation datasets, UI treatments, and human review workflows\u2014but the stack is finally showing that AI and thoughtful UX can make patient self-reporting actionable without losing the heart of human-centered care.</p> <p>If you\u2019re experimenting with your own self-reporting stack, I\u2019d love to compare notes. The work ahead is making these loops tight, auditable, and empathetic.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/","title":"Recursive Summarization Unlocks Effective LLM Integration in Healthcare","text":""},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#recursive-summarization-unlocks-effective-llm-integration-in-healthcare","title":"Recursive Summarization Unlocks Effective LLM Integration in Healthcare","text":"<p>Your patient has 247 pages of medical records spanning 8 years. Two ER visits, three specialists, ongoing knee osteoarthritis, recent ACL reconstruction. How do you create a coherent summary that preserves critical information while making it digestible for both clinicians and AI systems?</p> <p>The answer isn't just summarization, it's recursive summarization. And the secret isn't just what you summarize, but what you choose to preserve at each level of abstraction.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#what-is-recursive-summarization","title":"What Is Recursive Summarization?","text":"<p>Recursive summarization creates a hierarchical tree of information by repeatedly clustering and summarizing content at increasing levels of abstraction. Think of it as building a pyramid where each level captures essential information from the layer below, but with progressively broader context.</p> <p>This technique builds on established methods like hierarchical clustering for document organization and map-reduce approaches for long document processing.</p> <p>Here's how it works for healthcare:</p> <ol> <li>Level 0: Individual clinical notes, lab results, prescription records</li> <li>Level 1: Daily/visit summaries combining related notes from the same encounter</li> <li>Level 2: Episode summaries (e.g., \"ACL reconstruction recovery Q2 2024\" or \"chronic pain management\")  </li> <li>Level 3: Longitudinal condition summaries spanning multiple episodes</li> <li>Level 4: Complete patient timeline with major orthopedic events and functional outcomes</li> </ol> <p>This approach, inspired by systems like RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval), enables LLMs to access both granular details and high-level patient narratives depending on the clinical context needed.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#the-challenge-encounters-beyond-the-ehr","title":"The Challenge: Encounters Beyond the EHR","text":"<p>Healthcare happens everywhere, not just in structured EHR fields. Consider these real scenarios:</p> <p>Nurse triage call: \"Patient reports 8/10 knee pain, started after PT session yesterday. Swelling increased, can't bear weight. Taking prescribed oxycodone 5mg q6h. Post-op week 3 from ACL reconstruction.\"</p> <p>Patient text: \"Hey doc, my knee is really stiff in the mornings and the exercises are getting harder. Should I push through the pain? Also, when can I start jogging again?\"</p> <p>Post-discharge follow-up: \"Incision healing well, no signs of infection. Pain is down to 4/10 with ibuprofen. ROM improving but still can't fully extend. When can I return to work?\"</p> <p>Each interaction contains critical information that traditional EHRs often lose in free-text fields or don't capture at all. Recursive summarization can preserve both the immediate clinical details and the broader patterns these communications reveal.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#the-salient-features-problem","title":"The Salient Features Problem","text":"<p>Here's the catch: building upon past summaries alone isn't enough. What you choose to keep or abstract at each level determines everything.</p> <p>This is where subject matter expertise becomes critical. Clinical insights from orthopedic surgeons, physical therapists, and care coordinators are essential for defining what information matters at each abstraction level. Developers working in isolation will inevitably miss nuanced clinical relationships that experienced clinicians intuitively understand.</p> <p>Let's examine what matters:</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#keep-these-details","title":"Keep These Details:","text":"<ul> <li>Dosages and frequencies: \"Oxycodone 5mg q6h PRN\" not \"taking pain medication\"</li> <li>Timing relationships: \"Knee pain increased 24 hours after PT session\" </li> <li>Quantified symptoms: \"8/10 pain scale, ROM 0-90 degrees\" vs \"significant pain\"</li> <li>Functional outcomes: Weight-bearing status, return-to-activity timelines</li> <li>PT progress markers: Specific exercises, resistance levels, functional milestones</li> </ul>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#abstract-these-patterns","title":"Abstract These Patterns:","text":"<ul> <li>Recurring themes: Multiple PT compliance issues \u2192 \"Adherence challenges with rehabilitation protocol\"</li> <li>Stable recovery: \"Post-op healing progressing as expected\" vs daily wound checks</li> <li>Resolution of complications: \"Post-surgical swelling resolved\" vs daily measurements</li> </ul> <p>The distinction between what to preserve and what to abstract often requires clinical judgment that only comes from years of patient care experience. For example, knowing that \"terminal extension deficit\" in ACL recovery requires different monitoring than general ROM limitations is insight that developers simply cannot infer from data patterns alone.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#example-patient-encounter-hierarchy","title":"Example: Patient Encounter Hierarchy","text":"<p>Level 0 (Raw Data):</p> <ul> <li>3/15/2024 - Orthopedic follow-up note (847 words)</li> <li>3/15/2024 - MRI report (312 words)  </li> <li>3/15/2024 - PT evaluation (ROM measurements, strength testing)</li> <li>3/20/2024 - Patient portal message about PT difficulties</li> <li>3/22/2024 - Nurse callback about increased swelling</li> </ul> <p>Level 1 (Visit Summary):</p> <p>\"3/15/2024 Orthopedic follow-up: Patient reports improved weight-bearing tolerance, able to walk without crutches. ROM 0-110 degrees (target 0-130). MRI shows good graft healing, no complications. PT progressing but patient struggling with terminal extension exercises. Cleared for stationary bike. Follow-up in 6 weeks. Subsequent messages indicate increased swelling after PT - activity modification needed.\"</p> <p>Level 2 (Episode Summary):</p> <p>\"Q1 2024 ACL reconstruction recovery: Post-operative rehabilitation progressing appropriately. Achieved weight-bearing independence ahead of schedule. Graft healing well on imaging. PT compliance good but terminal extension remains challenging. Minor setback with activity-related swelling managed conservatively. Overall trajectory positive toward return-to-sport goals.\"</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#the-delta-based-approach-reasoning-about-change","title":"The Delta-Based Approach: Reasoning About Change","text":"<p>Traditional summaries lose temporal context. Delta-based recursive summarization tracks what changed between encounters, enabling sophisticated clinical reasoning. This approach leverages temporal reasoning techniques for medical data and longitudinal medical record analysis:</p> <pre><code>Previous Summary (12/2023):\n- \"Chronic knee pain, conservative management\"\n- \"ROM limited to 0-100 degrees, functional limitations\"\n\nCurrent Summary (3/2024):  \n- \"Post-ACL reconstruction, early recovery phase\" \n- \"ROM improved to 0-110 degrees, weight-bearing as tolerated\"\n\nDelta Analysis:\n+ IMPROVING: Range of motion (0-100\u00b0 \u2192 0-110\u00b0)\n+ IMPROVING: Functional status (non-weight bearing \u2192 WBAT)\n+ NEW INTERVENTION: Surgical reconstruction completed\n+ EMERGING CONCERN: PT compliance challenges with terminal extension\n+ CONTEXT: Return-to-sport timeline driving aggressive rehabilitation\n</code></pre> <p>This delta awareness enables LLMs to reason about recovery trajectories, rehabilitation responses, and emerging functional patterns that pure summarization misses.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#architecture-the-recursive-healthcare-summary-system","title":"Architecture: The Recursive Healthcare Summary System","text":"<p>The key innovation is the delta analysis layer that maintains change awareness throughout the hierarchy, enabling both detailed clinical decision-making and population-level insights.</p>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#implementation-reality-check","title":"Implementation Reality Check","text":"<p>The research shows promising results: LLMs often outperform human experts in clinical text summarization across completeness, correctness, and conciseness. Systems processing 82+ billion medical words from millions of encounters demonstrate this approach's feasibility at scale.</p> <p>But challenges remain:</p> <ul> <li>Information loss: Summarization inherently discards details, choosing wisely is critical</li> <li>Context windows: Even with larger models, 100+ page patient histories require intelligent chunking</li> <li>Clinical validation: Automated summaries need human oversight for high-stakes decisions</li> </ul>"},{"location":"2025/08/13/recursive-summary-is-all-you-need-healthcare-llm/#closing-thoughts","title":"Closing Thoughts","text":"<p>Healthcare generates massive amounts of unstructured data daily. Clinicians spend enormous time synthesizing information instead of caring for patients. Recursive summarization with delta-based reasoning offers a path forward, preserving essential clinical details while making complex patient histories navigable.</p> <p>The magic isn't in the AI doing the summarizing. It's in designing systems that know what clinical information to preserve, what to abstract, and how to maintain the temporal relationships that enable real medical reasoning.</p> <p>As healthcare moves toward AI-augmented workflows, recursive summarization isn't just a nice-to-have feature, it's the foundation that makes everything else possible.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/","title":"Can we reverse the curse of rigid journeys &amp; workflows with LLMs?","text":""},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#can-we-reverse-the-curse-of-rigid-journeys-workflows-with-llms","title":"Can we reverse the curse of rigid journeys &amp; workflows with LLMs?","text":"<p>Healthcare is littered with brittle decision-trees. Pre-op instructions, chronic-care check-ins, discharge follow-ups\u2014each new edge-case multiplies the branches. Most workflows we have are cron-based, running at specific times and are very hard to personalize around patient needs.</p> <p>Note</p> <p>Let's build on an idea. No more endless if/then/else trees\u2014just agents that watch, learn, and nudge exactly when it matters in healthcare.</p> <p>Below I sketch a different approach inspired by DeepMind's \"Era of Experience\" paper: treat patient journeys as living agents that observe real-time data, reason over it, and choose the next best action\u2014without needing humans to hand-craft every path.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#a-concrete-example","title":"A concrete example","text":"<p>Let's take an example of pre-appointment workflow for provider, (click to zoom the image):</p> <p></p> <p>Looks sane\u2014until you handle language preferences, missing lab-work, travel distance... you get the point. You'll be back to spaghetti code, and handling every edge case becomes difficult, not to mention dealing with diverse data makes it worse.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#enter-the-era-of-experience","title":"Enter the Era of Experience","text":"<p>Silver &amp; Sutton argue we're moving from imitating past data to learning from live experience:</p> <ul> <li>Streams of Experience: Agents maintain persistent internal state across months or years (e.g., a wellness coach monitoring wearables)</li> <li>Autonomous Actions &amp; Observations: Instead of reactive agents where you chat and get a reply, agents can execute actions like sending reminders to patients or flagging issues for doctors</li> <li>Grounded Rewards: Measuring environment-driven signals like biomarker data, heart rate, PHQ9 scores, or FHIR observation resources</li> <li>Beyond Human Level Reasoning: Language is universal but inefficient; experiential agents can invent non-linguistic internal codes and couple them to learned world-models for planning</li> </ul> <p>I want these four pillars baked into patient journeys.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#an-observer-based-paradigm-for-patient-experience-personalization","title":"An observer-based paradigm for patient experience personalization","text":"<ul> <li>Agents maintain streams of experience by observing EHR data, wearables, and internal application data</li> <li>Using an observer pattern where agents process incoming data</li> <li>Based on observations, agents use a broad set of policies and rules to carry out actions</li> <li>We can feed in FHIR APIs, event-based data, monitor data for an agent to observe</li> <li>As context windows improve, agents can build upon previous learning while incorporating new information</li> </ul> <p>Agents carry out actions while being aware of their past actions, which also become observable data.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#optimizing-for-the-rewards","title":"Optimizing for the rewards","text":"<p>These policies and journeys are built by subject matter experts like clinicians and researchers based on consensus about how to observe patient state and implement automation.</p>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#sketching-the-code-examples","title":"Sketching the code examples","text":"<pre><code>w = Workflow(\n    policy=\"\"\"\nYou're a virtual MA. Optimize for surgery readiness while minimizing alert fatigue.\n\n{add_rules}\n\n{policies_from_subject_matter_experts}\n\"\"\",\n    actions=[\n        \"action.send_sms\",\n        \"action.order_covid_test\",\n        \"action.notify_care_team\",\n        \"action.notify_primary_care_physician\",\n        \"action.&lt;concrete_action_or_tool_name&gt;\"\n    ],\n    recent_past_actions=\"\"\"\n        {recent_past_actions_taken}\n    \"\"\",\n    store=PostgresStore(),\n)\n\nw.add_event({\"type\": \"observation\", \"kg\": 102, \"ts\": \"2025-05-04T09:00Z\"})\n\nw.add_event({\"type\": \"appointment_booked\", ... })\n\n## more events for realtime API\n\nfor action in w.decide():\n    execute(action)\n</code></pre> <p>LLMs can summarize past actions, and as context windows grow, they can take data in any format or store it long-term.</p> <p>We can also use another LLM as a policy judge:</p> <pre><code>for action in w.decide():\n    llm_as_policy_judge_decision = llm_as_policy_judge.evaluate(action)\n    if llm_as_policy_judge_decision == \"yes\":\n        execute(action)\n</code></pre> <p>Or to mitigate risks, LLMs could predict the severity of actions and bring in human oversight when needed:</p> <pre><code>for action in w.decide():\n    llm_as_policy_judge_predicted_acuity = llm_as_policy_judge.acuity(action)\n    if llm_as_policy_judge_predicted_acuity in [\"high\", \"medium\"]:\n        care_team_decision = flag_care_team(action)\n        if care_team_decision == \"yes\":\n            execute(action)\n</code></pre>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#upsides-of-this-approach","title":"Upsides of this approach","text":"<ul> <li>Scales to thousands of edge-cases without exploding DAGs</li> <li>Handles messy data LLMs can understand HL7, FHIR, wearable APIs and unstructured notes etc..</li> <li>Enables culturally-relevant comms (tone, language, reading level) on the fly</li> </ul>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#downsides","title":"Downsides","text":"<ul> <li>Deterministic, auditable flows become stochastic\u2014regulators will ask \"why?\"</li> <li>Reward hacking is real: bad metrics \u2192 spam or worse</li> <li>Streaming RL + HIPAA-grade logging is early days</li> </ul>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#mitigating-the-risks","title":"Mitigating the risks","text":"<ul> <li>Policies and rewards must be well-defined</li> <li>Reasoning models need to predict severity accurately. For high-acuity actions like medication recommendations, care-team members must be involved. o3 model is already showing positive signs.</li> <li>Subject matter experts need to craft clear policies on what AI handles vs. what requires human intervention</li> <li>Extensive simulations and test scenarios are needed to validate the approach</li> </ul>"},{"location":"2025/05/04/reversing-the-curse-of-conditional-workflows-with-llms/#my-hope-and-conclusion","title":"My hope and conclusion","text":"<p>Rigid pathways got us this far, but they break under real-world complexity. By letting agents experience patients' lived data\u2014and grounding them in clinical rewards\u2014we can deliver care that adapts like a good nurse: quietly watching, acting only when helpful.</p> <p>Making this vision reality requires three things: well-crafted AI policies that encode clinical wisdom, a willingness to experiment in safe but meaningful ways, and subject matter experts collaborating across disciplines. The technology exists\u2014what's needed now is the courage to reimagine workflows from first principles.</p> <p>At RevelAI Health, we're bringing this approach to life in the MSK space. Our platform orchestrates pre-op and post-op journeys, unifies fragmented communications, and applies generative AI to handle complex operative workflows. We're seeing early validation that intelligent agents can dramatically reduce the friction patients experience while giving time back to care teams.</p> <p>Interested in joining us on this journey? We're hiring our founding engineer to help shape how AI transforms healthcare workflows from rigid decision trees to living, adaptive experiences.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/","title":"How Claude Code Made Me Fall in Love with the Terminal","text":""},{"location":"2025/08/04/terminal-is-all-we-need/#how-claude-code-made-me-fall-in-love-with-the-terminal","title":"How Claude Code Made Me Fall in Love with the Terminal","text":"<p>Like many of you, I recently made the full switch from Cursor to Claude Code. This transition marked more than just a tool change \u2013 it fundamentally transformed how I think about development environments.</p> <p>For years, I lived in VSCode (recently Cursor), relying heavily on mouse navigation and minimal keyboard shortcuts. I resisted the pull of Neovim and keyboard-centric workflows. But after embracing Claude Code, I discovered something profound: the terminal is the new IDE. You can run it everywhere with a consistent workflow \u2013 be it a Linux box, your Mac, or a VPS. That's all you need.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#the-terminal-renaissance","title":"The Terminal Renaissance","text":"<p>Post-Claude Code adoption, I discovered something profound. The terminal isn't just a command line anymore \u2013 it's become a powerful orchestration layer for AI-driven development. The ability to customize, control, and context-switch using nothing but keyboard commands feels like rediscovering the original Unix philosophy, but supercharged for the AI era.</p> <p>As a recent article on Coding for the Future Agentic World put it:</p> <p>\"The terminal is the new IDE - CLI agents turn your shell into an action\u2011oriented interface where prompts translate into multi\u2011file commits and tests\"</p> <p>This perfectly captures what I've been experiencing. The power of being in the terminal, switching contexts, and conducting multiple AI agents while navigating entirely via keyboard has transformed my productivity in ways I didn't expect.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#why-terminal-workflows-matter-now","title":"Why Terminal Workflows Matter Now","text":"<p>The shift to terminal-based development isn't just about nostalgia or hacker aesthetics. It's about finding the right interface for AI-augmented coding. Here's what I've discovered:</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#speed-vs-control","title":"Speed vs. Control","text":"<p>I was using Cursor's AI agents, and while the review UX was impressive, it was too fast. Yes, you read that right \u2013 too fast. </p> <p>When you're creating something from scratch, speed is a blessing. But when modifying large, established codebases \u2013 which is most of my work using Claude Code's @ commands \u2013 the equation changes. I found myself struggling to properly review the changes. Why? Because meaningful code review requires understanding not just what changed, but why and how it fits into the larger system.</p> <p>My workflow evolved into something more deliberate: Claude makes edits, I review them in the terminal, jump to another Zellij pane, and hit <code>&lt;line#&gt;G</code> in Neovim to inspect exactly what's being modified. This might seem slower, but it's actually faster in the long run. With AI, I want to do better code reviews and focus my attention where it matters most. This terminal-based approach helps me achieve exactly that.</p> <p>With Cursor agents, you can spawn multiple agents, but the context switching felt disconnected. It never matched the fluid experience of working in a tmux or Zellij window where you can see your AI \"cooking\" the code in real-time while maintaining full control over the review process.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#the-context-problem","title":"The Context Problem","text":"<p><code>.cursorrules</code> never quite worked for me. In contrast, Claude Code's slash commands \u2013 essentially fancy prompts \u2013 have made me incredibly productive. The direct, explicit nature of terminal commands aligns perfectly with how I think about giving instructions to AI.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#productive-impatience","title":"Productive Impatience","text":"<p>AI processing creates natural pauses in your workflow. Rather than sitting idle while Claude Code crunches through a complex task, I've learned to use these moments productively. I'll switch to another pane and start: - Writing test cases I want to validate - Drafting the next specification - Setting up another AI agent for a parallel task - Reviewing the project's architecture to anticipate edge cases</p> <p>The terminal makes this context switching seamless. Before AI completes one task, I'm already thinking three steps ahead, preparing context for the next interaction. It's like conducting an orchestra \u2013 while one section plays, you're already cueing the next.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#my-terminal-first-toolkit","title":"My Terminal-First Toolkit","text":"<p>Here's the stack that has revolutionized my development experience:</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#neovim-with-lazyvim","title":"Neovim with LazyVim","text":"<p>The transition from VSCode to Neovim was brutal for 3-4 days. But after pushing through the initial pain, I'm now far more effective. Navigating entirely via keyboard, reviewing code, and writing has become genuinely enjoyable.</p> <p>I use LazyVim and keep my configuration on GitHub for consistent experiences across servers and machines. The investment in learning vim motions has paid off exponentially.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#zellij-for-session-management","title":"Zellij for Session Management","text":"<p>Zellij is modern tmux \u2013 and it's delightful to work with. I've aliased it to <code>zz</code> for quick access:</p> <pre><code>alias zz=\"zellij\"\n</code></pre> <p>Starting a new session is as simple as <code>zz a backend-work</code>. The keybindings are intuitive: - <code>Ctrl + p</code> to create new panes - <code>Ctrl + t</code> for tabs - Arrow keys for navigation - Built-in resizing and moving</p> <p>When AI is processing in one pane, I can seamlessly switch to another. Each pane becomes like having a dedicated AI co-programmer working on a specific task. My role shifts to reviewing code, anticipating edge cases, and improving specifications.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#github-copilot-for-autocompletion","title":"GitHub Copilot for Autocompletion","text":"<p>While Cursor's completion model is excellent, GitHub Copilot in Neovim holds its own. It's not quite as sophisticated, but it's more than adequate for day-to-day coding.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#avantenvim-for-inline-editing","title":"Avante.nvim for Inline Editing","text":"<p>I missed Cursor's <code>Cmd + K</code> experience for quick edits. Avante.nvim fills this gap. It's not perfect, but with proper keybindings, I rarely need it \u2013 Claude Code handles the heavy lifting.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#ghostty-terminal","title":"Ghostty Terminal","text":"<p>This beautiful terminal won me over with its simplicity and speed. It's the perfect foundation for this entire workflow.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#claude-code-router","title":"Claude Code Router","text":"<p>Recently started using this to experiment with different models like Kimi for faster responses. If you haven't tried model routing, I highly recommend it \u2013 being able to switch between models based on task complexity is game-changing.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#additional-power-tools","title":"Additional Power Tools","text":"<ul> <li>lazygit - Git management without leaving the terminal</li> <li>lazydocker - Docker container management made simple</li> <li>Rainfrog - SQL database management in the terminal</li> <li>Docker Compose - Let Claude Code work with compose files to spin up dependencies and integrate multiple codebases</li> </ul>"},{"location":"2025/08/04/terminal-is-all-we-need/#the-slash-command-revolution","title":"The Slash Command Revolution","text":"<p>One of Claude Code's killer features is its slash commands. I've built up a library of custom commands that act as sophisticated prompts. These aren't just shortcuts \u2013 they're carefully crafted instructions that ensure consistent, high-quality AI assistance.</p> <p>For example, I have commands for: - <code>/architect</code> - Helps me think through system design before implementation - <code>/review</code> - Provides thorough code review with security and performance considerations - <code>/test</code> - Generates comprehensive test cases based on my codebase patterns</p> <p>With hooks, I can trigger linting and type checking automatically. The terminal becomes a highly customized environment where AI and traditional tools work in perfect harmony. Every command execution can trigger a cascade of validations, ensuring code quality remains high even as development speed increases.</p>"},{"location":"2025/08/04/terminal-is-all-we-need/#final-thoughts-was-it-worth-it","title":"Final Thoughts: Was It Worth It?","text":"<p>Absolutely.</p> <p>The initial learning curve for Neovim was steep, but living entirely in the terminal has reignited my joy in programming. Zellij's session management enables laser-focused work \u2013 each session becomes a dedicated workspace for deep concentration.</p> <p>The ability to conduct multiple AI agents simultaneously, review their work in real-time, and maintain complete keyboard control has made me significantly more productive. More importantly, it's made programming fun again.</p> <p>As AI continues to evolve, I believe terminal-based workflows will become increasingly important. The terminal provides the perfect balance of power, flexibility, and control for orchestrating AI agents. It's not about going backwards \u2013 it's about finding the right interface for the future of development.</p> <p>Whether you're writing code, searching files, or editing documentation, tools like Claude Code are transforming the humble terminal into a powerful AI-augmented development environment.</p> <p>The terminal isn't just alive \u2013 it's thriving. And for developers willing to invest in mastering it, the rewards are substantial</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/","title":"Using Nudges to Reinforce Healthy Behaviors","text":""},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#using-nudges-to-reinforce-healthy-behaviors","title":"Using Nudges to Reinforce Healthy Behaviors","text":"<p>Guest Post on Twilio Blog on how we Vincere used Twilio to Reinforce Healthy Habits That Improve the Lives of Underserved Populations</p> <p>Vincere Health believes clinicians play an integral role in promoting sustainable behavior change and that technology facilitates and personalizes this relationship at scale. Vincere Health is one of few health technology platforms built for people in diverse socio-economic categories.</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#the-journey","title":"The Journey","text":"<p>Smoking is the leading cause of preventable disease, disability, and death in the United States. The recent pandemic made matters worse where cigarette sales increased in 2020 for the first time in over 20 years. There are over 34 million adult smokers in the U.S., many of whom are in the lower income population. Reaching this population and providing effective support at scale while keeping costs down can best be achieved with the aid of technology and a carefully designed patient experience.</p> <p>Vincere Health built a platform to offer low-cost access to addiction healthcare with an option for using reward-based habit training to increase care compliance. While providing addiction healthcare, this platform also solves problems related to patient engagement, patient outreach at scale, and clinician productivity by automating administrative tasks and decreasing their cognitive burden.</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#building-a-platform-for-a-personalized-care-journey","title":"Building a Platform for a Personalized Care Journey","text":"<p>Smoking cessation is best represented as a journey, not a single event. Personalized care experiences tailored to each participant's needs have shown increased rates of success in conquering addiction while increasing care compliance and quit rates.</p> <p>Vincere Health realized early on in their journey that creating a platform that allows care team members to build care pathways themselves, instead of hard-coding a program and spending subsequent product and engineering time to tweak it, is the key to creating truly differentiated and personalized experiences. Most of the existing workflow automation no-code tools are not particularly user-friendly or built to solve similar use cases. By building a platform that enables drag and drop care pathways, Vincere Health can empower clinical teams to truly have autonomy, agility to experiment, and flexibility to personalize programs for diverse demographics.</p> <p>Vincere Health built a nudge engine that allows care teams to create personalized journeys in the following ways:</p> <p>\u2013 Create a care journey for multiple days or months that can track a variety of events on a calendar using their Campaign Studio UI. A campaign includes customized messaging, remote patient monitoring (RPM), and assessments / surveys.</p> <p>\u2013 Define personalized nudges and tasks. Vincere Health uses Twilio's Programmable Messaging API to power automated SMS messages at specific intervals serving relevant educational content, motivational nudges, behavior enforcement, and medication or appointment reminders for care coordination. When patients respond, Vincere Health integrates the Conversations API with their chatbot so the patient, clinician, and chabot can all take part in a two-way conversation on a single thread while remaining HIPAA-compliant.</p> <p>\u2013 Have the option to define financial reward criteria for achieving certain goals. For example, participants can earn rewards for completing a self-reported survey, a set of tasks, breath tests, or any RPM device test compliance.</p> <p>\u2013 Load preset care plan templates as new participants onboard to provide intervention or design and templetize their own automated care plan.</p> <p>\u2013 Deploy programs with app-less (SMS) experience or using a mobile app via API within minutes.</p> <p>During the program, Vincere Health collected and aggregated data from participants to measure engagement based on habit compliance. This data allowed care teams to segment participants and take proactive actions.</p> <p>We want clinicians to be in charge of care pathways and let them create the programs themselves. We want to allow coaches to be autonomous and independent. They can experiment on their own with the help of our platform using Twilio's APIs.</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#using-twilio-as-the-communication-cloud-provider","title":"Using Twilio as the Communication Cloud Provider","text":"<p>Vincere Health's goal was to create a frictionless experience and reduce access barriers to the programs their care teams have built on their platform. Using omni-channel communication where participants can communicate with their care team and access the program either through SMS (app-less experience) or in-app messages has been an integral part of Vincere Health's success in increasing patient engagement.</p> <p>Twilio was chosen to power this communication because it provided the necessary tools to build HIPAA-compliant, omni-channel communications channels. Twilio's APIs and SDKs enabled Vincere Health to quickly build a scalable and reliable platform.</p> <p>Twilio Programmable Messaging API was used to send all the SMS nudges and reminders at scale. The long-term maintainability and plug-and-play nature of the Conversations API helped Vincere Health involve their care team members in real-time to build personalized relationships through two-way conversations.</p> <p>\"A few of the things that stand out the most are developer experience, HIPAA compliance documentation, and long term maintainability. You can own the experience completely while Twilio is the underlying infrastructure for communication.\"</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#going-forward","title":"Going Forward","text":"<p>With a custom communication platform powered by Twilio and amazing health coaches, Vincere Health has been able to launch successful clinical validations with leading institutions such as Boston Medical Center and their research has been published by the American Thoracic Society Journal.</p> <p>Vincere Health's goal started with creating tools to support holistic care journeys that enhance care team effectiveness. Automation of day-to-day tasks for patient outreach and timely care team involvement has resulted in meaningful and long-term change for people who need it the most. Vincere Health's next steps are to continue to refine and scale their platform by adding intelligent layers using machine learning and advanced data gathering techniques. Their goal is to engage participants proactively with compassion and relevant care that meets their changing needs.</p>"},{"location":"2022/10/04/using-nudges-to-reinforce-health-behaviors/#the-results","title":"The Results","text":"<p>Vincere Health's strategic partners and individual participants are seeing higher program satisfaction rates of 82% Net Promoter Score, higher program engagement rates of 7.4 average weekly touchpoints, and 68% reduction in tobacco usage across populations when carbon monoxide (CO) was measured objectively using devices during clinical trials. Over 150,000 SMS-based nudges were sent resulting in 20% of health coaches' time saved through automated reach and reminders.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/","title":"AI Sucks at Analyzing Data","text":""},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#ai-sucks-at-analyzing-data","title":"AI Sucks at Analyzing Data","text":"<p>User asks an AI data analysis tool: \"Pull all patient communication encounters from last month.\"</p> <p>The AI confidently writes a SQL query, hits the <code>encounters</code> table directly, and returns 1,247 records. AI confidently answers that. Three days later, you discover the actual number should have been 3,891, because the real path is <code>patients \u2192 patient_queue \u2192 queue_encounters \u2192 encounters</code>. The AI missed two-thirds of your data.</p> <p>This isn't a hypothetical. I've seen this exact scenario play out at RevelAI Health, where we work with health communication data. And it's not unique to us, across healthcare, finance, and any domain with messy legacy systems, AI data tools are confidently wrong in ways that are hard to detect.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#the-real-problem-isnt-the-model","title":"The Real Problem Isn't the Model","text":"<p>Data, business rules, and relationships are messy. Table names are often misleading. Developers structure databases in ways that made sense five years ago but don't reflect current business logic. Relationships between tables encode tribal knowledge that exists only in the heads of a few engineers.</p> <p>And we can't expect LLMs to magically infer these relationships from schema alone. They won't. They can't.</p> <p>There are dozens of companies building tools that promise: \"Connect your database, ask questions in plain language, get insights.\" Tools like Julius, Hex, ChatGPT's Advanced Data Analysis, and Claude's analysis features all offer variations on this theme.</p> <p>In my experience, they work fine for:</p> <ul> <li>Simple CSV uploads with self-explanatory column names</li> <li>Basic SQL databases with well-defined foreign keys</li> <li>Schemas designed by data teams with semantic modeling in mind</li> </ul> <p>But the moment data relationships become messy, which is what real-world data actually looks like, these tools don't just fail. They fail confidently. They return plausible-looking results that are subtly (or catastrophically) wrong.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#what-would-actually-work","title":"What Would Actually Work","text":"<p>I see this problem being approached in a fundamentally different way. Instead of rushing to the data-analysis part, we should focus on building domain knowledge first.</p> <p>Here's what I envision:</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#1-interview-style-schema-discovery","title":"1. Interview-Style Schema Discovery","text":"<p>When you connect a database, the tool doesn't immediately start answering questions. Instead, it:</p> <ol> <li>Analyzes the schema and presents its understanding of how tables relate</li> <li>Asks clarifying questions: \"I see <code>encounters</code> and <code>patient_queue</code>. Which table should I use to find all patient communications?\"</li> <li>Learns from the expert's corrections and stores that as long-term memory</li> </ol> <p>Think of it like onboarding a junior data analyst. You wouldn't expect them to write perfect queries on day one. You'd walk them through the schema, explain the business logic, and correct their mistakes. Over time, they'd build an internal model of how your data works.</p> <p>Why don't we do this with AI tools?</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#2-long-term-memory-bank","title":"2. Long-Term Memory Bank","text":"<p>Every correction, every clarification, every \"actually, use this path instead\" should be stored as structured knowledge\u2014not just dumped into a vector database, but organized as business rules:</p> <pre><code>{\n  \"rule_id\": \"patient_encounters_path\",\n  \"domain\": \"behavioral_health_comms\",\n  \"learned_from\": \"john@example.com\",\n  \"learned_at\": \"2025-10-15\",\n  \"context\": \"When querying patient communication encounters\",\n  \"incorrect_approach\": \"SELECT * FROM encounters WHERE...\",\n  \"correct_approach\": \"JOIN patients \u2192 patient_queue \u2192 queue_encounters \u2192 encounters\",\n  \"confidence\": 0.95,\n  \"validated_by\": [\"jane@example.com\", \"mike@example.com\"]\n}\n</code></pre> <p>This isn't just context for the next query. It's a knowledge base that persists across sessions, gets validated by multiple team members, and improves over time.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#3-reinforcement-learning-from-analyst-behavior","title":"3. Reinforcement Learning from Analyst Behavior","text":"<p>Imagine a tool that watches how senior data analysts write queries, sees when they correct the AI's suggestions, and learns from those signals. Not in a creepy surveillance way\u2014but as an opt-in \"training mode\" where the tool explicitly asks: \"I see you changed my query. Can you help me understand why?\"</p> <p>Recent research in LLM reflection processes (Meta-Policy Reflexion) shows that language models can learn from past experiences through structured reflection, although it's easier said that done in real-world setting.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#4-transparent-heres-what-ive-learned-ui","title":"4. Transparent \"Here's What I've Learned\" UI","text":"<p>Users should be able to see:</p> <ul> <li>What relationships the tool has learned</li> <li>Which team members validated each rule</li> <li>Where the tool is still uncertain</li> <li>What questions it has about the schema</li> </ul> <p>This builds trust. When the tool says \"Based on corrections from Sarah and Mike, I now route patient encounter queries through the queue table,\" you know it's not hallucinating\u2014it's using actual domain knowledge.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#heres-the-architecture-diagram","title":"Here's the Architecture Diagram","text":"<p>Current approach vs. what I'm proposing (following is generated by Claude):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 CURRENT APPROACH (Stateless)                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  User Question \u2192 Schema Inference \u2192 SQL Generation \u2192       \u2502\n\u2502  Execute \u2192 Return Results                                   \u2502\n\u2502                                                             \u2502\n\u2502  (Each query starts from scratch, no learning)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROPOSED APPROACH (Stateful Learning)                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502   Initial    \u2502         \u2502  Domain         \u2502              \u2502\n\u2502  \u2502   Setup      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502  Knowledge      \u2502              \u2502\n\u2502  \u2502   Interview  \u2502         \u2502  Bank           \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                    \u2502                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502                        \u2502\n\u2502  \u2502 User         \u2502                  \u2502                        \u2502\n\u2502  \u2502 Question     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502                        \u2502\n\u2502                                    \u2193                        \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502                          \u2502  Query          \u2502                \u2502\n\u2502                          \u2502  Generation     \u2502                \u2502\n\u2502                          \u2502  (Context-Aware)\u2502                \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                   \u2502                         \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502                          \u2502   Execution     \u2502                \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                   \u2502                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502  \u2502  Analyst     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502  Correction &amp;   \u2502               \u2502\n\u2502  \u2502  Correction  \u2502        \u2502  Learning Loop  \u2502\u2500\u2500\u2500\u2510           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502           \u2502\n\u2502                                   \u2502             \u2502           \u2502\n\u2502                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502                          (Updates domain knowledge)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#why-this-is-hard","title":"Why This Is Hard","text":"<p>Let's face it: this is easier said than done.</p> <p>As Andrej Karpathy observed on the Dwarkesh podcast, we're trying to make bigger leaps without solving the underlying problem\u2014and I'm seeing the same pattern with AI data analysis tools. There's a lot of optimism around them, but some fundamental challenges remain unaddressed.</p> <p>The memory architecture isn't there yet. Deciding what becomes a long-term memory, when to retrieve it, and how to balance learned rules with schema changes\u2014these are tough challenges we haven't solved elegantly.</p> <p>Key open questions:</p> <ol> <li>Memory decay: How do you handle schema migrations that invalidate learned rules?</li> <li>Conflicting corrections: What happens when two analysts give contradictory guidance?</li> <li>Confidence calibration: How does the tool know when to trust its learned knowledge vs. ask for clarification?</li> </ol> <p>There might be a stopgap UX solution: a dashboard showing \"Here's what the tool has learned about your business rules from different stakeholders,\" with validation workflows before rules become permanent.</p> <p>But we're not there yet.</p>"},{"location":"2025/11/01/why-ai-sucks-at-data-analysis/#conclusion","title":"Conclusion","text":"<p>I should note\u2014I haven't done exhaustive research on every tool in the market. There are a few that come close:</p> <p>Existing Tools: Vanna AI (RAG-based text-to-SQL with manual training), AskYourDatabase (custom question/SQL pair training), and Tableau Pulse &amp; Power BI Copilot (enterprise BI with semantic layers) all represent progress, but require either manual training or pre-built data models\u2014none combine interview-style discovery, long-term memory, and transparent learning from corrections.</p> <p>I'm keeping an eye on:</p> <ul> <li>Memory architecture research and more implementations (like the Meta-Policy Reflexion paper)</li> <li>Tools that add interview-style schema discovery</li> <li>Reinforcement learning approaches in data tools</li> <li>Transparent \"here's what I learned\" UX patterns</li> </ul> <p>This is one of those problems where I have strong intuitions but limited visibility into what's being built behind closed doors. If I've missed tools that are actually doing this well, please let me know in the comments.</p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/","title":"Writing reactive templates with HandlebarsJs.","text":""},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#writing-reactive-templates-with-handlebarsjs","title":"Writing reactive templates with HandlebarsJs.","text":"<p>The JavaScript community is going through the phase of evolution. There are so many frameworks, techniques and ideas that have been going on. It is really exciting but keeping up with it all is really a hard thing to do.</p> <p>I started using Meteor on my first job. I was in love with Meteor because of two reasons.</p> <ol> <li>A strong boilerplate. (You don\u2019t have to worry about configurations).</li> <li>It provides you reactivity out of the box. (Making Single page applications is fun with it).</li> </ol> <p>Blaze in Meteor have a unique approach towards DOM. It keeps track of reactive dependencies by compiling html into something called htmljs which will only touch those parts of DOM that really needs to update instead of rendering the whole template. Read this article on MeteorHacks.</p> <p>Do I want to use Meteor or React every time ? What if I don\u2019t have single page application like the one I am working on these days where we need to make small reusable components with rich client side interactions. I don\u2019t want to use whole framework like Angular or a view layer like React for that. Because part of me miss the out of box reactivity in Meteor that maximizes the separation of concerns and code organisation. I decided to write a miniature library with Handlebars templates and to come up with a good way to integrate in our web application. The goal is to keep the data-flow plainly visible, making it easy to read and understand the code.</p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#wait-why-do-i-need-reactive-programming","title":"Wait! Why do I need Reactive Programming ?","text":"<p>The best way to understand reactive programming is to compare it with event-driven programming. You use event-driven programming every time when you write a JavaScript app. Consider this JQuery event.</p> <p></p><pre><code>$('div[name=\"some\"]').on('click', () =&gt; {\n    console.log('div have been clicked !');\n});\n</code></pre> This div will react every time someone clicks over it with a console log.<p></p> <p>How about if I can have reactive data object that I am passing to the template. When your direct multiple (or single) variable value changes it should trigger a callback to observe it and update it\u2019s value in the DOM. That can be done through reactive programming primitives. With the approach of reactive programming I can have these benefits.</p> <ol> <li>Updating variables will update their values where used in DOM.</li> <li>Maximizing separation of concern and providing clean and declarative way of organizing the code. (Will have no more spaghetti code with the all the changes nested under JQuery\u2019s listeners callbacks).</li> <li>Observing the data passed to the template through observers. (If the listeners are set on object keys that are passed to the template).</li> <li>Abstraction over asynchronous HTTP calls by setting promises to the templates.</li> </ol> <p>If I set key of the object that I am passing as a data to the template it should update the DOM in real time and run observer callbacks if the observers are set to this data object\u2019s key.</p> <pre><code>// updates the DOM as well\nReactiveHbs.set('counter', counter.get() + 1);\n// Observer\nReactiveHbs.reactOnChange('counter', { debounce: 1000 }, (tpl) =&gt; {\n    console.log( 'counter have been changed ' );\n    console.log( 'value', tpl.get('counter') );        \n});\n</code></pre> <p>So basically the set method generates all these changes. Since the Lodash is heavily concentrated on performance I am using Lodash for get and set methods.</p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#optimal-ways-to-update-dom","title":"Optimal ways to update DOM.","text":"<p>After going through a lot of articles I found that these can be the optimal ways to update the DOM on change.</p> <ol> <li>Keeping the DOM depth small.</li> <li>Working with elements without appending to the DOM, and then just append everything together once everything is set up. (i.e using doc-fragments or strings).</li> </ol> <p>With handlebars changes will always be off-DOM. Compile it on change and then append it to DOM. So upon change there will be only single DOM manipulation.</p> <p>Since I am not using any virtual-DOM to diff the changes compiling template again and appending to DOM will be an expensive method on every change. But like I said this approach can be good with small templates or when your whole application is not a single page application and you don\u2019t want to use any large front-end framework. On every compilation I use JQuery empty() method to avoid memory leaks which removes other constructs such as data and event handlers from the child elements before removing the elements themselves.</p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#helpers","title":"Helpers","text":"<p>Instead of using global handlebars helpers bind helpers to only one template with the helper function.</p> <pre><code>// bind helpers to only one template\nReactiveHbs.helpers({\n    add(x, y) {\n        return x + y;\n    }\n});\n</code></pre>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#events","title":"Events","text":"<p>I really like the way you can define events in BackboneJs or Meteor. I tried to use the same approach. These events will be delegated events with respect to the container.</p> <pre><code>// bind delegated events\nReactiveHbs.events({\n    'click button[type=\"submit\"]': (e, elm, tpl) =&gt; {\n        e.preventDefault();\n        tpl.set('someData', $(elem).attr('data-text')); \n    }\n});\n</code></pre> <p>The non-delegated events can be defined under onRendered callback. Which will be triggered once the template is compiled and is in the DOM.</p> <pre><code>// non-delegated events\nReactiveHbs.onRendered(function() {\n   let self = this;\n   $('button[type=\"submit\"]').on('click', (e) =&gt; {\n       e.preventDefault();\n       self.set('someData', $(elem).attr('data-text'));\n   });\n});\n</code></pre>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#abstraction-over-async-http-calls","title":"Abstraction over async HTTP calls","text":"<p>I really like how you can separate your business logic in AngularJs by making factory methods that you can inject into your controllers. For the HTTP async calls you can define promises in an object and can use them in the template.</p> <pre><code>ReactiveHbs.promises({\n    getAllUsers() {\n        return $.get('https://api.github.com/users');\n    },\n});\n// usage\nReactiveHbs.executePromise('getAllUsers', (err, data) =&gt; {\n    if ( !err ) console.log(data);\n});\n</code></pre>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#demos","title":"Demos","text":"<p>So we have helpers and the events for the template. Lets play with it with a common counter example.</p> <pre><code>let counter = new ReactiveHbs({\n    container: '.mount',               // html template mount point\n    template: '#tpl',                  // hadnlebars template\n    data: {\n      count: 0\n    }\n\n});\n// Events\ncounter.events({\n    'click [name=\"increment-count\"]': (e, elm, tpl) =&gt; {\n        tpl.set( 'count', tpl.get('count') + 1 );\n    }\n});\ncounter.render();\n</code></pre> <p>Codepen to play with</p> <p>Instead of making a regular main-stream todo example to show reactivity I tried to come up with Trello like task cards example.</p> <p>Trello like Codepen to play with</p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#performance","title":"Performance","text":"<p>I created a simple performance test with 10,000 item list and a reverse button. Hitting the reverse button renders the template again with a reverse list. On my machine the render time on average is 360ms which is pretty nice as compare to AngularJs which takes on average 600ms to render. See this codepen for AngularJS.</p> <p>Definitely Angular\u2019s dirty checking algorithm performance on large templates will be better when you cannot afford to render templates again on every change.</p> <p>Reactive Handlebars Performance.</p> <p>While clicking twice the reverse list button this is the Google Chrome\u2019s timeline performance overview.</p> <p></p>"},{"location":"2016/10/18/writing-reactive-templates-with-handlebarsjs/#next-steps","title":"Next Steps","text":"<p>See the GitHub</p>"},{"location":"archive/2026/","title":"2026","text":""},{"location":"archive/2026/#2026","title":"2026","text":""},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2025/#2025","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2024/#2024","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2023/#2023","title":"2023","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2022/#2022","title":"2022","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2021/#2021","title":"2021","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"archive/2017/#2017","title":"2017","text":""},{"location":"archive/2016/","title":"2016","text":""},{"location":"archive/2016/#2016","title":"2016","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"page/2/#home","title":"Home","text":""},{"location":"page/3/","title":"Home","text":""},{"location":"page/3/#home","title":"Home","text":""}]}