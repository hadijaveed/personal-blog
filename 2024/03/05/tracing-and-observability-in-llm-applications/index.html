<!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--><!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes about startups, AI, health-care and overall engineering"><meta name=author content="Hadi Javeed"><link href=https://hadijaveed.me/2024/03/05/tracing-and-observability-in-llm-applications/ rel=canonical><link href=../../../../2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/ rel=prev><link href=../../../08/11/how-llms-revolutionized-my-productivity/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../feed_rss_updated.xml><link rel=icon href=../../../../assets/logo.jpeg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>From Concept to Production with Observability in LLM Applications - Hadi Javeed's blog</title><link rel=stylesheet href=../../../../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XS27CVTCE3"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XS27CVTCE3",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XS27CVTCE3",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="From Concept to Production with Observability in LLM Applications - Hadi Javeed's blog"><meta property=og:description content="Notes about startups, AI, health-care and overall engineering"><meta property=og:image content=https://hadijaveed.me/assets/images/social/posts/Tracing-and-Observability-in-LLM-Applications.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://hadijaveed.me/2024/03/05/tracing-and-observability-in-llm-applications/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="From Concept to Production with Observability in LLM Applications - Hadi Javeed's blog"><meta name=twitter:description content="Notes about startups, AI, health-care and overall engineering"><meta name=twitter:image content=https://hadijaveed.me/assets/images/social/posts/Tracing-and-Observability-in-LLM-Applications.png><link href=../../../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#from-concept-to-production-with-observability-in-llm-applications class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Hadi Javeed's blog" class="md-header__button md-logo" aria-label="Hadi Javeed's blog" data-md-component=logo> <img src=../../../../assets/logo.jpeg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Hadi Javeed's blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> From Concept to Production with Observability in LLM Applications </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../about/ class=md-tabs__link> About me </a> </li> <li class=md-tabs__item> <a href=https://newsletter.hadijaveed.me class=md-tabs__link> Subscribe </a> </li> <li class=md-tabs__item> <a href=../../../../archive/2025/ class=md-tabs__link> Archive </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Hadi Javeed's blog" class="md-nav__button md-logo" aria-label="Hadi Javeed's blog" data-md-component=logo> <img src=../../../../assets/logo.jpeg alt=logo> </a> Hadi Javeed's blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <a href=../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../about/ class=md-nav__link> <span class=md-ellipsis> About me </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=https://newsletter.hadijaveed.me class=md-nav__link> <span class=md-ellipsis> Subscribe </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <!-- Page content --> <article class="md-content__inner md-typeset"> <header class=md-post__header> <!-- Post authors --> <!-- Post metadata --> <div class="md-post__meta md-meta"> <div class=md-meta__list> <div style="margin-right: 1rem;"> <span class=md-author> <a href=https://www.linkedin.com/in/hadijaveed/ > <img src="https://avatars.githubusercontent.com/u/10227760?v=5" alt="Hadi Javeed"> </a> </span> </div> <!-- Post date --> <div class="md-meta__item blog-meta-header"> <time datetime="2024-03-05 00:00:00+00:00">2024/03/05</time></div> <!-- Post categories --> <!-- Post readtime --> <div class="md-meta__item blog-meta-header"> 10 min read </div> </div> <!-- Draft marker --> </div> </header> <h1 id=from-concept-to-production-with-observability-in-llm-applications>From Concept to Production with Observability in LLM Applications<a class=headerlink href=#from-concept-to-production-with-observability-in-llm-applications title="Permanent link">¶</a></h1> <p>Understanding observability in AI applications, particularly in Large Language Models (LLMs), is crucial. It's all about tracking how your model performs over time, which is especially challenging with text generation outputs. Unlike categorical outputs, text generation can vary widely, making it essential to monitor the behavior and performance of your model closely.</p> <!-- more --> <p>Imagine you're developing an application tailored to a specific use case. Perhaps you're enhancing an LLM with an external corpus through techniques like <a href=https://arxiv.org/pdf/2005.11401.pdf target=_blank>RAG (Retrieval-Augmented Generation) </a> or interfacing with a database API to process unstructured text. By leveraging relevant snippets retrieved in this way, you aim for your model to generate useful outputs. With the advent of tools like <a href=https://www.langchain.com/ target=_blank>LangChain</a> and <a href=https://www.llamaindex.ai/ target=_blank>LLamaIndex</a>, alongside embedding models, building such systems has become more straightforward. Your development team's initial reaction might be overwhelmingly positive, but the real challenge emerges when transitioning from a development to a production environment. How do you ensure the accuracy and reliability of your system in real-world scenarios?</p> <p><strong>The Evolution of Chatbots and Classification Applications with LLMs</strong></p> <p>As LLMs grow increasingly accessible, many teams are venturing into creating innovative applications. An approach might involve using a document corpus to develop a RAG pipeline tailored to your domain-specific data. Thanks to open-source libraries, assembling these applications has become significantly easier.</p> <p>You might employ a prompt to enhance the retrieval workflow, utilizing content from a vector store. Given the impressive reasoning capabilities of LLMs, such applications can provide substantial value depending on the use case, earning you accolades from your team for swift development.</p> <p><strong>Navigating the Challenges of Production Readiness</strong></p> <p>When your application is in staging, and your team begins to use it extensively, you're on the cusp of deploying it in a production environment. This stage brings about a critical question: How do you measure the accuracy and performance of your application? LLM-generated responses introduce a high degree of subjectivity, rendering traditional unit tests inadequate. Developing robust test cases becomes imperative, ensuring they're revisited with every modification to the application, be it in the prompts or any other component.</p> <p><strong>Investing in LLM Operations: A Necessity, Not a Choice</strong></p> <p>Collecting comprehensive and well-rounded feedback can seem daunting. Human annotation and labeling, while valuable, are often costly. A practical first step involves identifying 50-100 common queries and patterns. Collaborate with your team or subject matter experts to craft ideal responses for these scenarios.</p> <h3 id=llm-response-evaluation>LLM Response Evaluation<a class=headerlink href=#llm-response-evaluation title="Permanent link">¶</a></h3> <p>An effective strategy is to conduct automated evaluations based on these ideal responses. The more diverse and case-specific your questions are, the better. Even a small subset of questions reviewed by human labelers can provide invaluable insights.</p> <p>Consider implementing an evaluation pipeline similar to <a href=https://github.com/explodinggradients/ragas target=_blank>RAGAS</a>, focusing on metrics like faithfulness and answer relevancy:</p> <ul> <li><strong>Faithfulness</strong>: Assess whether the LLM creates outputs based solely on the provided content, avoiding hallucinations.</li> <li><strong>Relevancy</strong>: Evaluate how the LLM's responses align with the user's questions.</li> </ul> <p>Beyond these metrics, you might explore additional measures tailored to your specific use case. Regularly running these automated tests, especially after updates to your RAG strategy or prompts, can significantly enhance your application's reliability. Incorporating these tests into your continuous deployment process can further streamline operations.</p> <h3 id=tracing-and-collecting-spans-insights-into-execution>Tracing and Collecting Spans: Insights into Execution<a class=headerlink href=#tracing-and-collecting-spans-insights-into-execution title="Permanent link">¶</a></h3> <p>Drawing inspiration from <a href=https://opentelemetry.io/docs/concepts/signals/traces/ target=_blank>OpenTelemetry</a>, Traces&nbsp;give us the big picture of what happens when a request is made to an LLM application. Traces are essential to understanding the full “path” a request takes in your application, e.g, prompt, query-expansion, RAG retrieved top-k document, functional call and other mechanisms incorporated into your LLM application are represented as <a href=https://opentelemetry.io/docs/concepts/signals/traces/#spans target=_blank>SPAN</a> under one trace. Spans represent individual unit of work or operation e.g, vector store call, functional call or others.</p> <p>Understanding the intricacies of your LLM's performance is vital. Perhaps the issue isn't with the LLM itself but with the RAG component or a lack of relevant data in your corpus. Identifying the root cause of errors or hallucinations requires comprehensive traceability of your application's execution paths.</p> <p><strong>Example: Leveraging Langsmith SDK for Enhanced Observability</strong></p> <p>Selecting the right tools for tracing can dramatically affect your operational efficiency. While custom solutions are possible, they often require substantial effort to implement correctly. Tools like Langchain provide abstractions that facilitate tracking multiple execution units, types, and attributes without overcomplicating your codebase, allowing you to focus more on product development and less on operational infrastructure. </p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=k>class</span><span class=w> </span><span class=nc>LLMTracer</span><span class=p>:</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>:</span> <span class=nb>dict</span><span class=p>,</span> <span class=n>meta</span><span class=p>:</span> <span class=nb>dict</span> <span class=o>=</span> <span class=p>{}):</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pipeline</span> <span class=o>=</span> <span class=n>RunTree</span><span class=p>(</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>            <span class=n>run_type</span><span class=o>=</span><span class=s2>"chain"</span><span class=p>,</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>            <span class=n>name</span><span class=o>=</span><span class=s2>"&lt;your-application&gt;"</span><span class=p>,</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>            <span class=n>inputs</span><span class=o>=</span><span class=n>inputs</span><span class=p>,</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>            <span class=n>extra</span><span class=o>=</span><span class=n>meta</span><span class=p>,</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>        <span class=p>)</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>    <span class=k>def</span><span class=w> </span><span class=nf>add_log</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>inputs</span><span class=p>:</span> <span class=nb>dict</span> <span class=o>=</span> <span class=p>{},</span> <span class=n>outputs</span><span class=p>:</span> <span class=nb>dict</span> <span class=o>=</span> <span class=p>{},</span> <span class=n>run_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"llm"</span><span class=p>):</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>        <span class=n>log</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pipeline</span><span class=o>.</span><span class=n>create_child</span><span class=p>(</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>            <span class=n>name</span><span class=o>=</span><span class=n>name</span><span class=p>,</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>            <span class=n>run_type</span><span class=o>=</span><span class=n>run_type</span><span class=p>,</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>            <span class=n>inputs</span><span class=o>=</span><span class=n>inputs</span><span class=p>,</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>        <span class=p>)</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>        <span class=n>log</span><span class=o>.</span><span class=n>end</span><span class=p>(</span><span class=n>outputs</span><span class=o>=</span><span class=n>outputs</span><span class=p>)</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>        <span class=n>log</span><span class=o>.</span><span class=n>post</span><span class=p>()</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a>    <span class=k>def</span><span class=w> </span><span class=nf>final</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>outputs</span><span class=p>:</span> <span class=nb>dict</span> <span class=o>=</span> <span class=p>{}):</span>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pipeline</span><span class=o>.</span><span class=n>end</span><span class=p>(</span><span class=n>outputs</span><span class=o>=</span><span class=n>outputs</span><span class=p>)</span>
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pipeline</span><span class=o>.</span><span class=n>post</span><span class=p>()</span>
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a>
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a>
</span><span id=__span-0-24><a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a><span class=c1>## example usage in the code</span>
</span><span id=__span-0-25><a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a><span class=n>tracer</span> <span class=o>=</span> <span class=n>LLMTracer</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=n>messages</span><span class=p>,</span> <span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s2>"model"</span><span class=p>:</span> <span class=s2>"xyz"</span><span class=p>,</span> <span class=o>**</span><span class=n>other</span> <span class=p>})</span>
</span><span id=__span-0-26><a id=__codelineno-0-26 name=__codelineno-0-26 href=#__codelineno-0-26></a>
</span><span id=__span-0-27><a id=__codelineno-0-27 name=__codelineno-0-27 href=#__codelineno-0-27></a><span class=c1>## add logs to the execution trace</span>
</span><span id=__span-0-28><a id=__codelineno-0-28 name=__codelineno-0-28 href=#__codelineno-0-28></a><span class=n>tracer</span><span class=o>.</span><span class=n>add_log</span><span class=p>(</span><span class=s2>"functional_call"</span><span class=p>,</span> <span class=n>inputs</span><span class=o>=</span><span class=p>{</span><span class=o>**</span><span class=n>function_inputs</span><span class=p>},</span> <span class=n>outputs</span><span class=o>=</span><span class=p>{</span><span class=o>**</span><span class=n>function_outputs</span><span class=p>)})</span>
</span><span id=__span-0-29><a id=__codelineno-0-29 name=__codelineno-0-29 href=#__codelineno-0-29></a>
</span><span id=__span-0-30><a id=__codelineno-0-30 name=__codelineno-0-30 href=#__codelineno-0-30></a>
</span><span id=__span-0-31><a id=__codelineno-0-31 name=__codelineno-0-31 href=#__codelineno-0-31></a><span class=c1>## RAG call results or something</span>
</span><span id=__span-0-32><a id=__codelineno-0-32 name=__codelineno-0-32 href=#__codelineno-0-32></a><span class=n>tracer</span><span class=o>.</span><span class=n>add_log</span><span class=p>(</span><span class=s2>"rag"</span><span class=p>,</span> <span class=n>inputs</span><span class=o>=</span><span class=p>{</span><span class=o>**</span><span class=n>rag_query</span><span class=p>},</span> <span class=n>outputs</span><span class=o>=</span><span class=p>{</span><span class=o>**</span><span class=n>rag_docs_etc</span><span class=p>})</span>
</span><span id=__span-0-33><a id=__codelineno-0-33 name=__codelineno-0-33 href=#__codelineno-0-33></a>
</span><span id=__span-0-34><a id=__codelineno-0-34 name=__codelineno-0-34 href=#__codelineno-0-34></a>
</span><span id=__span-0-35><a id=__codelineno-0-35 name=__codelineno-0-35 href=#__codelineno-0-35></a><span class=c1>## towards the end of execution, usually before sending the reply back or final output</span>
</span><span id=__span-0-36><a id=__codelineno-0-36 name=__codelineno-0-36 href=#__codelineno-0-36></a><span class=n>tracer</span><span class=o>.</span><span class=n>final</span><span class=p>(</span><span class=n>output</span><span class=o>=</span><span class=p>{</span><span class=o>**</span><span class=n>your</span> <span class=n>final</span> <span class=n>message</span> <span class=ow>or</span> <span class=n>the</span> <span class=n>output</span><span class=p>})</span>
</span></code></pre></div> <p>Leveraging either their <a href=https://docs.smith.langchain.com/user_guide target=_blank>UI</a> or SDK, you have the flexibility to meticulously select and integrate traces into your testing pipeline or direct them towards an annotation queue, especially if your process incorporates human labelers.</p> <p>The essence of this approach underscores the necessity of a robust tracing infrastructure within your LLM application, enabling the tracking of key metrics. It's imperative to monitor metadata associated with inputs, such as chunk-size, model, prompt, and the number of documents. The more comprehensive your tracking, the more accurately you can assess what aspects are performing optimally and which areas require refinement.</p> <p>The choice of tracing application remains a matter of personal preference. Options range from DataDog and OpenTrace to custom-built solutions using Clickhouse or Postgres. However, managed services like Langsmith or Arize offer distinct advantages, particularly in streamlining test case automation and facilitating the collection of annotations via human labelers.</p> <blockquote> <p>Also the traces and spans let you monitor your number of token usage, latency and pricing.</p> </blockquote> <h3 id=sql-database-for-tracing>SQL Database for tracing:<a class=headerlink href=#sql-database-for-tracing title="Permanent link">¶</a></h3> <p>Although tools such as Arize and Langsmith offer impressive features, they are not indispensable. You can adopt a strategy to monitor events using a tabular format as shown below as long as your infrastructure can support it</p> <table> <thead> <tr> <th>query_id</th> <th>question</th> <th>response</th> <th>source_documents</th> <th>chunks</th> <th>mean_cosine_scores</th> <th>re_ranker_score</th> <th>metadata</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>user-question</td> <td>llm-response</td> <td>[docid1, docid2]</td> <td>[chunkid1, chunkid2]</td> <td>0.855</td> <td>0.8</td> <td>{***json-based}</td> </tr> <tr> <td>2</td> <td>...</td> <td>...</td> <td>[]</td> <td>[]</td> <td></td> <td></td> <td>{****}</td> </tr> </tbody> </table> <p>Additionally, you can track user feedback in a separate table, linked by query IDs. This feedback on whether responses were helpful or not can be instrumental in fine-tuning your model or overall understanding how model could be improved later</p> <table> <thead> <tr> <th>query_id</th> <th>user_id</th> <th>helpful</th> <th>response_suggestion</th> <th>response_critique</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>user-id</td> <td>yes/no</td> <td>re-written response</td> <td>user-feedback</td> </tr> <tr> <td>2</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> </tbody> </table> <h3 id=prompt-management-versioning>Prompt Management (Versioning):<a class=headerlink href=#prompt-management-versioning title="Permanent link">¶</a></h3> <p>Implementing an effective version control system for your prompts can significantly enhance your testing processes. Consider the scenario where you're conducting A/B tests between two different sets of prompts to determine which yields better performance. The ability to trace the evolution of your prompts—reviewing every change to understand whether these modifications have led to improvements or declines in production performance—can be incredibly valuable.</p> <p>The method you choose for managing version control of your prompts is entirely up to you. Langsmith offers a private hub that allows you to track versions of your prompts along with their input variables, although this might not significantly enhance your prompt management strategy. Personally, I prefer to maintain prompts within a code editor, typically in .txt or .py files, with a designated variable for version control, and organize them within a specific directory.</p> <div class="language-shell highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>prompts/
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=w>    </span>system_prompt/
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=w>            </span><span class=m>2023</span>-12-28.txt
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=w>            </span><span class=m>2024</span>-01-12.txt
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=w>    </span>other_prompts/
</span></code></pre></div> <p>However, managing prompts in this manner presents challenges, particularly within a production-grade system where updates necessitate a code deployment. An alternative approach involves utilizing the Langsmith hub. Regardless of the method, it's imperative to exercise extreme caution when updating prompts in production, ensuring thorough testing in a lower environment beforehand.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>SYSTEM_PROMPT</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>pull</span><span class=p>(</span><span class=sa>f</span><span class=s2>"system_prompt:</span><span class=si>{</span><span class=n>prompt_versopm</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span><span class=o>.</span><span class=n>template</span>
</span></code></pre></div> <p>Now, the version of the prompt must be retrieved from the configuration or your database at the time of inference. Alternatively, you can configure your system to always use the most recent version of the prompt.</p> <h3 id=rag-evaluation>RAG Evaluation:<a class=headerlink href=#rag-evaluation title="Permanent link">¶</a></h3> <p>When working with proprietary and domain-specific data in your LLM application, it's crucial to implement robust measures for evaluating your Retrieval-Augmented Generation (RAG) pipeline. A comprehensive RAG pipeline typically encompasses the following components:</p> <ul> <li> <p>An embedding model, which can be either proprietary or open-source. For benchmarking, consider the <a href=https://huggingface.co/spaces/mteb/leaderboard target=_blank>MTEB benchmark</a>.</p> </li> <li> <p>A vector store, such as PgVector, Pinecone, or similar.</p> </li> <li> <p>A chunking strategy detailing how data is segmented and whether chunks overlap.</p> </li> <li> <p>Document ingestion processes to ensure clean data extraction from PDFs, HTML, or other formats.</p> </li> <li> <p>Metadata filtering to refine the embedding search space through specific criteria.</p> </li> <li> <p>A combination of hybrid search techniques or <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html target=_blank>Reciprocal Rank Fusion</a>, utilizing both dense and sparse vectors, possibly integrating a BM25 filter for enhanced keyword search.</p> </li> <li> <p>A <a href=https://txt.cohere.com/rerank/ target=_blank>re-ranker or cross-encoder</a> to improve result relevance.</p> </li> <li> <p>Query expansion techniques for optimizing the search experience through query rewriting or extraction.</p> </li> </ul> <p>Initiating a RAG pipeline might seem straightforward, but constructing a production-grade, accurate system introduces numerous complexities. For instance, replacing an embedding model with one of a different dimensionality necessitates regenerating all embeddings, a task manageable within a new database or index. Similarly, modifications to chunking strategies, document parsing, or the implementation of Reciprocal Rank Fusion or metadata-based filtering raise questions about the efficacy of these changes.</p> <p>To address these challenges, it's essential to develop specialized, robust test cases focused on retrieval. Unlike end-to-end LLM evaluations, testing individual components of the RAG pipeline can provide insightful feedback. Recommended test cases include:</p> <ul> <li>Precision@K</li> <li>Recall</li> <li>Mean Cosine Scores and Re-ranker scores</li> </ul> <p>Tracking these metrics requires a collection of 100-200 diverse test cases tailored to your specific use case. Regular analysis of RAG results upon any modification or content addition is vital.</p> <p>Enhance your RAG pipeline evaluation by meticulously documenting metadata, such as top-k results, the size of the last chunk used, and the embedding model employed. The more metadata you track, the more nuanced your understanding of the pipeline's performance, facilitating targeted improvements.</p> <p><strong>Embedding based cluster analysis</strong>:</p> <p>Utilizing <a href=https://umap-learn.readthedocs.io/en/latest/clustering.html target=_blank>HDBSCAN</a> to segment embeddings into distinct inference groups can be instrumental in pinpointing segments of your embeddings that are underperforming or deviating from expected patterns. Similarly, <a href=https://umap-learn.readthedocs.io/en/latest/clustering.html target=_blank>UMAP</a> can facilitate a deeper comprehension of how your embeddings capture semantic meanings in a format that's easy to visualize. ArizeAI offers an impressive 3D visualization tool that's worth exploring, although I haven't personally experimented with it yet.</p> <p>For those with access to production data, initiating a notebook to visualize and dissect embeddings can be enlightening. In this context, tools like <a href=https://arize.com/llm/ target=_blank>Arize</a> prove to be invaluable resources for such analytical endeavors.</p> <h2 id=fine-tuning>Fine-Tuning:<a class=headerlink href=#fine-tuning title="Permanent link">¶</a></h2> <p>Details on fine-tuning metrics and their significance will be provided in the future. As of now, I have not systematically tracked these metrics. However, I am currently in the process of doing so and plan to share insights on the necessity and impact of fine-tuning a model based on my experiences. This section will be updated accordingly.</p> <h2 id=conclusion>Conclusion:<a class=headerlink href=#conclusion title="Permanent link">¶</a></h2> <p>In the rapidly evolving domain of production-grade LLM applications, there exists no one-size-fits-all strategy. The current GPU shortage raises questions about the capability of mid to smaller sized companies to support workloads exceeding 1M+ QPS. However, it's anticipated that these capacity constraints will diminish over time. Nonetheless, for LLM operations of any scale, it's crucial to have robust operational components in place. These include diligent response monitoring and establishing benchmarks for accuracy, relevancy, and RAG metrics. Such measures empower developers to make informed modifications with confidence, supported by data, and to pinpoint precise areas where adjustments are necessary.</p> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">¶</a></h2> <ul> <li><a href=https://arxiv.org/pdf/2005.11401.pdf target=_blank>RAG (Retrieval-Augmented Generation) </a></li> <li><a href=https://github.com/explodinggradients/ragas target=_blank>RAGAS</a></li> <li><a href=https://docs.smith.langchain.com/user_guide target=_blank>Langsmith</a></li> <li><a href=https://umap-learn.readthedocs.io/en/latest/clustering.html target=_blank>UMAP &amp; HDBSCAN</a></li> <li><a href=https://huggingface.co/spaces/mteb/leaderboard target=_blank>MTEB benchmark</a></li> <li><a href=https://opentelemetry.io/docs/concepts/signals/traces/ target=_blank>OpenTelemetry</a></li> <li><a href=https://txt.cohere.com/rerank/ target=_blank>Cohere Rerank</a></li> </ul> <div class=subscribe-container> <h3>Subscribe</h3> <p>Honest takes on AI, startups, and digital health—delivered to your inbox.</p> <p>Your privacy is paramount. Expect content once or twice a month. Unsubscribe anytime if you don't like it.</p> <a href=https://newsletter.hadijaveed.me class="md-button md-button--primary" target=_blank>Get Email Updates</a> </div> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../../../2023/05/15/navigating-ai-hype-thinking-about-llms-niche-application/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Navigating the AI Hype and Thinking about Niche LLM Applications"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Navigating the AI Hype and Thinking about Niche LLM Applications </div> </div> </a> <a href=../../../08/11/how-llms-revolutionized-my-productivity/ class="md-footer__link md-footer__link--next" aria-label="Next: How LLMs Revolutionized My Productivity"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> How LLMs Revolutionized My Productivity </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> © 2024 <a href=https://www.linkedin.com/in/hadijaveed target=_blank rel=noopener>Hadi Javeed</a> </div> </div> <div class=md-social> <a href=https://github.com/hadijaveed target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"></path></svg> </a> <a href=https://twitter.com/hadijpk target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"></path></svg> </a> <a href=https://www.linkedin.com/in/hadijaveed target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../..", "features": ["toc.integrate", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy", "announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.top", "navigation.instant", "navigation.instant.prefetch", "navigation.expand", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.footer", "navigation.tabs", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>